\documentclass[10pt]{article}
\usepackage{times}
\usepackage{fullpage}
\usepackage[override]{cmtt}
\usepackage{graphicx}
\usepackage[bookmarks=false,hyperfootnotes=false]{hyperref}

\title{A Sophomoric\footnote{As in intended for second-year students,
    not as in immaturely pretentious}\ \ \ \ Introduction to\\ Shared-Memory
    Parallelism and Concurrency}
\author{Dan Grossman\\
Adapted to C\# by Andrei Paleyes}
\date{Version of \today}

\begin{document}

\maketitle

\newpage

\setcounter{tocdepth}{2}
\tableofcontents

\section{Meta-Introduction: An Instructor's View of These Notes}
\label{sec:meta-intro}

\subsection{Where This Material Fits in a Changing Curriculum}

\medskip
\noindent \textbf{These notes teach parallelism and concurrency as part of an
  advanced sophomore-level data-structures course -- the course that
  covers asymptotic complexity, balanced trees, hash tables, graph
  algorithms, sorting, etc.}
\medskip

\medskip
\noindent \emph{Why parallelism and concurrency should be taught early:}
\medskip

Parallelism and concurrency are increasingly important topics in
computer science and engineering.  Traditionally, most undergraduates
learned rather little about these topics and did so rather late in the
curriculum: Senior-level operating-systems courses cover threads,
scheduling, and synchronization.  Early hardware courses have circuits
and functional units with parallel parts.  Advanced architecture
courses discuss cache coherence.  Electives might cover parallel
algorithms or use distributed computing to solve embarrassingly
parallel tasks.

Little of this scattered material emphasizes the essential concepts of
parallelism and concurrency --- and certainly not in a central place
such that subsequent courses can rely on it.  These days, most desktop
and laptop computers have multiple cores.  Modern high-level languages
have threads built into them and standard libraries use threads (e.g.,
Windows Presentation Foundantion in .NET Framework).  It no longer seems reasonable to
bring up threads ``as needed'' or delay until an operating-systems
course that should be focusing on operating systems.  There is no
reason to introduce threads first in C or assembly when all the usual
conveniences of high-level languages for introducing core concepts
apply.

\medskip
\noindent \emph{Why parallelism and concurrency should not be taught too early:}
\medskip

Conversely, it is tempting to introduce threads ``from day one'' in
introductory programming courses before students learn ``sequential
habits.''  I suspect this approach is infeasible in most curricula.
For example, it may make little sense in programs where most students
in introductory courses do not end up majoring in computer science.
``Messing with intro'' is a high-risk endeavor, and introductory
courses are already packed with essential concepts like variables,
functions, arrays, linked structures, etc.  There is probably no room.

\medskip
\noindent \emph{So put it in the data-structures course:}
\medskip

There may be multiple natural places to introduce parallelism and
concurrency, but I claim ``sophomore-level'' data structures (after
CS2 and discrete math, but before ``senior-level'' algorithms) works
very well.  Here are some reasons:
\begin{itemize}
\item There is room: We made room by removing three weeks
  on skew heaps, leftist heaps, binomial queues, splay
  trees, disjoint-set, and network flow.  Some of the trimming was
  painful and will be compensated for in senior-level algorithms, but
  all this material seems relatively less important.  There was
  still plenty of room for essential data structures and related
  concepts such as asymptotic analysis, (simple) amortization, graphs,
  and sorting.
\item Fork-join parallel algorithms are amenable to asymptotic
  analysis in terms of ``work'' and ``span'' over dags --- all
  concepts that fit very naturally in the course.  Amdahl's Law is
  fundamentally an asymptotic argument too.
\item Ideas from sequential algorithms already in the course
  reappear with parallelism.  For example, just as constant factors compel
  efficient quicksort implementations to switch to an $O(n^2)$ sort
  for small $n$, constant factors compel efficient parallel
  algorithms to switch to sequential algorithms for small problem
  sizes.  Parallel sorting algorithms are also good examples of
  non-trivial parallelization.
\item Many canonical examples for concurrency involve basic
  data structures: bounded buffers for condition variables, dictionaries
  for reader/writer locks, parallel unstructured graph traversals,
  etc.  Making a data structure ``thread-safe'' is an ideal way to
  think about what it means to be ``thread-safe.''
\item We already used C\# in the course.  .NET 4 (and higher)'s Task Parallel Library
  is excellent for teaching fork-join parallelism.  C\# built-in
  support for threads, locks, and condition variables is sufficient
  for teaching concurrency.  
\end{itemize}
On the other hand, instructors wishing to introduce message passing or
distributed computation will have to consider whether it makes sense
in this course.  I focus on shared memory, only mentioning other
models.  I do not claim shared memory is ``better'' (that's an endless
argument on which I have no firm opinion), only that it is an
important model and a good one to start with pedagogically.  I also do
not emphasize asynchrony and masking I/O latency.  Such topics are
probably better covered in a course on systems programming, though one
could add them to these notes.

While most of the material is these notes is not specific to a
particular programming language, all examples and discussions use C\#
when a specific language is warranted.  A C++ version of these
materials is also available thanks to Steve Wolfman from
the University of British Columbia.  Porting to additional languages
should be quite doable, and I would be happy to collaborate with people
interested in doing so.

For more information on the motivation and context, see a SIGCSE2012
paper coauthored with Ruth E. Anderson.%~\cite{spac-sigcse}.

\subsection{Six Theses On A Successful Approach to this Material}

In summary, these notes rest on several theses for how to teach this
material:
\begin{enumerate}
\item Integrate it into a data-structures course.
\item Distinguish parallelism (using extra computational units to do more work
  per unit time) from concurrency (managing access to shared
  resources).  Teach parallelism first because it is easier and helps
  establish a non-sequential mindset.
\item Teach in a high-level language, using a library for fork-join
  parallelism.  Teach how to use parallelism, threads, locks, etc.  Do
  not teach how to implement them.  
\item Conversely, do not teach in terms
  of higher-order parallel patterns like maps and reduces.  Mention
  these, but have students actually do the divide-and-conquer
  underlying these patterns.
\item Assume shared memory since one programming model is hard enough.
  \item Given the limited time and student background, do not focus on
  memory-hierarchy issues (e.g., caching), much like these issues are
  mentioned (e.g., with B-trees) but rarely central in data-structures
  courses. (Adding a discussion should prove straightforward.)
\end{enumerate}
If you strongly disagree with these theses, you will probably
not like these notes --- but you might still try them.

\subsection{How to Use These Notes --- And Improve Them}

These notes were originally written for CSE332 at the University of
Washington\\ (\url{http://courses.cs.washington.edu/courses/cse332}).
They account for 3 weeks of a required 10-week course (the University uses a
quarter system).  Alongside these notes are PowerPoint slides,
homework assignments, and a programming project.  In fact, these notes
were the last aspect to be written --- the first edition of the course
went great without them and students reported parallelism to be their favorite
aspect of the course.

Surely these notes have errors and explanations could be
improved.  \emph{Please} let me know of any problems you find.  I am a
perfectionist: if you find a typo I would like to know.  Ideally you
would first check that the most recent version of the notes does not
already have the problem fixed.

I encourage you to use these notes in whatever way works best for you.
The \LaTeX\ sources are also available.  I would \emph{like} to know if
you are using these notes and how.  It motivates me to improve them
and, frankly, it's not bad for my ego.  Constructive criticism
is also welcome.  That said, I don't expect any thoughtful instructor
to agree completely with me on what to cover and how to cover it.

Contact me at the email {\tt djg} and then the at-sign and then {\tt
  cs.washington.edu}.  The current home for these notes and related
  materials is 
\url{http://homes.cs.washington.edu/~djg/teachingMaterials/}.
  Students are more than welcome to contact me: who better to let me
  know where these notes could be improved.

\subsection{Acknowledgments}

I deserve no credit for the material in these notes.  If anything, my
role was simply to distill decades of wisdom from others down to three
weeks of core concepts and integrate the result into a data-structures
course.  When in doubt, I stuck with the basic and simplest topics and
examples.

I was particularly influenced by Guy Blelloch and Charles Leisersen in
terms of teaching parallelism before concurrency and emphasizing
divide-and-conquer algorithms that do not consider the number of
processors.  Doug Lea and other developers of Java's ForkJoin
framework provided a wonderful library that, with some hand-holding,
is usable by sophomores.  Larry Snyder was also an excellent resource
for parallel algorithms.  

The treatment of shared-memory synchronization is heavily influenced
by decades of operating-systems courses, but with the distinction of
ignoring all issues of scheduling and synchronization implementation.
Moreover, the emphasis on the need to avoid data races in high-level
languages is frustratingly under-appreciated despite the noble work of
memory-model experts such as Sarita Adve, Hans Boehm, and Bill Pugh.

Feedback from Ruth Anderson, Kim Bruce, Kristian Lieberg, Tyler
Robison, Cody Schroeder, and Martin Tompa helped improve explanations
and remove typos.  Tyler and Martin deserve particular mention for
using these notes when they were very new.  James Fogarty made many
useful improvements to the presentation slides that accompany these
reading notes.  Steve Wolfman created the C++ version of these notes.

Nicholas Shahan created almost all the images and diagrams in these
notes, which make the accompanying explanations much better.

I have had enlightening and enjoyable discussions on ``how to teach this
stuff'' with too many researchers and educators over the last few
years to list them all, but I am grateful.

This work was funded in part via grants from the U.S. National Science
Foundation and generous support, financial and otherwise, from Intel
Labs University Collaborations.

\section{Introduction}
\label{sec:intro}

\subsection{More Than One Thing At Once}

In \emph{sequential programming}, one thing happens at a time.
Sequential programming is what most people learn first and how most
programs are written.  Probably every program you have written in C\#
(or a similar language) is sequential: execution starts at the
beginning of \texttt{Main} and proceeds one assignment / call / return
/ arithmetic operation at a time.

Removing the one-thing-at-a-time assumption complicates writing
software.  The multiple \emph{threads of execution} (things performing
computations) will somehow need to coordinate so that they can work
together to complete a task --- or at least not get in each other's
way while they are doing separate things.  These notes cover basic
concepts related to \emph{multithreaded programming}, i.e., programs
where there are multiple threads of execution.  We will
cover:
\begin{itemize}
\item How to create multiple threads
\item How to write and analyze divide-and-conquer algorithms that use  
  threads to produce results more quickly
\item How to coordinate access to shared objects so that multiple
  threads using the same data do not produce the wrong answer
\end{itemize}

A useful analogy is with cooking.  A sequential program is like having
one cook who does each step of a recipe in order, finishing one step
before starting the next.  Often there are multiple steps that could
be done at the same time --- if you had more cooks.  But having more
cooks requires extra coordination.  One cook may have to wait for
another cook to finish something.  And there are limited resources: If
you have only one oven, two cooks won't be able to bake casseroles at
different temperatures at the same time.  In short, multiple cooks
present efficiency opportunities, but also significantly
complicate the process of producing a meal.

Because multithreaded programming is so much more difficult, it is
best to avoid it if you can.  For most of computing's history, most
programmers wrote only sequential programs.  Notable exceptions were:
\begin{itemize}
\item Programmers writing programs to solve such computationally large
  problems that it would take years or centuries for one computer to
  finish.  So they would use multiple computers together.
\item Programmers writing systems like an operating system where a key
  point of the system is to handle multiple things happening at once.
  For example, you can have more than one program running at a time.
  If you have only one processor, only one program can \emph{actually}
  run at a time, but the operating system still uses threads to keep
  track of all the running programs and let them take turns.  If the
  taking turns happens fast enough (e.g., 10 milliseconds), humans
  fall for the illusion of simultaneous execution.  This is called
  \emph{time-slicing}.
\end{itemize}
Sequential programmers were lucky: since every 2 years or so computers
got roughly twice as fast, most programs would get exponentially
faster over time without any extra effort.

Around 2005, computers stopped getting twice as fast every 2 years.
To understand why requires a course in computer architecture.  In
brief, increasing the clock rate (very roughly and technically
inaccurately speaking, how quickly instructions execute) became
infeasible without generating too much heat.  Also, the relative cost
of memory accesses can become too high for faster processors to help.

Nonetheless, chip manufacturers still plan to make exponentially more
powerful chips.  Instead of one processor running faster, they will
have more processors.  The next computer you buy will likely have 4
processors (also called \emph{cores}) on the same chip and the number
of available cores will likely double every few years.

What would 256 cores be good for?  Well, you can run multiple programs
at once --- for real, not just with time-slicing.  But for an
individual program to run any faster than with one core, it will need
to do more than one thing at once.  This is the reason that
multithreaded programming is becoming more important.  To be clear,
\emph{multithreaded programming is not new.  It has existed for
decades and all the key concepts are just as old.}
Before there were multiple cores on one chip, you could use multiple
chips and/or use time-slicing on one chip --- and both remain
important techniques today.  The move to multiple cores on one chip is
``just'' having the effect of making multithreading something that
more and more software wants to do.

\subsection{Parallelism vs. Concurrency}

These notes are organized around a fundamental distinction between
\emph{parallelism} and \emph{concurrency}.  Unfortunately, the way we
define these terms is not entirely standard, so you should not assume
that everyone uses these terms as we will.  Nonetheless, most computer
scientists agree that this distinction is important.

\medskip
\noindent\textbf{Parallel programming is about using additional computational
resources to produce an answer faster.}
\medskip

As a canonical example, consider the trivial problem of summing up all
the numbers in an array.  We know no sequential algorithm can do
better than $\Theta(n)$ time.  Suppose instead we had 4 processors.  Then
hopefully we could produce the result roughly 4 times faster by having
each processor add 1/4 of the elements and then we could just add
these 4 partial results together with 3 more additions.  $\Theta(n/4)$
is still $\Theta(n)$, but constant factors can matter.  Moreover, when
designing and analyzing a \emph{parallel algorithm}, we should leave
the number of processors as a variable, call it $P$.  Perhaps we can
sum the elements of an array in time $O(n/P)$ given $P$ processors.
As we will see, in fact the best bound under the assumptions we will
make is $O(\log n + n/P)$.

In terms of our cooking analogy, parallelism is about using extra
cooks (or utensils or pans or whatever) to get a large meal finished
in less time.  If you have a huge number of potatoes to slice, having
more knives and people is really helpful, but at some point adding
more people stops helping because of all the communicating and
coordinating you have to do: it is faster for me to slice one potato by
myself than to slice it into fourths, give it to four other people, and
collect the results.

\medskip
\noindent\textbf{Concurrent programming is about correctly and efficiently
  controlling access by multiple threads to shared resources.}
\medskip

As a canonical example, suppose we have a dictionary implemented as a
hashtable with operations {\tt insert}, {\tt lookup}, and {\tt
  delete}.  Suppose that inserting an item already in the table is
supposed to update the key to map to the newly inserted value.
Implementing this data structure for sequential programs is something
we assume you could already do correctly.  Now suppose different
threads use the \emph{same} hashtable, potentially at the same time.
Suppose two threads even try to {\tt insert} the same key at the same
time.  What might happen?  You would have to look at your sequential
code carefully, but it is entirely possible that the same key might
end up in the table twice.  That is a problem since a subsequent {\tt
  delete} with that key might remove only one of them, leaving the key
in the dictionary.

To prevent problems like this, concurrent programs use
\emph{synchronization primitives} to prevent multiple threads from
\emph{interleaving their operations} in a way that leads to incorrect
results.  Perhaps a simple solution in our hashtable example is to
make sure only one thread uses the table at a time, finishing an
operation before another thread starts.  But if the table is large,
this is unnecessarily inefficient most of the time if the threads
are probably accessing different parts of the table.

In terms of cooking, the shared resources could be something like an
oven.  It is important not to put a casserole in the oven unless the
oven is empty.  If the oven is not empty, we could keep checking until
it is empty.  In C\#, you might naively write:
\begin{verbatim}
    while (true)
    {
       if (OvenIsEmpty())
       {
          PutCasseroleInOven();
          break;
       }
    }
\end{verbatim}
Unfortunately, code like this is broken if two threads run it at the
same time, which is the primary complication in concurrent programming.  They
might both see an empty oven and then both put a casserole in.  We
will need to learn ways to check the oven and put a casserole in
without any other thread doing something with the oven in the
meantime.

\medskip
\noindent\textbf{Comparing Parallelism and Concurrency}
\medskip

We have emphasized here how parallel programming and concurrent
programming are different.  Is the problem one of using extra
resources effectively or is the problem one of preventing a bad
interleaving of operations from different threads?  It is
all-too-common for a conversation to become muddled because one person
is thinking about parallelism while the other is thinking about
concurrency.

In practice, the distinction between parallelism and concurrency is
not absolute.  Many programs have aspects of each.  Suppose you had a
huge array of values you wanted to insert into a hash table.  From the
perspective of dividing up the insertions among multiple threads, this
is about parallelism.  From the perspective of coordinating access to
the hash table, this is about concurrency.  Also, parallelism does
typically need some coordination: even when adding up integers in an
array we need to know when the different threads are done with their
chunk of the work.

We believe parallelism is an easier concept to start with than
concurrency.  You probably found it easier to understand how to use
parallelism to add up array elements than understanding why the
while-loop for checking the oven was wrong.  (And if you still don't
understand the latter, don't worry, later sections will explain
similar examples line-by-line.)  So we will start with parallelism
(Sections \ref{sec:fork-join}, \ref{sec:analyze-fork-join},
\ref{sec:fancy-parallel}), getting comfortable with multiple things
happening at once.  Then we will switch our focus to concurrency
(Sections \ref{sec:basic-concurrency}, \ref{sec:races},
\ref{sec:guidelines}, \ref{sec:deadlock}, \ref{sec:other-synch}) and
shared resources (using memory instead of ovens), learn many of the
subtle problems that arise, and present programming guidelines to
avoid them.

\subsection{Basic Threads and Shared Memory}

Before writing any parallel or concurrent programs, we need some way to
\emph{make multiple things happen at once} and some way for those
different things to \emph{communicate}.  Put another way, your computer
may have multiple cores, but all the C\# constructs you know are for
sequential programs, which do only one thing at once.  Before showing
any C\# specifics, we need to explain the \emph{programming model}.

The model we will assume is \emph{explicit threads} with \emph{shared
  memory}.  A \emph{thread} is itself like a running sequential
program, but one thread can create other threads that are part of the
same program and those threads can create more threads, etc.  Two or
more threads can communicate by writing and reading fields
of the same objects.  In other words, they share memory.  This is only
one model of parallel/concurrent programming, but it is the only one we
will use.  The next section briefly mentions other models that
a full course on parallel/concurrent programming would likely cover.

Conceptually, all the threads that have been started but not yet
terminated are ``running at once'' in a program.  In practice, they
may not all be running at any particular moment:
\begin{itemize}
\item There may be more threads than processors.  It is up to the C\#
  implementation, with help from the underlying operating system, to
  find a way to let the threads ``take turns'' using the available
  processors.  This is called \emph{scheduling} and is a major topic
  in operating systems.  All we need to know is that it is not under
  the C\# programmer's control: you create the threads and the system
  schedules them.
\item A thread may be waiting for something to happen before it
  continues.  For example, the next section discusses the {\tt join}
  primitive where one thread does not continue until another thread
  has terminated.
\end{itemize}

Let's be more concrete about what a thread is and how threads
communicate.  It is helpful to start by enumerating the key pieces that
a \emph{sequential} program has \emph{while it is running} (see
also Figure~\ref{fig:sequential_state}):
\begin{enumerate}
\item One \emph{call stack}, where each \emph{stack frame} holds the
  local variables for a method call that has started but not yet
  finished.  Calling a method pushes a new frame and returning from a
  method pops a frame.  Call stacks are why recursion is not
  ``magic.''
\item One \emph{program counter}.  This is just a low-level name for
  keeping track of what statement is currently executing.  In a
  sequential program, there is exactly one such statement.
\item Static fields of classes.
\item Objects.  An object is created by calling {\tt new}, which
  returns a reference to the new object.  We call the memory that holds all
  the objects the \emph{heap}.  This use of the word ``heap''
  has nothing to do with heap data structure used to implement
  priority queues.  It is separate memory from the memory used for the
  call stack and static fields.
\end{enumerate}

\begin{figure}
\begin{center}
\includegraphics[scale=.6]{images/sequential_stack_frames.png}
\end{center}
\caption{The key pieces of an executing sequential program: A program
  counter, a call stack, and a heap of objects.  (There are also
  static fields of classes, which we can think of as being in the heap.)}
\label{fig:sequential_state}
\end{figure}

With this overview of the sequential \emph{program state}, it is much
easier to understand threads: 

\medskip
\noindent\textbf{Each thread has its own call
  stack and program counter, but all the threads share one collection
  of static fields and objects.} (See also Figure~\ref{fig:multi_state}.)
\medskip

\begin{figure}
\begin{center}
\includegraphics[scale=.6]{images/multi_stack_frames.png}
\end{center}
\caption{The key pieces of an executing multithreaded program: Each
  thread has its own program counter and call-stack, but objects in
  the heap may be shared by multiple threads.  (Static fields of
  classes are also shared by all threads.)}  
\label{fig:multi_state}
\end{figure}

\begin{itemize}
\item When a new thread starts running, it will have its own new call
  stack.  It will have one frame on it, which is \emph{like} that thread's
  {\tt Main}, but it won't actually be {\tt Main}.
\item When a thread returns from its first method, it terminates.
\item Each thread has its own program counter and local variables, so
  there is no ``interference'' from other threads for these things.
  The way loops, calls, assignments to variables, exceptions,
  etc. work for each thread is just like you learned in sequential
  programming and is separate for each thread.
\item What is different is how static fields and objects work.  In
  sequential programming we know {\tt x.f=42; y = x.f; } always
  assigns {\tt 42} to the variable {\tt y}.  But now the object
  that {\tt x} refers to might also have its {\tt f} field written to
  by other threads, so we cannot be so sure.
\end{itemize}
In practice, even though all objects \emph{could} be shared among
threads, most are not.  In fact, just as having static fields
is often poor style, having lots of objects shared among threads is
often poor style.  But we need \emph{some} shared objects
because that is how threads communicate.  If we are going to create
parallel algorithms where helper threads run in parallel to compute
partial answers, they need some way to communicate those partial
answers back to the ``main'' thread.  The way we will do it is to
have the helper threads write to some object fields that the main
thread later reads.

We finish this section with some C\# specifics for exactly how to
create a new thread in C\#.  The details vary in different languages
and in fact the parallelism portion of these notes mostly uses a
different C\# library with slightly different specifics.  In addition
to creating threads, we will need other language constructs for
coordinating them.  For example, for one thread to read the result
another thread wrote as its answer, the reader often needs to know the
writer is done.  We will present such primitives as we need them.

To create a new thread in C\# requires that you define a new method
(step 1) and then perform two actions at run-time (steps 2--3):
\begin{enumerate}
\item Define a method that does the intended work and returns
  {\tt void}. It must take no arguments, but the example below shows
  how to work around this inconvenience. Let's call this method {\tt Run}.
\item Create an instance of the class {\tt System.Threading.Thread} and
  pass a defined method {\tt Run} as an argument. Notice that your method
  is an implementation of a delegate {\tt System.Threading.ThreadStart}
  which {\tt Thread} constructor expects. Also note that this does not
  yet create a running thread. It just creates an object of class {\tt Thread}. 
\item Call the {\tt Start} method of the object you created in step 2. This
  step does the “magic” creation of a new thread. That new thread will
  execute the method that you defined in step 1. Notice that you do not
  call the method {\tt Run} itself; that would just be an ordinary method
  call. You call {\tt Start}, which makes a new thread that runs it.
  The call to {\tt Start} “returns immediately” so the caller continues
  on, in parallel with the newly-created thread running your method.
  The new thread terminates when its execution completes.
\end{enumerate}
Here is a complete example of a useless C\# program that starts with one
thread and then creates 20 more threads:
\begin{verbatim}
class ExampleThread
{
    static void Run(int i)
    {
        Console.WriteLine("Thread {0} says hi", i);
        Console.WriteLine("Thread {0} says bye", i);
    }

    static void Main(string[] args)
    {
        for (int i = 1; i <= 20; i++)
        {
            int j = i;
            Thread t = new Thread(() => Run(j));
            t.Start();
        }
    }
}
\end{verbatim}
When this program runs, it will print 40 lines of output, one of which
is:
\begin{verbatim}
     Thread 13 says hi
\end{verbatim}
Interestingly, we cannot predict the order for these 40 lines of
output.  In fact, if you run the program multiple times, you will
probably see the output appear in different orders on different runs.
After all, each of the 21 separate threads running ``at the same
time'' (conceptually, since your machine may not have 21 processors
available for the program) can run in an unpredictable order.  The
main thread is the first thread and then it creates 20 others.  The
main thread always creates the other threads in the same
order, but it is up to the C\# implementation to let all the
threads ``take turns'' using the available processors.  There is no
guarantee that threads created earlier run earlier.  Therefore,
multithreaded programs are often \emph{nondeterministic}, meaning
their output can change even if the input does not.  This is a main
reason that multithreaded programs are more difficult to test and
debug.  Figure~\ref{fig:thread_output} shows two possible orders of
execution, but there are many, many more.

\begin{figure}
\begin{center}
\includegraphics[scale=1.0]{images/threaded_output.png}
\end{center}
\caption{Two possible outputs from running the program that creates 20
threads that each print two lines.}
\label{fig:thread_output}
\end{figure}

So is any possible ordering of the 40 output lines possible?  No.
Each thread still runs sequentially.  So we will always see
\verb|Thread 13 says hi|
\emph{before} the line
\verb|Thread 13 says bye|
even though there may be other lines in-between.
We might also wonder if two lines of output would ever be mixed,
something like:
\begin{verbatim}
     Thread 13 Thread says 14 says hi hi
\end{verbatim}
This is really a question of how the {\tt Console.WriteLine} method
handles concurrency and the answer happens to be that it will always
keep a line of output together, so this would not occur.  In general,
concurrency introduces new questions about how code should and does
behave.

We can also see how the example worked around the rule that {\tt Run}
cannot take any arguments. The standard idiom is to wrap a call to
{\tt Run} into a call of a different method that takes no arguments,
and use closure to pass any “arguments” for the new thread to {\tt Run}.

\subsection{Other Models}

While these notes focus on using threads and shared memory to handle
parallelism and concurrency, it would be misleading to suggest that
this is the only model for parallel/concurrent programming.  Shared
memory is often considered \emph{convenient} because communication
uses ``regular'' reads and writes of fields to objects.  However, it
is also considered \emph{error-prone} because communication is
implicit; it requires deep understanding of the code/documentation to
know which memory accesses are doing inter-thread communication and
which are not.  The definition of shared-memory programs is also much
more subtle than many programmers think because of issues regarding
\emph{data races}, as discussed in Section~\ref{sec:races}.  

Here are three well-known, popular alternatives to shared memory.  As
is common in computer science, no option is ``clearly better.''
Different models are best-suited to different problems, and any model
can be abused to produce incorrect or unnecessarily complicated
software.  One can also build abstractions using one model on top of
another model, or use multiple models in the same program.  These are
really different perspectives on how to describe parallel/concurrent
programs.

\medskip
\noindent\textbf{Message-passing} is the natural alternative to shared memory.
In this model, we have explicit threads, but they do not share
objects.  To communicate, there is a separate notion of a
\emph{message}, which sends a \emph{copy} of some data to its
recipient.  Since each thread has its own objects, we do not have to
worry about other threads wrongly updating fields.  But we do have to
keep track of different copies of things being produced by messages.
When processors are far apart, message passing is likely a more
natural fit, just like when you send email and a copy of the message
is sent to the recipient.  Here is a visual interpretation of message-passing:

\begin{center}
\medskip
\includegraphics[scale=0.4]{images/shared_memory_alternatives_1.png}
\end{center}


\medskip
\noindent\textbf{Dataflow} provides more structure than having ``a bunch of
threads that communicate with each other however they want.''
Instead, the programmer uses primitives to create a directed acyclic
graph (DAG).  A node in the graph performs some computation using
inputs that arrive on its incoming edges.  This data is provided by
other nodes along their outgoing edges.  A node starts computing when
all of its inputs are available, something the implementation keeps
track of automatically.  Here is a visual interpretation of dataflow
where different nodes perform different operations for some
computation, such as ``filter,'' ``fade in,'' and ``fade out:''

\begin{center}
\medskip
\includegraphics[scale=0.4]{images/shared_memory_alternatives_2.png}
\end{center}


\medskip
\noindent\textbf{Data parallelism} does not have explicit threads or nodes
running different parts of the program at different times.  Instead,
it has primitives for parallelism that involve applying the
\emph{same} operation to different pieces of data at the same time.
For example, you would have a primitive for applying some function to
every element of an array.  The implementation of this primitive would
use parallelism rather than a sequential for-loop.  Hence all the
parallelism is done for you provided you can express your program
using the available primitives.  Examples include vector instructions
on some processors and map-reduce style distributed systems.  Here is
a visual interpretation of data parallelism:

\begin{center}
\medskip
\includegraphics[scale=0.4]{images/shared_memory_alternatives_3.png}
\end{center}

\section{Basic Fork-Join Parallelism}
\label{sec:fork-join}

This section shows how to use threads and shared memory to implement
simple parallel algorithms.  The only synchronization primitive we
will need is {\tt join}, which causes one thread to wait until another
thread has terminated.  We begin with simple pseudocode and then show
how using threads in C\# to achieve the same idea requires a bit more
work (Section~\ref{sec:csharp-threads}).  We then argue that it is best
for parallel code to \emph{not} be written in terms of the number of
processors available (Section~\ref{sec:no-numprocs}) and show how to
use recursive divide-and-conquer instead
(Section~\ref{sec:divide-conquer}).  Because C\# threads are not
engineered for this style of programming, we switch to the .Net Task
Parallel Library which is designed for our needs
(Section~\ref{sec:dotnet-tpl}).  With all of this discussion in terms of
the single problem of summing an array of integers, we then turn to
other similar problems, introducing the terminology of \emph{maps} and
\emph{reduces} (Section~\ref{sec:map-reduce}) as well as data
structures other than arrays (Section~\ref{sec:beyond-arrays}).

\subsection{A Simple Example: Okay Idea, Inferior Style}
\label{sec:csharp-threads}

Most of this section will consider the problem of computing the sum of
an array of integers.  An $O(n)$ sequential solution to this problem
is trivial:
\begin{verbatim}
int Sum(int[] arr)
{
    int ans = 0;
    for (int i = 0; i < arr.Length; i++)
        ans += arr[i];
    return ans;
}
\end{verbatim}
If the array is large and we have extra processors available, we can
get a more efficient parallel algorithm.  Suppose we have 4
processors.  Then we could do the following:
\begin{itemize}
\item Use the first processor to sum the first 1/4 of the array and
  store the result somewhere.
\item Use the second processor to sum the second 1/4 of the array and
  store the result somewhere.
\item Use the third processor to sum the third 1/4 of the array and
  store the result somewhere.
\item Use the fourth processor to sum the fourth 1/4 of the array and
  store the result somewhere.
\item Add the 4 stored results and return that as the answer.
\end{itemize}
This algorithm is clearly correct provided that the last step is
started only after the previous four steps have completed.  
The first four steps can occur in parallel.  More generally,
if we have $P$ processors, we can divide the array into $P$ equal
segments and have an algorithm that runs in time $O(n/P + P)$ where
$n/P$ is for the parallel part and $P$ is for combining the stored
results.  Later we will see we can do better if $P$ is very large,
though that may be less of a practical concern.

\emph{In pseudocode}, a convenient way to write this kind of algorithm
is with a {\tt FORALL} loop.  A {\tt FORALL loop} is like a {\tt for}
loop except it does all the iterations in parallel.  Like a regular
{\tt for} loop, the code after a {\tt FORALL} loop does not execute
until the loop (i.e., all its iterations) are done.  Unlike the {\tt
  for} loop, the programmer is ``promising'' that all the iterations
can be done at the same time without them interfering with each
other.  Therefore, if one loop iteration writes to a location, then
another iteration must not read or write to that location.  However,
it is fine for two iterations to read the same location: that does not
cause any interference.

Here, then, is a pseudocode solution to using 4 processors to sum an
array.  Note it is essential that we store the 4 partial results in
separate locations to avoid any interference between loop
iterations.\footnote{We must take care to avoid bugs due to
  integer-division truncation with the arguments to {\tt SumRange}.
  We need to process each array element exactly once even if {\tt len} is
  not divisible by 4.  This code is correct; notice in particular that
  {\tt ((i+1)*len)/4} will always be {\tt len} when {\tt i==3} because
  {\tt 4*len} is divisible by 4.  Moreover, we could write {\tt
    (i+1)*len/4} since {\tt *} and {\tt /} have the same precedence
  and associate left-to-right.  But {\tt (i+1)*(len/4)} would
  \emph{not} be correct.  For the same reason, defining a variable
  {\tt int rangeSize = len/4} and using {\tt (i+1)*rangeSize} would
  \emph{not} be correct.}
\goodbreak
\begin{verbatim}
int Sum(int[] arr)
{
    results = new int[4];
    len = arr.Length;
    FORALL(i=0; i < 4; ++i)
    {
        results[i] = sumRange(arr,(i*len)/4,((i+1)*len)/4);
    }
    return results[0] + results[1] + results[2] + results[3];
}

int SumRange(int[] arr, int lo, int hi)
{
    result = 0;
    for(j=lo; j < hi; ++j)
        result += arr[j];
    return result;
}
\end{verbatim}

Unfortunately, C\# and most other general-purpose languages do not
have a {\tt FORALL} loop.  (C\# has various kinds of for-loops, but
all run all iterations on one thread.)  We can encode this programming pattern
explicitly using threads as follows:
\begin{enumerate}
\item In a regular {\tt for} loop, create one thread to do each
  iteration of our {\tt FORALL} loop, passing the data needed in the
  constructor.  Have the threads store their answers in fields of
  themselves.
\item Wait for all the threads created in step 1 to
  terminate.
\item Combine the results by reading the answers out of the fields of
  the threads created in step 1.
\end{enumerate}

To understand this pattern, we will first show a \emph{wrong}
version to get the idea.  That is a common technique in these notes
--- learning from wrong versions is extremely useful --- but wrong
versions are always clearly indicated.

Here is our WRONG attempt:
\begin{verbatim}
class SumRange
{
    int left;
    int right;
    int[] arr;
    public int Answer { get; private set; }

    public SumRange(int[] a, int left, int right)
    {
        this.left = left;
        this.right = right;
        this.arr = a;
        Answer = 0;
    }

    public void Run()
    {
        for (int i = left; i < right; i++)
        {
            Answer += arr[i];
        }
    }
}

public static int Sum(int[] arr)
{
    int len = arr.Length;
    int ans = 0;

    SumRange[] s = new SumRange[4];
    for (int i = 0; i < 4; i++)
    {
        SumRange sr = new SumRange(arr, (i * len) / 4, ((i + 1) * len) / 4);
        s[i] = sr;
        Thread t = new Thread(() => sr.Run());
        t.Start();
    }

    for (int i = 0; i < 4; i++)
    {
        ans += s[i].Answer;
    }

    return ans;
}
\end{verbatim}

The code above gets most of the pieces right.  The {\tt Sum} method
creates 4 instances of {\tt SumRange} and then creates instances of
{\tt System.Threading.Thread} to call {\tt Run} on each of them.
The {\tt SumRange} constructor takes as arguments the data that the
thread needs to do its job, in this case, the array and the range for
which this thread is responsible.  (We use a convenient convention that
ranges \emph{include} the low bound and \emph{exclude} the high
bound.)  The {\tt SumRange} constructor stores this data in fields of
the object so that the new thread has access to them in the {\tt Run}
method.

Notice each {\tt SumRange} object also has an {\tt Answer} property. 
This is shared memory for communicating the answer back from the helper 
thread to the main thread. So the main thread can sum the 4 {\tt Answer} 
properties from the threads it created to produce the final answer.

The bug in this code has to do with synchronization: The main thread
does not wait for the helper threads to finish before it sums the
{\tt Answer} properties.  Remember that {\tt Start} returns immediately ---
otherwise we would not get any parallelism.  So the {\tt Sum} method's
second for-loop probably starts running before the helper threads are
finished with their work.  Having one thread (the main thread) read a
field while another thread (the helper thread) is writing the same
field is a bug, and here it would produce a wrong (too-small)
answer.  We need to delay the second for-loop until the helper
threads are done.

There is a method in {\tt System.Threading.Thread} that is just what 
we need. If one thread, in our case the main thread, calls the {\tt Join} 
method of a {\tt System.Threading.Thread} object, in our case one of 
the helper threads, then this call blocks (i.e., does not return) unless/until 
the thread corresponding to the object has terminated. So we can add 
another for-loop to {\tt Sum} in-between the two loops already there 
to make sure all the helper threads finish before we add together the results:
\begin{verbatim}
for(int i=0; i < 4; i++)
   t[i].Join();
\end{verbatim}
Notice it is the main thread that is calling {\tt Join}, which takes
no arguments.  On the first loop iteration, the main thread will block
until the first helper thread is done.  On the second loop iteration,
the main thread will block until the second helper thread is done.  It
is certainly possible that the second helper thread actually finished
before the first thread.  This is not a problem: a call to {\tt Join}
when the helper thread has already terminated just returns right away
(no blocking).

Essentially, we are using two for-loops, where the first one creates
helper threads and the second one waits for them all to terminate, to
encode the idea of a FORALL-loop.  This style of parallel programming
is called ``fork-join parallelism.''  It is like we create a ``(4-way
in this case) fork in the road of execution'' and send each helper
thread down one path of the fork.  Then we join all the paths of the
fork back together and have the single main thread continue.
Fork-join parallelism can also be \emph{nested}, meaning one of the
helper threads forks its own helper threads.  In fact, we will soon
argue that this is better style.  The term ``join'' is common
in different programming languages and libraries, though honestly it
is not the most descriptive English word for the concept.

It is common to combine the joining for-loop and the
result-combining for-loop. Understanding why this is still correct
helps understand the {\tt Join} primitive.  So far we have suggested
writing code like this in our {\tt Sum} method:
\goodbreak
\begin{verbatim}
for(int i=0; i < 4; i++)
    t[i].Join();
for(int i=0; i < 4; i++)
    ans += s[i].Answer;
return ans;
\end{verbatim}
There is nothing wrong with the code above, but the following is also
correct:
\begin{verbatim}
for(int i=0; i < 4; i++)
{
    t[i].Join();
    ans += s[i].Answer;
}
return ans;
\end{verbatim}
Here we do not wait for all the helper threads to finish before we
start producing the final answer.  But we still ensure that the main
thread does not access a helper thread's {\tt Answer} property until at least that
helper thread has terminated.

Here, then, is a complete and correct program.  \footnote{Technically,
  for very large arrays, {\tt i*len} might be too large and we should
  declare one of these variables to have type {\tt long}.  We will
  ignore this detail throughout this chapter to avoid distractions,
  but {\tt long} is a wiser choice when dealing with arrays that may
  be very large.}  There is no change to
the {\tt SumRange} class.  This example shows many of the key
concepts of fork-join parallelism, but Section~\ref{sec:no-numprocs}
will explain why it is poor style and can lead to suboptimal
performance.  Sections~\ref{sec:divide-conquer} and~\ref{sec:dotnet-tpl}
will then present a similar but better approach.

\begin{verbatim}
class SumRange
{
    int left;
    int right;
    int[] arr;
    public int Answer { get; private set; }

    public SumRange(int[] a, int left, int right)
    {
        this.left = left;
        this.right = right;
        this.arr = a;
        Answer = 0;
    }

    public void Run()
    {
        for (int i = left; i < right; i++)
        {
            Answer += arr[i];
        }
    }
}

public static int Sum(int[] arr)
{
    int len = arr.Length;
    int ans = 0;

    SumRange[] s = new SumRange[4];
    Thread[] t = new Thread[4];
    for (int i = 0; i < 4; i++)
    {
        SumRange sr = new SumRange(arr, (i * len) / 4, ((i + 1) * len) / 4);
        s[i] = sr;
        t[i] = new Thread(sr.Run);
        t[i].Start();
    }

    for (int i = 0; i < 4; i++)
    {
        t[i].Join();
        ans += s[i].Answer;
    }

    return ans;
}
\end{verbatim}

\subsection{Why Not To Use One Thread Per Processor}
\label{sec:no-numprocs}

Having now presented a basic parallel algorithm, we will argue that
the approach the algorithm takes is poor style and likely to lead to
unnecessary inefficiency.  Do not despair: the concepts we have
learned like creating threads and using {\tt Join} will remain useful
--- and it was best to explain them using a too-simple approach.
Moreover, many parallel programs are written in pretty much exactly
this style, often because libraries like those in
Section~\ref{sec:dotnet-tpl} are unavailable.  Fortunately, such
libraries are now available on many platforms.

The problem with the previous approach was dividing the work into
exactly 4 pieces.  This approach assumes there are 4 processors
available to do the work (no other code needs them) and that each
processor is given approximately the same amount of work.  Sometimes
these assumptions may hold, but it would be better to use algorithms
that do not rely on such brittle assumptions.  The rest of this
section explains in more detail why these assumptions are unlikely to
hold and some partial solutions.  Section~\ref{sec:divide-conquer}
then describes the better solution that we advocate.

\medskip
\noindent{\bf Different computers have different numbers of processors}
\medskip

We want parallel programs that effectively use the processors
available to them.  Using exactly 4 threads is a horrible approach.
If 8 processors are available, half of them will sit idle and our
program will be no faster than with 4 processors.  If 3 processors are
available, our 4-thread program will take approximately twice as long
as with 4 processors. If 3 processors are available and we rewrite our
program to use 3 threads, then we will use resources effectively and
the result will only be about 33\% slower than when we had 4
processors and 4 threads.  (We will take 1/3 as much time as the
sequential version compared to 1/4 as much time.  And 1/3 is 33\%
slower than 1/4.)  But we do not want to have to edit our code every
time we run it on a computer with a different number of processors.

A natural solution is a core software-engineering principle you
should already know: Do not use constants where a variable is 
appropriate.  Our {\tt Sum} method can take as a parameter the number
of threads to use, leaving it to some other part of the program to
decide the number.  (There are C\# library methods to ask for the
number of processors on the computer, for example, but we argue next
that using that number is often unwise.)  It would look like this:
\begin{verbatim}
public static int Sum(int[] arr, int numThreads)
{
    int len = arr.Length;
    int ans = 0;

    SumRange[] s = new SumRange[numThreads];
    Thread[] t = new Thread[numThreads];
    for (int i = 0; i < numThreads; i++)
    {
        SumRange sr = new SumRange(arr, (i * len) / numThreads, ((i + 1) * len) / numThreads);
        s[i] = sr;
        t[i] = new Thread(sr.Run);
        t[i].Start();
    }

    for (int i = 0; i < numThreads; i++)
    {
        t[i].Join();
        ans += s[i].Answer;
    }

    return ans;
}
\end{verbatim}
Note that you need to be careful with integer division not to
introduce rounding errors when dividing the work.

\medskip
\noindent{\bf The processors available to part of the code can change}
\medskip

The second dubious assumption made so far is that every processor is
available to the code we are writing.  But some processors may be
needed by other programs or even other parts of the same program.  We
have parallelism after all --- maybe the caller to {\tt Sum} is
already part of some outer parallel algorithm.  The operating system
can reassign processors at any time, even when we are in the middle of
summing array elements.  It is fine to assume that the underlying C\#
implementation will try to use the available processors effectively,
but we should not assume 4 or even {\tt numThreads} processors will be
available from the beginning to the end of running our parallel algorithm.  

\medskip
\noindent{\bf We cannot always predictably divide the work into
  approximately equal pieces}
\medskip

In our {\tt Sum} example, it is quite likely that the threads
processing equal-size chunks of the array take approximately the same
amount of time.  They may not, due to memory-hierarchy issues or other
architectural effects, however.  Moreover, more sophisticated
algorithms could produce a large \emph{load imbalance}, meaning different
helper threads are given different amounts of work.  As a simple
example (perhaps too simple for it to actually matter), suppose we
have a large {\tt int[]} and we want to know how many elements of the
array are prime numbers.  If one portion of the array has more large
prime numbers than another, then one helper thread may take longer.

In short, giving each helper thread an equal number of data elements
is not necessarily the same as giving each helper thread an equal
amount of work.  And any load imbalance hurts our efficiency
since we need to wait until all threads are completed.

\medskip
\noindent{\bf A solution: Divide the work into smaller pieces}
\medskip

We outlined three problems above.  It turns out we can solve all three
with a perhaps counterintuitive strategy: \emph{Use substantially more
  threads than there are processors.}  For example, suppose to sum the
elements of an array we created one thread for each 1000 elements.
Assuming a large enough array (size greater than 1000 times the number of
processors), the threads will not all run at once since a processor
can run at most one thread at a time.  But this is fine: the system
will keep track of what threads are waiting and keep all the
processors busy.  There is some overhead to creating more threads, so
we should use a system where this overhead is small.

This approach clearly fixes the first problem: any number of
processors will stay busy until the very end when there are fewer
1000-element chunks remaining than there are processors.  It also
fixes the second problem since we just have a ``big pile'' of threads
waiting to run.  If the number of processors available changes, that
affects only how fast the pile is processed, but we are always doing
useful work with the resources available.  Lastly, this approach helps
with the load imbalance problem: Smaller chunks of work make load
imbalance far less likely since the threads do not run as long.  Also,
if one processor has a slow chunk, other processors can continue
processing faster chunks.

We can go back to our cutting-potatoes analogy to understand this
approach: Rather than give each of 4 cooks (processors), 1/4 of the
potatoes, we have them each take a moderate number of potatoes, slice
them, and then return to take another moderate number.  Since some
potatoes may take longer than others (they might be dirtier or have
more eyes), this approach is better balanced and is probably worth the
cost of the few extra trips to the pile of potatoes --- especially if
one of the cooks might take a break (processor used for a different
program) before finishing his/her pile.

Unfortunately, this approach still has two problems addressed in
Sections~\ref{sec:divide-conquer} and~\ref{sec:dotnet-tpl}.
\begin{enumerate}
\item We now have more results to combine.  Dividing the array into 4
  total pieces leaves $\Theta(1)$ results to combine.  Dividing the array
  into 1000-element chunks leaves {\tt arr.Length}/1000, which is
  $\Theta(n)$, results to combine.  Combining the results with a
  sequential for-loop produces an $\Theta(n)$ algorithm, albeit
  with a smaller constant factor.  To see the problem even more
  clearly, suppose we go to the extreme and use 1-element chunks ---
  now the results combining reimplements the original sequential
  algorithm.  In short, we need a better way to combine results.
\item C\# threads were not designed for small tasks like adding
  1000 numbers.  They will work and produce the correct answer 
  (only if setup correctly - details are mentioned later on), but
  the constant-factor overheads of creating a C\# thread are far too
  large.  A C\# program that creates 100,000 threads on a small
  desktop computer is unlikely to run well at all --- each thread just
  takes too much memory and the scheduler is overburdened and provides
  no asymptotic run-time guarantee.  In short, we need a different
  implementation of threads that is designed for this kind of
  fork/join programming.
\end{enumerate}

\subsection{Divide-And-Conquer Parallelism}
\label{sec:divide-conquer}

This section presents the idea of divide-and-conquer parallelism using
C\# threads.  Then Section~\ref{sec:dotnet-tpl} switches to using a
library where this programming style is actually efficient.  This
progression shows that we can understand all the ideas using the basic
notion of threads even though in practice we need a library that is
designed for this kind of programming.

The key idea is to \emph{change our algorithm} for summing the
elements of an array to use recursive divide-and-conquer.  To sum all
the array elements in some range from {\tt lo} to {\tt hi}, do the following:
\begin{enumerate}
\item If the range contains only one element, return that element
  as the sum.  Else in parallel:
  \begin{enumerate}
  \item Recursively sum the elements from {\tt lo} to the middle of the range.
  \item Recursively sum the elements from the middle of the range to {\tt hi}.
  \end{enumerate}
\item Add the two results from the previous step.
\end{enumerate}
The essence of the recursion is that steps 1a and 1b will themselves
use parallelism to divide the work of their halves in half again.  It
is the same divide-and-conquer recursive idea as you have seen in
algorithms like mergesort.  For sequential algorithms for simple
problems like summing an array, such fanciness is overkill.  But for
parallel algorithms, it is ideal.

As a small example (too small to actually want to use parallelism),
consider summing an array with 10 elements.  The algorithm
produces the following tree of recursion, where the range {\tt [i,j)}
includes {\tt i} and excludes {\tt j}:
\begin{verbatim}
Thread: sum range [0,10)
   Thread: sum range [0,5)
      Thread: sum range [0,2) 
         Thread: sum range [0,1) (return arr[0])
         Thread: sum range [1,2) (return arr[1])
         add results from two helper threads
      Thread: sum range [2,5)
         Thread: sum range [2,3) (return arr[2])
         Thread: sum range [3,5) 
            Thread: sum range [3,4) (return arr[3])
            Thread: sum range [4,5) (return arr[4])
            add results from two helper threads
         add results from two helper threads
      add results from two helper threads
   Thread: sum range [5,10)
      Thread: sum range [5,7)
         Thread: sum range [5,6) (return arr[5])
         Thread: sum range [6,7) (return arr[6])
         add results from two helper threads
      Thread: sum range [7,10)
         Thread: sum range [7,8) (return arr[7])
         Thread: sum range [8,10) 
            Thread: sum range [8,9) (return arr[8])
            Thread: sum range [9,10) (return arr[9])
            add results from two helper threads
         add results from two helper threads
      add results from two helper threads
   add results from two helper threads
\end{verbatim}
The total amount of work done by this algorithm is $O(n)$ because we
create approximately $2n$ threads and each thread either returns an
array element or adds together results from two helper threads it
created.  Much more interestingly, if we have $O(n)$ processors, then
this algorithm can run in $O(\log n)$ time, which is exponentially
faster than the sequential algorithm.  The key reason for the
improvement is that the algorithm is combining results in parallel.
The recursion forms a binary tree for summing subranges and the height
of this tree is $\log n$ for a range of size $n$.  See
Figure~\ref{fig:recursive_calls_binary_tree}, which shows the recursion in
a more conventional tree form where the number of nodes is growing
exponentially faster than the tree height.
With enough processors, the total running time corresponds to the 
tree \emph{height}, not the tree \emph{size}: this is the fundamental 
running-time benefit of parallelism.  Later sections will discuss why 
the problem of summing an array has such an efficient parallel algorithm; 
not every problem enjoys exponential improvement from parallelism.

\begin{figure}
\begin{center}
\includegraphics[scale=.6]{images/recursive_calls_binary_tree.png}
\end{center}
\caption{Recursion for summing an array where each node is a thread
  created by its parent.  For example (see shaded region), the thread
  responsible for summing the array elements from index 5 (inclusive)
  to index 10 (exclusive), creates two threads, one to sum the
  elements from index 5 to 7 and the other to sum the elements from
  index 7 to 10.}
\label{fig:recursive_calls_binary_tree}
\end{figure}

Having described the algorithm in English, seen an example, and
informally analyzed its running time, let us now consider an actual
implementation with C\# threads and then modify it with two important
improvements that affect only constant factors, but the constant
factors are large.  Then the next section will show the ``final''
version where we use the improvements and use a different library for
the threads.

To start, here is the algorithm directly translated into C\#,
omitting some boilerplate like putting the main {\tt Sum} method in a
class and handling exceptions.\footnote{This program may fail to compute the 
 correct result in default .NET settings. By default every thread in 32 
 bit app gets 1 Mb of memory in .NET. And the whole app gets 2 Gb. So 
 creating about 2000 threads can easily make app run out of memory. 
 The .NET Task Parallel Library introduced in Section~\ref{sec:dotnet-tpl} 
 does not have this problem.}
\begin{verbatim}
class SumRange
{
    int left;
    int right;
    int[] arr;
    public int Answer { get; private set; }

    public SumRange(int[] a, int l, int r)
    {
        left = l;
        right = r;
        arr = a;
        Answer = 0;
    }

    public void Run()
    {
        if (right - left == 1)
        {
            Answer = arr[left];
        }
        else
        {
            SumRange leftRange = new SumRange(arr, left, (left + right) / 2);
            SumRange rightRange = new SumRange(arr, (left + right) / 2, right);

            Thread leftThread = new Thread(leftRange.Run);
            Thread rightThread = new Thread(rightRange.Run);
            leftThread.Start();
            rightThread.Start();
            leftThread.Join();
            rightThread.Join();

            Answer = leftRange.Answer + rightRange.Answer;
        }
    }
}

public static int Sum(int[] arr)
{
    SumRange s = new SumRange(arr, 0, arr.Length);
    s.Run();
    return s.Answer;
}
\end{verbatim}
Notice how each thread creates two helper threads {\tt leftThread} and {\tt
  rightThread} and then waits for them to finish.  Crucially, the calls to
  {\tt leftThread.Start()} \emph{and} {\tt rightThread.Start()} precede the calls to
  {\tt leftThread.Join()} \emph{and} {\tt rightThread.Join()}.  If for example, {\tt
  leftThread.Join()} came before {\tt rightThread.Start()}, then the algorithm
  would have no effective parallelism whatsoever.  It would still
  produce the correct answer, but so would the original much simpler
  sequential program.
  
  In practice, code like this produces far too many threads to be
  efficient.  To add up four numbers, does it really make sense to
  create six new threads?  Therefore, implementations of fork/join
  algorithms invariably use a \emph{cutoff} below which they switch
  over to a sequential algorithm.  Because this cutoff is a constant,
  it has no effect on the asymptotic behavior of the algorithm.  What
  it does is eliminate the vast majority of the threads created, while
  still preserving enough parallelism to balance the load among the
  processors.

Here is code using a cutoff of 1000.  As you can see, using a cutoff
does not really complicate the code.
\begin{verbatim}
class SumRange
{
    static int Sequential_Cutoff = 100;
    int left;
    int right;
    int[] arr;
    public int Answer { get; private set; }

    public SumRange(int[] a, int l, int r)
    {
        left = l;
        right = r;
        arr = a;
        Answer = 0;
    }

    public void Run()
    {
        if (right - left < Sequential_Cutoff)
        {
            for (int i = left; i < right; i++)
            {
                Answer += arr[i];
            }
        }
        else
        {
            SumRange leftRange = new SumRange(arr, left, (left + right) / 2);
            SumRange rightRange = new SumRange(arr, (left + right) / 2, right);

            Thread leftThread = new Thread(leftRange.Run);
            Thread rightThread = new Thread(rightRange.Run);
            leftThread.Start();
            rightThread.Start();
            leftThread.Join();
            rightThread.Join();

            Answer = leftRange.Answer + rightRange.Answer;
        }
    }
}

public static int Sum(int[] arr)
{
    SumRange s = new SumRange(arr, 0, arr.Length);
    s.Run();
    return s.Answer;
}
\end{verbatim}

Using cut-offs is common in divide-and-conquer programming, even for
sequential algorithms.  For example, it is typical for quicksort to be
slower than an $O(n^2)$ sort like insertionsort for small arrays
($n < 10$ or so).  Therefore, it is common to have the recursive
quicksort switch over to insertionsort for small subproblems.  In
parallel programming, switching over to a sequential algorithm below a
cutoff is \emph{the exact same idea}.  In practice, the cutoffs are
usually larger, with numbers between 500 and 5000 being typical.

It is often worth doing some quick calculations to understand the
benefits of things like cutoffs.  Suppose we are summing an array with
$2^{30}$ elements.  Without a cutoff, we would use $2^{31}-1$, (i.e.,
two billion) threads.  With a cutoff of 1000, we would use
approximately $2^{21}$ (i.e., 2 million) threads since the last $10$
levels of the recursion would be eliminated.  Computing
$1-2^{21}/2^{31}$, we see we have eliminated $99.9\%$ of the threads.
Use cutoffs!

Our second improvement may seem anticlimactic compared to cutoffs
because it only reduces the number of threads by an additional
factor of two.  Nonetheless, it is worth seeing for efficiency
especially because the Task Parallel Library in the next section performs
poorly if you do not do this optimization ``by hand.''  The key is to
notice that all threads that create two helper threads are not doing
much work themselves: they divide the work in half, give it to two
helpers, wait for them to finish, and add the results.  Rather than
having all these threads wait around, it is more efficient to create
\emph{one helper thread} to do half the work and have the thread do
the other half \emph{itself}.  Modifying our code to do this is easy
since we can just call the {\tt Run} method directly, without passing 
it into "magic" {\tt Thread} object.
\begin{verbatim}
class SumRange
{
    static int Sequential_Cutoff = 100;
    int left;
    int right;
    int[] arr;
    public int Answer { get; private set; }

    public SumRange(int[] a, int l, int r)
    {
        left = l;
        right = r;
        arr = a;
        Answer = 0;
    }

    public void Run()
    {
        if (right - left < Sequential_Cutoff)
        {
            for (int i = left; i < right; i++)
            {
                Answer += arr[i];
            }
        }
        else
        {
            SumRange leftRange = new SumRange(arr, left, (left + right) / 2);
            SumRange rightRange = new SumRange(arr, (left + right) / 2, right);

            Thread leftThread = new Thread(leftRange.Run);
            leftThread.Start();
            rightRange.Run();
            leftThread.Join();

            Answer = leftRange.Answer + rightRange.Answer;
        }
    }
}

public static int Sum(int[] arr)
{
    SumRange s = new SumRange(arr, 0, arr.Length);
    s.Run();
    return s.Answer;
}
\end{verbatim}
Notice how the code above creates two {\tt SumRange} objects, but 
only creates one helper thread. It then does the right half of the 
work itself by calling {\tt rightRange.Run()}. There is
only one call to {\tt Join} because only one helper thread was
created.  The order here is still essential so that the two halves of
the work are done in parallel.  Creating a {\tt SumRange} object for
the right half and then calling {\tt Run} rather than creating a
thread may seem odd, but it keeps the code from getting more
complicated and still conveys the idea of dividing the work into two
similar parts that are done in parallel.

Unfortunately, even with these optimizations, the code above will run
poorly in practice, especially if given a large array.  The
implementation of C\# threads is not engineered for threads that do
such a small amount of work as adding 1000 numbers: it takes much
longer just to create, start running, and dispose of a thread.
The space overhead may also be prohibitive. In particular, it is not
uncommon for a C\# implementation to pre-allocate some amount of memory 
for the stack, which might be 1MB or more.  So
creating thousands of threads could use gigabytes of space.  Hence we
will switch to the library described in the next section for parallel
programming.  We will return to C\# threads when we learn
concurrency because the synchronization operations we will use work
with C\# threads.

\subsection{The .NET Task Parallel Library}
\label{sec:dotnet-tpl}

.NET 4 (and higher) includes classes in the {\tt System.Threading} and {\tt System.Threading.Tasks} namespaces
designed exactly for the kind of fine-grained fork-join parallel
computing these notes use.  In addition to supporting lightweight
threads (which the library calls \emph{tasks}) that are small enough
that even a million of them should not overwhelm the system, the
implementation includes a scheduler and run-time system with provably
optimal expected-time guarantees, as described in
Section~\ref{sec:analyze-fork-join}.  Similar libraries for other
languages include Intel's Thread Building Blocks, Java's ForkJoin Framework, 
and others.  The core ideas and
implementation techniques go back much further to the Cilk language,
an extension of C developed since 1994.

This section describes just a few practical details and library
specifics.  Compared to C\# threads, the core ideas are all the same,
but some of the method names and interfaces are different --- in
places more complicated and in others simpler.  Naturally, we give a
full example (actually two) for summing an array of numbers.  The
actual library contains many other useful features and classes, but we
will use only the primitives related to forking and joining,
implementing anything else we need ourselves.

We first show a full program that is as
much as possible like the version we wrote using C\# threads.  We
show a version using a sequential cut-off and only one helper thread
at each recursive subdivision though removing these important
improvements would be easy.  After discussing this version, we show a
second version that uses C\# generic types and a different library
class.  This second version is better style, but easier to understand
after the first version.

\medskip
\noindent FIRST VERSION (INFERIOR STYLE):

\begin{verbatim}
using System.Threading.Tasks;

public class DivideAndConquerTaskParallel
{
    class SumRange
    {
        static int Sequential_Cutoff = 100;
        int left;
        int right;
        int[] arr;
        public int Answer { get; private set; }

        public SumRange(int[] a, int l, int r)
        {
            left = l;
            right = r;
            arr = a;
            Answer = 0;
        }

        public void Run()
        {
            if (right - left < Sequential_Cutoff)
            {
                for (int i = left; i < right; i++)
                {
                    Answer += arr[i];
                }
            }
            else
            {
                SumRange leftRange = new SumRange(arr, left, (left + right) / 2);
                SumRange rightRange = new SumRange(arr, (left + right) / 2, right);

                Task leftTask = Task.Factory.StartNew(leftRange.Run);
                rightRange.Run();
                leftTask.Wait();

                Answer = leftRange.Answer + rightRange.Answer;
            }
        }
    }

    public static int Sum(int[] arr)
    {
        SumRange s = new SumRange(arr, 0, arr.Length);
        s.Run();
        return s.Answer;
    }
}
\end{verbatim}

There are some differences compared to using C\# threads, but 
the overall structure of the algorithm should look similar. 
Furthermore, most of the changes are just different names for classes and methods:
\begin{itemize}
\item Class {\tt System.Threading.Tasks.Task} instead of {\tt System.Threading.Thread}.
\item Parallelism starts with static method call {\tt Task.Factory.StartNew}.
\item Method for waiting another task's finish is now called {\tt Wait} instead of {\tt Join}.
\end{itemize}

Such details as starting a new task with {\tt System.Threading.Thread} 
are there because the library is not built into the C\# language, so 
we have to do a little extra to use it. What you really need to know is 
that {\tt Task} instances should not be created explicitly, but rather 
by the tasks factory, so that the library could deal with underlying 
details such as the partitioning of the work, the scheduling of threads 
on the {\tt ThreadPool}, cancellation support, state management, and 
other low-level details. Otherwise {\tt Task} is very similar to {\tt Thread}, 
with {\tt Start} and {\tt Wait} being used just as we used {\tt Start} 
and {\tt Join} before.

We will present one final version of our array-summing program to demonstrate 
one more aspect of TPL that you should use as a matter of style. The {\tt Task} 
class is best only when the subcomputations do not produce a result, whereas 
in our example they do: the sum of the range. It is quite common not to produce 
a result, for example a parallel program that increments every element of an 
array. So far, the way we have “returned” results is via a property, 
which we called {\tt Answer}.

Instead, we can use generic class {\tt Task<T>} instead of {\tt Task}. The type 
parameter here is the type of value that passed delegate should return. 
Here is the full version of the code using this more convenient and less 
error-prone class, followed by an explanation:

\medskip
\noindent FINAL, BETTER VERSION:

\begin{verbatim}
using System.Threading.Tasks;

public class DivideAndConquerTaskParallelResult
{
    class SumRange
    {
        static int Sequential_Cutoff = 100;
        int left;
        int right;
        int[] arr;

        public SumRange(int[] a, int l, int r)
        {
            left = l;
            right = r;
            arr = a;
        }

        public int Run()
        {
            if (right - left < Sequential_Cutoff)
            {
                int ans = 0;
                for (int i = left; i < right; i++)
                {
                    ans += i;
                }
                return ans;
            }
            else
            {
                SumRange leftRange = new SumRange(arr, left, (left + right) / 2);
                SumRange rightRange = new SumRange(arr, (left + right) / 2, right);

                Task<int> leftTask = Task.Factory.StartNew<int>(leftRange.Run);
                int rightAns = rightRange.Run();
                leftTask.Wait();
                int leftAns = leftTask.Result;

                return leftAns + rightAns;
            }
        }
    }

    public static int Sum(int[] arr)
    {
        SumRange s = new SumRange(arr, 0, arr.Length);
        return s.Run();
    }
}
\end{verbatim}

Here are the differences from the version that uses non-generic {\tt
  Task}:
\begin{itemize}
\item {\tt Answer} property is gone.
\item {\tt Run} returns an integer as a result of computation.
\item We use {tt Task$<$int$>$} instead of {\tt Task}.
\item Tasks now have an additional property {\tt Result} which we 
 use to get result of a task run.
\end{itemize}

If you are familiar with C\# generic types, this use of them should
not be particularly perplexing.  The library is also using static
overloading for the {\tt StartNew} method.  But as \emph{users} of the
library, it suffices just to follow the pattern in the example above.

\subsection{Reductions and Maps}
\label{sec:map-reduce}

It may seem that given all the work we did to implement something as
conceptually simple as summing an array that fork/join programming is
too complicated.  To the contrary, it turns out that many, many
problems can be solved very much like we solved this one.  Just like
regular for-loops took some getting used to when you started
programming but now you can recognize exactly what kind of loop you
need for all sorts of problems, divide-and-conquer parallelism 
often follows a small number of patterns.  Once you know the patterns,
most of your programs are largely the same.

For example, here are several problems for which efficient parallel
algorithms look almost identical to summing an array:
\begin{itemize}
\item Count how many array elements satisfy some property (e.g., how
  many elements are the number 42).
\item Find the maximum or minimum element of an array.
\item Given an array of strings, compute the sum (or max, or min) of
  all their lengths.
\item Find the left-most array index that has an element satisfying
  some property.
\end{itemize}
Compared to summing an array, all that changes is the base case for
the recursion and how we combine results.  For example, to find the
index of the leftmost 42 in an array of length $n$, we can do the
following (where a final result of $n$ means the array does not hold a
42):
\begin{itemize}
\item For the base case, return {\tt lo} if {\tt arr[lo]} holds 42 and
  {\tt n} otherwise.
\item To combine results, return the smaller number.
\end{itemize}
Implement one or two of these problems to convince yourself they are not any
harder than what we have already done.  Or come up with additional
problems that can be solved the same way.

Problems that have this form are so common that there is a
name for them: \emph{reductions}, which you can remember by
realizing that we take a collection of data items (in an array) and
\emph{reduce} the information down to a single result.  As we have
seen, the way reductions can work in parallel is to compute answers
for the two halves recursively and in parallel and then merge these to
produce a result.  

However, we should be clear that \emph{not every problem over an array
  of data can be solved with a simple parallel reduction}.  To avoid
  getting into arcane problems, let's just describe a general
  situation.  Suppose you have sequential code like this:
\begin{verbatim}
interface BinaryOperation<T>
{
    T M(T x, T y);
}

class C<T>
{
    T Fold(T[] arr, BinaryOperation<T> binop, T initialValue)
    {
        T ans = initialValue;
        for (int i = 0; i < arr.Length; i++)
            ans = binop.M(ans, arr[i]);
        return ans;
    }
}
\end{verbatim}
The name {\tt Fold} is conventional for this sort of algorithm.  The
idea is to start with {\tt initialValue} and keep updating the
``answer so far'' by applying some binary function {\tt M} to the
current answer and the next element of the array.

Without any additional information about what {\tt M} computes,
this algorithm cannot be effectively parallelized since we cannot
process {\tt arr[i]} until we know the answer from the first {\tt i-1}
iterations of the for-loop.  For a more humorous example of a
procedure that cannot be sped up given additional resources:
9 women can't make a baby in 1 month.

So what do we have to know about the {\tt BinaryOperation} above in
order to use a parallel reduction?  It turns out all we need is that
the operation is \emph{associative}, meaning for all $a$, $b$, and
$c$,\ \ $m(a,m(b,c))$ is the same as $m(m(a,b),c)$.  Our
array-summing algorithm is correct because $a+(b+c)=(a+b)+c$.  Our
find-the-leftmost-index-holding 42 algorithm is correct because
\emph{min} is also an associative operator.

Because reductions using associative operators are so common, we could
write one generic algorithm that took the operator, and what to do for
a base case, as arguments.  This is an example of higher-order
programming, and the Task Parallel Library has several classes providing
this sort of functionality.  Higher-order programming has many, many
advantages (see the end of this section for a popular one), but when first
\emph{learning} a programming pattern, it is often useful to ``code it
up yourself'' a few times.  For that reason, we encourage writing your
parallel reductions manually in order to see the parallel
divide-and-conquer, even though they all really look the same.

Parallel reductions are not the only common pattern in parallel
programming.  An even simpler one, which we did not start with because
it is just so easy, is a parallel \emph{map}.  A map performs an
operation on each input element independently; given an array of inputs,
it produces an array of outputs of the same length.  A simple example
would be multiplying every element of an array by 2.  An example using
two inputs and producing a separate output would be vector addition.
Using pseudocode, we could write:
\begin{verbatim}
int[] Add(int[] arr1, int[] arr2)
{
  assert(arr1.Length == arr2.Length);
  int[] ans = new int[arr1.Length];
  FORALL(int i=0; i < arr1.Length; i++)
    ans[i] = arr1[i] + arr2[i];
  return ans;
}
\end{verbatim}
Coding up this algorithm in the Task Parallel Library is straightforward:
Have the main thread create the {\tt ans} array and pass it before starting the
parallel divide-and-conquer.  Each thread object will have a reference
to this array but will assign to different portions of it.  Because
there are no other results to combine, using {\tt
  Task} is appropriate.  Using a sequential cut-off and
creating only one new thread for each recursive subdivision of the
problem remain important --- these ideas are more general than the
particular programming pattern of a map or a reduce.

Recognizing problems that are fundamentally maps and/or reduces over
large data collections is a valuable skill that allows efficient
parallelization.  In fact, it is one of the key ideas behind Google's
MapReduce framework and the open-source variant Hadoop.  In these
systems, the programmer just writes the operations that describe how
to map data (e.g., ``multiply by 2'') and reduce data (e.g., ``take
the minimum'').  The system then does all the parallelization, often
using hundreds or thousands of computers to process gigabytes or
terabytes of data.  For this to work, the programmer must provide
operations that have no side effects (since the order they occur
is unspecified) and reduce operations that are associative (as we
discussed).  As parallel programmers, it is often enough to ``write
down the maps and reduces'' --- leaving it to systems like the
Task Parallel Library or Hadoop to do the actual scheduling of the
parallelism.

\subsection{Data Structures Besides Arrays}
\label{sec:beyond-arrays}

So far we have considered only algorithms over one-dimensional arrays.
Naturally, one can write parallel algorithms over any data structure,
but divide-and-conquer parallelism requires that we can efficiently
(ideally in $O(1)$ time) divide the problem into smaller pieces.  For
arrays, dividing the problem involves only $O(1)$ arithmetic on
indices, so this works well.  

While arrays are the most common data structure in parallel
programming, balanced trees, such as AVL trees or B trees, also
support parallel algorithms well.  For example, with a binary tree, we
can fork to process the left child and right child of each node in
parallel.  For good sequential cut-offs, it helps to have stored at
each tree node the number of descendants of the node, something easy
to maintain.  However, for trees with guaranteed balance properties,
other information --- like the height of an AVL tree node --- should
suffice.

Certain tree problems will not run faster with parallelism.  For
example, searching for an element in a balanced binary search tree
takes $O(\log n)$ time with or without parallelism.  However, maps and
reduces over balanced trees benefit from parallelism.  For example,
summing the elements of a binary tree takes $O(n)$ time sequentially
where $n$ is the number of elements, but with a sufficiently large number of
processors, the time is $O(h)$, where $h$ is the height of the tree.
Hence, tree balance is even more important with parallel programming:
for a balanced tree $h=\Theta(\log n)$ compared to the worst case
$h=\Theta(n)$.

For the same reason, parallel algorithms over regular linked lists are
typically poor.  Any problem that requires reading all $n$ elements of
a linked list takes time $\Omega(n)$ regardless of how many processors
are available.  (Fancier list data structures like skip lists are
better for exactly this reason --- you can get to all the data in
$O(\log n)$ time.)  Streams of input data, such as from files,
typically have the same limitation: it takes linear time to read the
input and this can be the bottleneck for the algorithm.

There can still be benefit to parallelism with such ``inherently
sequential'' data structures and input streams.  Suppose we had a map
operation over a list but each operation was itself an expensive
computation (e.g., decrypting a significant piece of data).  If each
map operation took time $O(x)$ and the list had length $n$, doing each
operation in a separate thread (assuming, again, no limit on the
number of processors) would produce an $O(x+n)$ algorithm compared to
the sequential $O(xn)$ algorithm.  But for simple operations like
summing or finding a maximum element, there would be no benefit.

\section{Analyzing Fork-Join Algorithms}
\label{sec:analyze-fork-join}

As with any algorithm, a fork-join parallel algorithm should be
correct and efficient.  This section focuses on the latter even though
the former should always be one's first concern.  For efficiency, we
will focus on asymptotic bounds and analyzing algorithms that are not
written in terms of a fixed number of processors.  That is, just as
the size of the problem $n$ will factor into the asymptotic running
time, so will the number of processors $P$.  The ForkJoin framework
(and similar libraries in other languages) will give us an optimal
expected-time bound for any $P$.  This section explains what that
bound is and what it means, but we will not discuss \emph{how} the
framework achieves it.  

We then turn to discussing Amdahl's Law, which analyzes the running
time of algorithms that have both sequential parts and parallel parts.
The key and depressing upshot is that programs with even a small
sequential part quickly stop getting much benefit from running with
more processors.

Finally, we discuss Moore's ``Law'' in contrast to Amdahl's Law.
While Moore's Law is also important for understanding the progress of
computing power, it is not a mathematical theorem like Amdahl's Law.

\subsection{Work and Span}

\subsubsection{Defining Work and Span}

We define $T_P$ to be the time a program/algorithm takes to run if
there are $P$ processors available during its execution.  For example,
if a program was the only one running on a quad-core machine, we would
be particularly interested in $T_4$, but we want to think about $T_P$
more generally.  It turns out we will reason about the general $T_P$
in terms of $T_1$ and $T_\infty$:

\begin{itemize}
\item $T_1$ is called the \emph{work}.  By definition, this is how
  long it takes to run on one processor.  More intuitively, it is just
  the total of all the running time of all the pieces of the
  algorithm: we have to do all the work before we are done, and there
  is exactly one processor (no parallelism) to do it.  In terms of
  fork-join, we can think of $T_1$ as doing one side of the fork and
  then the other, though the total $T_1$ does not depend on how the
  work is scheduled.
\item $T_\infty$ is called the \emph{span}, though other common terms
  are the \emph{critical path length} or \emph{computational depth}.
  By definition, this is how long it takes to run on an unlimited
  number of processors.  Notice this is \emph{not} necessarily $O(1)$
  time; the algorithm still needs to do the forking and combining of
  results.  For example, under our model of computation --- where
  creating a new thread and adding two numbers are both $O(1)$
  operations --- the algorithm we developed is asymptotically optimal
  with $T_\infty=\Theta(\log n)$ for an array of length $n$.
\end{itemize}

We need a more precise way of characterizing the execution of a
parallel program so that we can describe and compute the work, $T_1$,
and the span, $T_\infty$.  We will describe a program execution as a
directed acyclic graph (dag) where:
\begin{itemize}
\item Nodes are pieces of work the program performs.  Each node will
  be a constant, i.e., $O(1)$, amount of work that is performed
  sequentially.  So $T_1$ is asymptotically just the number of nodes
  in the dag.
\item Edges represent that the source node must complete before the
  target node begins.  That is, there is a \emph{computational
  dependency} along the edge.  This idea lets us visualize $T_\infty$:
  With unlimited processors, we would immediately start every node as
  soon as its predecessors in the graph had finished.  Therefore
  $T_\infty$ is just the length of the longest path in the dag.
\end{itemize}
Figure~\ref{fig:tinf_in_a_dag} shows an example dag and the longest
path, which determines $T_\infty$.
\begin{figure}
\begin{center}
\includegraphics[scale=.6]{images/tinf_in_a_dag.png}
\end{center}
\caption{An example dag and the path (see thicker blue arrows) that determines its span.}
\label{fig:tinf_in_a_dag}
\end{figure}

If you have studied combinational hardware circuits, this model is
strikingly similar to the dags that arise in that setting.  For
circuits, \emph{work} is typically called the \emph{size} of the circuit,
(i.e., the amount of hardware)
and \emph{span} is typically called the \emph{depth} of the circuit,
(i.e., the time, in units of ``gate delay,'' to produce an answer).

With basic fork-join divide-and-conquer parallelism, the execution
dags are quite simple: The $O(1)$ work to set up two smaller
subproblems is one node in the dag.  This node has two outgoing edges
to two new nodes that start doing the two subproblems.  (The fact that
one subproblem might be done by the same thread is not relevant here.
Nodes are not threads. They are $O(1)$ pieces of work.)  The two
subproblems will lead to their own dags.  When we join on the results
of the subproblems, that creates a node with incoming edges from the
last nodes for the subproblems.  This same node can do an $O(1)$
amount of work to combine the results.  (If combining results is more
expensive, then it needs to be represented by more nodes.)

Overall, then, the dag for a basic parallel reduction would look like
this:

\begin{center}
\includegraphics[scale=.45]{images/forkjoindag.jpg}
\end{center}

The root node represents the computation that divides the array into
two equal halves.  The bottom node represents the computation that
adds together the two sums from the halves to produce the final
answer.  The base cases represent reading from a one-element range
assuming no sequential cut-off.  A sequential cut-off ``just'' trims
out levels of the dag, which removes most of the nodes but affects the
dag's longest path by ``only'' a constant amount.  Note that this dag
is a conceptual description of how a program executes; the dag is not
a data structure that gets built by the program.

From the picture, it is clear that a parallel reduction is basically
described by two balanced binary trees whose size is proportional to
the input data size.  Therefore $T_1$ is $O(n)$ (there are
approximately $2n$ nodes) and $T_\infty$ is $O(\log n)$ (the height of
each tree is approximately $\log n$).  For the particular reduction we
have been studying --- summing an array --- Figure~\ref{fig:array_sum}
visually depicts the work being done for an example with 8 elements.
The work in the nodes in the top half is to create two subproblems.
The work in the nodes in the bottom half is to combine two results.
\begin{figure}
\begin{center}
\includegraphics[scale=.6]{images/array_sum.png}
\end{center}
\caption{Example execution dag for summing an array.}
\label{fig:array_sum}
\end{figure}


The dag model of parallel computation is much more general than for
simple fork-join algorithms.  It describes all the work that is done
and the earliest that any piece of that work could begin.  To repeat,
$T_1$ and $T_\infty$ become simple graph properties: the number of
nodes and the length of the longest path, respectively.

\subsubsection{Defining Speedup and Parallelism}

Having defined work and span, we can use them to define some other
terms more relevant to our real goal of reasoning about $T_P$.  After
all, if we had only one processor then we would not study parallelism
and having infinity processors is impossible.

We define the \emph{speedup} on $P$ processors to be $T_1/T_P$.  It is
basically the ratio of how much faster the program runs given the
extra processors.  For example, if $T_1$ is 20 seconds and $T_4$ is 8
seconds, then the speedup for $P=4$ is 2.5.  

You might naively expect a speed-up of 4, or more generally $P$ for
$T_P$.  In practice, such a \emph{perfect speedup} is rare due to
several issues including the overhead of creating threads and
communicating answers among them, memory-hierarchy issues, and the
inherent computational dependencies related to the span.  In the rare
case that doubling $P$ cuts the running time in half (i.e., doubles
the speedup), we call it \emph{perfect linear speedup}.  In practice,
this is not the absolute limit; one can find situations
where the speedup is even higher even though our simple computational
model does not capture the features that could cause this.  

It is important to note that reporting only $T_1/T_P$ can be
``dishonest'' in the sense that it often overstates the advantages of
using multiple processors.  The reason is that $T_1$ is the time it
takes to run the \emph{parallel algorithm} on one processor, but this
algorithm is likely to be much slower than an algorithm designed
sequentially.  For example, if someone wants to know the benefits of
summing an array with parallel fork-join, they probably are most
interested in comparing $T_P$ to the time for the sequential for-loop.
If we call the latter $S$, then the ratio $S/T_P$ is usually the
speed-up of interest and will be lower, due to constant factors like
the time to create recursive tasks, than the definition of speed-up
$T_1/T_P$.  One measure of the overhead of using multiple threads is
simply $T_1/S$, which is usually greater than 1.

As a final definition, we call $T_1/T_\infty$ the \emph{parallelism}
of an algorithm.  It is a measure of how much improvement one could
possibly hope for since it should be at least as great as the speed-up
for any $P$.  For our parallel reductions where the work is $\Theta(n)$ and
the span is $\Theta(\log n)$, the parallelism is $\Theta(n/\log n)$.  In
other words, there is exponential available parallelism ($n$ grows
exponentially faster than $\log n$), meaning with enough processors we
can hope for an exponential speed-up over the sequential version.

\subsubsection{The Task Parallel Library Bound}

Under some important assumptions we will describe below, algorithms
written using the Task Parallel Library, in particular the
divide-and-conquer algorithms in these notes, have the following
\emph{expected} time bound:

\[ T_P\ \ \mbox{is}\ \ O(T_1/P + T_\infty) \]

The bound is \emph{expected} because internally the library uses
randomness, so the bound can be violated from ``bad luck'' but such
``bad luck'' is exponentially unlikely, so it simply will not occur in
practice.  This is exactly like the expected-time running-time
guarantee for the sequential quicksort algorithm when a pivot element
is chosen randomly. Because these notes do not describe the
library's implementation, we will not see where the randomness
arises.

Notice that, ignoring constant factors, this bound is optimal:
Given only $P$ processors, no code can expect to do better than
$T_1/P$ or better than $T_\infty$.  For small $P$, the term $T_1/P$ is
likely to be dominant and we can expect roughly linear speed-up.  As
$P$ grows, the span becomes more relevant and the limit on the
run-time is more influenced by $T_\infty$.

Constant factors can be relevant, and it is entirely possible that a
hand-crafted parallel algorithm in terms of some fixed $P$ could do
better than a generic library that has no idea what sort of parallel
algorithm it is running.  But just like we often use asymptotically
optimal data structures even if hand-crafted ones for our task might
be a little faster, using a library such as this is often an excellent
approach.

Thinking in terms of the program-execution dag, it is rather amazing
that a library can achieve this optimal result.  While the program is
running, it is the framework's job to choose among all the threads
that \emph{could} run next (they are not blocked waiting for some
other thread to finish) and assign $P$ of them to processors.  For
simple parallel reductions, the choice hardly matters because all
paths to the bottom of the dag are about the same length, but for
arbitrary dags it seems important to work on the longer paths.  Yet it
turns out a much greedier algorithm that just picks randomly among the
available threads will do only a constant factor worse.  But this is
all about the library's internal scheduling algorithm (which is not
actually totally random) and we, as library users, can just rely on
the provided bound.

However, as mentioned above, the bound holds only under a couple
assumptions.  The first is that all the threads you create to do
subproblems do approximately the same amount of work.  Otherwise, if a
thread with much-more-work-to-do is scheduled very late, other
processors will sit idle waiting for this laggard to finish.  The
second is that all the threads do a small but not tiny amount of work.
This again helps with load balancing. In other words, just avoid
threads that do millions of operations as well as threads that do
dozens.

To summarize, as a user of a library like this, your job is to pick a
good parallel algorithm, implement it in terms of divide-and-conquer
with a reasonable sequential cut-off, and analyze the expected
run-time in terms of the provided bound.  The library's job is to give
this bound while trying to maintain low constant-factor overheads.
While this library is particularly good for \emph{this} style of
programming, this basic division is common: application writers
develop good algorithms and rely on some underlying \emph{thread scheduler}
 to deliver reasonable performance.

\subsection{Amdahl's Law}

So far we have analyzed the running time of a parallel algorithm.
While a parallel algorithm could have some ``sequential parts''
(a part of the dag where there is a long linear sequence of nodes), it
is common to think of an execution in terms of some entirely parallel
parts (e.g., maps and reductions) and some entirely sequential parts.
The sequential parts could simply be algorithms that have not been
parallelized or they could be inherently sequential, like reading in
input.  As this section shows, even a little bit of sequential work in
your program drastically reduces the speed-up once you have a
significant number of processors.  

This result is really a matter of very basic algebra.  It is named
after Gene Amdahl, who first articulated it.  Though almost all computer
scientists learn it and understand it, it is all too common to forget
its implications.  It is, perhaps, counterintuitive that just a little
non-parallelism has such a drastic limit on speed-up.  But it's a
fact, so learn and remember Amdahl's Law!

With that introduction, here is the full derivation of Amdahl's Law:
Suppose the work $T_1$ is 1, i.e., the total program execution time on
one processor is 1 ``unit time.''  Let $S$ be the portion of the
execution that cannot be parallelized and assume the rest of the
execution ($1-S$) gets perfect linear speed-up on $P$ processors for
any $P$.  Notice this is a charitable assumption about the parallel
part equivalent to assuming the span is $O(1)$.  Then:

\[ T_1 = S + (1-S) = 1 \]
\[ T_P = S + (1-S)/P \]

Notice all we have assumed is that the parallel portion $(1-S)$ runs
in time $(1-S)/P$.  Then the speed-up, by definition is:

\[ \mbox{Amdahl's Law:}\ \ \ T_1/T_P = 1/(S+(1-S)/P) \]

As a corollary, the parallelism is just the simplified equation as $P$
goes to $\infty$:

\[ T_1/T_\infty = 1/S \]

The equations may look innocuous until you start plugging in values.
For example, if 33\% of a program is sequential, then a billion
processors can achieve a speed-up of at most 3.  That is just common
sense: they cannot speed-up 1/3 of the program, so even if the rest of
the program runs ``instantly'' the speed-up is only 3.

The ``problem'' is when we expect to get twice the performance from
twice the computational resources.  If those extra resources are
processors, this works only if most of the execution time is still
running parallelizable code.  Adding a second or third processor can
often provide significant speed-up, but as the number of processors
grows, the benefit quickly diminishes.

Recall that from 1980--2005 the processing speed of desktop computers
doubled approximately every 18 months.  Therefore, 12 years or so was
long enough to buy a new computer and have it run an old program 100
times faster.  Now suppose that instead in 12 years we have 256
processors rather than 1 but all the processors have the same speed.
What percentage of a program would have to be perfectly parallelizable
in order to get a speed-up of 100?  Perhaps a speed-up of 100 given
256 cores seems easy?  Plugging into Amdahl's Law, we need:

\[ 100 \leq 1 / (S + (1-S)/256) \]

Solving for $S$ reveals that at most $0.61\%$ of the program can be
sequential.

Given depressing results like these --- and there are many, hopefully
you will draw some possible-speedup plots as homework exercises ---
it is tempting to give up on parallelism as a means to performance
improvement.  While you should never forget Amdahl's Law, you should
also not entirely despair.  Parallelism does provide real speed-up for
performance-critical parts of programs.  You just do not get to
speed-up \emph{all} of your code by buying a faster computer.  More
specifically, there are two common workarounds to the fact-of-life
that is Amdahl's Law:

\begin{enumerate}
\item We can find new parallel algorithms.  Given enough processors,
  it is worth parallelizing something (reducing span) even if it means
  more total computation (increasing work).  Amdahl's Law says that as
  the number of processors grows, span is more important than work.
  This is often described as ``scalability matters more than
  performance'' where scalability means can-use-more-processors and
  performance means run-time on a small-number-of-processors.  In
  short, large amounts of parallelism can change your algorithmic
  choices.
\item We can use the parallelism to solve new or bigger problems
  rather than solving the same problem faster.  For example, suppose
  the parallel part of a program is $O(n^2)$ and the sequential part
  is $O(n)$.  As we increase the number of processors, we can increase
  $n$ with only a small increase in the running time.  One area where
  parallelism is very successful is computer graphics (animated
  movies, video games, etc.).  Compared to years ago, it is not so
  much that computers are rendering the same scenes faster; it is that
  they are rendering more impressive scenes with more pixels and more
  accurate images.  In short, parallelism can enable new things
  (provided those things are parallelizable of course) even if the old
  things are limited by Amdahl's Law.
\end{enumerate}

\subsection{Comparing Amdahl's Law and Moore's Law}

There is another ``Law'' relevant to computing speed and the number of
processors available on a chip.  Moore's Law, again named after its
inventor, Gordon Moore, states that the number of transistors per unit area on a
chip doubles roughly every 18 months.  That increased transistor
density used to lead to faster processors; now it is leading to more
processors.

Moore's Law is an observation about the semiconductor industry that
has held for decades.  The fact that it has held for so long is
entirely about empirical evidence --- people look at the chips that
are sold and see that they obey this law.  Actually, for many years it
has been a self-fulfilling prophecy: chip manufacturers expect
themselves to continue Moore's Law and they find a way to achieve
technological innovation at this pace.  There is no inherent
mathematical theorem underlying it.  Yet we expect the number of
processors to increase exponentially for the foreseeable future.

On the other hand, Amdahl's Law is an irrefutable fact of algebra.

\section{Fancier Fork-Join Algorithms: Prefix, Pack, Sort}
\label{sec:fancy-parallel}

This section presents a few more sophisticated parallel algorithms.
The intention is to demonstrate (a) sometimes problems that seem
inherently sequential turn out to have efficient parallel
algorithms, (b) we can use parallel-algorithm techniques as
building blocks for other larger parallel algorithms, and (c) we can
use asymptotic complexity to help decide when one parallel algorithm
is better than another.  The study of parallel algorithms
could take an entire course, so we will pick just a few examples
that represent some of the many key parallel-programming patterns.

As is common when studying algorithms, we will not show full C\#
implementations.  It should be clear at this point that one could code
up the algorithms using the Task Parallel Library even if it may not be
entirely easy to implement more sophisticated techniques.

\subsection{Parallel-Prefix Sum}

Consider this problem: Given an array of $n$ integers {\tt input},
produce an array of $n$ integers {\tt output} where {\tt output[i]} is
the sum of the first {\tt i} elements of {\tt input}.  In other words,
we are computing the sum of \emph{every} prefix of the input array and
returning all the results.  This is called the \emph{prefix-sum
  problem}.\footnote{It is common to distinguish the
  inclusive-sum (the first {\tt i} elements) from the exclusive-sum
  (the first {\tt i-1} elements); we will assume inclusive sums are
  desired.}  Figure~\ref{fig:prefix_sum_output} shows an example
input and output.
A $\Theta(n)$ sequential solution is trivial:

\begin{verbatim}
int[] PrefixSum(int[] input)
{
    int[] output = new int[input.Length];
    output[0] = input[0];
    for (int i = 1; i < input.Length; i++)
    {
        output[i] = output[i-1] + input[i];
    }

    return output;
}
\end{verbatim}

\begin{figure}
\begin{center}
\includegraphics[scale=0.6]{images/prefix_sum_output.png}
\end{center}
\caption{Example input and output for computing a prefix sum.  Notice
  the sum of the first 5 elements is 52.}
\label{fig:prefix_sum_output}
\end{figure}


It is not at all obvious that a good parallel algorithm, say, one with
$\Theta(\log n)$ span, exists.  After all, it seems we need {\tt
  output[i-1]} to compute {\tt output[i]}.  If so, the span will be
$\Theta(n)$.  Just as a parallel reduction uses a totally different
algorithm than the straightforward sequential approach, there is also
an efficient parallel algorithm for the prefix-sum problem.  Like many
clever data structures and algorithms, it is not something most people
are likely to discover on their own, but it is a useful technique
to know.

The algorithm works in two passes.  We will call the first pass the
``up'' pass because it builds a binary tree from bottom to top.  We
first describe the resulting tree and then explain how it can be
produced via a fork-join computation.
Figure~\ref{fig:prefix-pass-one} shows an example.
\begin{itemize}
\item Every node holds the sum of the integers for some range of the
  {\tt input} array.
\item The root of the tree holds the sum for the entire range 
  $[0,n)$.\footnote{As before, we describe ranges as including their left
  end but excluding their right end.}
\item A node's left child holds the sum for the left half of the
  node's range and the node's right child holds the sum for the right
  half of the node's range.  For example, the root's left child is for
  the range $[0,n/2)$ and the root's right child is for the range
  $[n/2,n)$.
\item Conceptually, the leaves of the tree hold the sum for
  one-element ranges.  So there are $n$ leaves.  In practice, we would
  use a sequential cut-off and have the leaves store the sum for a
  range of, say, approximately 500 elements.
\end{itemize}

To build this tree --- and we do mean here to build the actual tree
data-structure\footnote{As a side-note, if you have seen an
  array-based representation of a complete tree, for example with a
  binary-heap representation of a priority queue, then notice that
  \emph{if} the array length is a power of two, then the tree we build
  is also complete and therefore amenable to a compact array
  representation.  The length of the array needs to be $2n-1$ (or less
  with a sequential cut-off).  If the array length is not a power of
  two and we still want a compact array, then we can either act as
  though the array length is the next larger power of two or use a more
  sophisticated rule for how to divide subranges so that we always
  build a complete tree.} because we need it in the second pass --- we
can use a straightforward fork-join computation:

\begin{itemize}
\item The overall goal is to produce the node for the range $[0,n)$.
\item To build the node for the range $[x,y)$:
  \begin{itemize}
  \item If $x==y-1$, produce a node holding {\tt input[x]}.
  \item Else recursively in parallel build the nodes for $[x,(x+y)/2)$ 
    and $[(x+y)/2,y)$.  Make these the left and right
  children of the result node.  Add their answers together for the
  result node's sum.
  \end{itemize}
\end{itemize}
In short, the result of the divide-and-conquer is a tree node and the
way we ``combine results'' is to use the two recursive results as the
subtrees.  So we build the tree ``bottom-up,'' creating larger
subtrees from as we return from each level of the recursion.
Figure~\ref{fig:prefix-pass-one} shows an example of this
bottom-up process, where each node stores the range it stores the sum
for and the corresponding sum.  The ``fromleft'' field is blank --- we
use it in the second pass.

Convince yourself this algorithm is $\Theta(n)$ work and $\Theta(\log
n)$ span.

The description above assumes no sequential cut-off.  With a cut-off,
we simply stop the recursion when $y-x$ is below the cut-off and
create one node that holds the sum of the range $[x,y)$, 
computed sequentially.

\begin{figure}
\begin{center}
\includegraphics[scale=.7]{images/prefix_sum_example_1.png}
\end{center}
\caption{Example of the first pass of the parallel prefix-sum
  algorithm: the overall result (bottom-right) is a binary tree where
  each node holds the sum of a range of elements of the input.  Each
  node holds the index range for which it holds the sum (two numbers
  for the two endpoints) and the sum of that range.  At the lowest
  level, we write $r$ for range and $s$ for sum just for formatting
  purposes.  The fromleft field is used in the second pass.  We can
  build this tree bottom-up with $\Theta(n)$ work and $\Theta(\log n)$
  span because a node's sum is just the sum of its children.}
\label{fig:prefix-pass-one}
\end{figure}

Now we are ready for the second pass called the ``down'' pass, where
we use this tree to compute the prefix-sum.  The essential trick is
that we process the tree from top to bottom, \emph{passing ``down'' as
  an argument the sum of the array indices to the left of the node}.
Figure~\ref{fig:prefix-pass-two} shows an example.  Here are the details:
\begin{itemize}
\item The argument passed to the root is 0.  This is because there are no
  numbers to the left of the range $[0,n)$ so their sum is 0.
\item The argument passed to a node's left child is the same argument
  passed to the node.  This is because the sum of numbers to the left
  of the range $[x,(x+y)/2)$ is the sum of numbers to the left of
  the range $[x,y)$.  
\item The argument passed to a node's right child is the argument
  passed to the node \emph{plus} the sum stored at the node's left child.
  This is because the sum of numbers to the left of the range
  $[(x+y)/2, y)$ is the sum to the left of $x$ plus the sum of the
  range $[x, (x+y)/2)$.  This is why we stored these sums in the up pass!
\end{itemize}
When we reach a leaf, we have exactly what we need: {\tt output[i]}
is {\tt input[i]} plus the value passed down to the $i^{th}$ leaf.
Convincing yourself this algorithm is correct will likely require
working through a short example while drawing the binary tree.

This second pass is also amenable to a parallel fork-join computation.
When we create a subproblem, we just need the value being passed down
and the node it is being passed to.  We just start with a value of 0
and the root of the tree.  This pass, like the first one, is
$\Theta(n)$ work and $\Theta(\log n)$ span.  So the algorithm overall
is $\Theta(n)$ work and $\Theta(\log n)$ span.  It is
\emph{asymptotically} no more expensive than computing just the sum of
the whole array.  The parallel-prefix problem, surprisingly, has a
solution with exponential parallelism!

If we used a sequential cut-off, then we have a range of output values
to produce at each leaf.  The value passed down is still just what we
need for the sum of all numbers to the left of the range and then a
simple sequential computation can produce all the output-values for
the range at each leaf proceeding left-to-right through the range.

\begin{figure}
\begin{center}
\includegraphics[scale=.7]{images/prefix_sum_example_2.png}

\vspace{3ex}

\includegraphics[scale=.7]{images/prefix_sum_example_3.png}
\end{center}
\caption{Example of the second pass of the parallel prefix-sum
  algorithm.  Starting with the result of the first pass and a
  ``fromleft'' value at the root of 0, we proceed down the tree
  filling in fromleft fields in parallel, propagating the same
  fromleft value to the left-child and the fromleft value plus the
  left-child's sum to the right-value.  At the leaves, the fromleft
  value plus the (1-element) sum is precisely the correct prefix-sum
  value. This pass is $\Theta(n)$ work and $\Theta(\log n)$ span.}
\label{fig:prefix-pass-two}
\end{figure}

Perhaps the prefix-sum problem is not particularly interesting.  But
just as our original sum-an-array problem exemplified the
parallel-reduction pattern, the prefix-sum problem exemplifies the
more general parallel-prefix pattern.  Here are two other general
problems that can be solved the same way as the prefix-sum problem;
you can probably think of more.  
\begin{itemize}
\item Let {\tt output[i]} be the minimum (or maximum) of all elements
  to the left of {\tt i}.
\item Let {\tt output[i]} be a count of how many elements to the left
  of {\tt i} satisfy some property.
\end{itemize}

Moreover, many parallel algorithms for problems that are not
``obviously parallel'' use a parallel-prefix computation as a helper
method.  It seems to be ``the trick'' that comes up over and over
again to make things parallel.  Section~\ref{sec:pack} gives an
example, developing an algorithm on top of parallel-prefix sum.  We
will then use \emph{that} algorithm to implement a parallel variant of
quicksort.

\subsection{Pack}
\label{sec:pack}

This section develops a parallel algorithm for this problem: Given an array
{\tt input}, produce an array {\tt output} containing only those
elements of {\tt input} that satisfy some property, and in the same
order they appear in {\tt input}.  For example if the property is,
``greater than 10'' and {\tt input} is {\tt
  \{17,4,6,8,11,5,13,19,0,24\}}, then {\tt output} is {\tt
  \{17,11,13,19,24\}}.  Notice the length of {\tt output} is unknown in
advance but never longer than {\tt input}.  A $\Theta(n)$ sequential
solution using our ``greater than 10'' example is:

\begin{verbatim}
int[] GreaterThenTen(int[] input)
{
    int count = 0;
    for (int i = 0; i < input.Length; i++)
    {
        if (input[i] > 10)
            count++;
    }

    int[] output = new int[count];
    int index = 0;
    for (int i = 0; i < input.Length; i++)
    {
        if (input[i] > 10)
        {
            output[index] = input[i];
            index++;
        }
    }

    return output;
}
\end{verbatim}
Writing a generic version is really no more difficult; as in
Section~\ref{sec:map-reduce} it amounts to a judicious use of generics
and higher-order programming.  In general, let us call this pattern a
\emph{pack} operation, adding it to our patterns of maps, reduces, and
prefixes.  However, the term \emph{pack} is not standard, nor is there
a common term to our knowledge.  \emph{Filter} is also descriptive,
but underemphasizes that the order is preserved.

This problem looks difficult to solve with effective
parallelism.  Finding which elements should be part of the output is a
trivial map operation, but knowing what output index to
use for each element requires knowing how many elements to the left
also are greater than 10.  But that is exactly what a prefix
computation can do!

We can describe an efficient parallel pack algorithm almost entirely
in terms of helper methods using patterns we already know.  For
simplicity, we describe the algorithm in terms of 3 steps, each of
which is $\Theta(n)$ work and $\Theta(\log n)$ span; in practice it is
straightforward to do the first two steps together in one fork-join
computation.
\begin{enumerate}
\item Perform a parallel map to produce a \emph{bit vector} where a 1
  indicates the corresponding input element is greater than 10.  So
  for {\tt \{17,4,6,8,11,5,13,19,0,24\}} this step produces
  {\tt \{1,0,0,0,1,0,1,1,0,1\}}.
\item Perform a parallel prefix sum on the bit vector produced in the
  previous step.  Continuing our example produces
  {\tt \{1,1,1,1,2,2,3,4,4,5\}}.
\item The array produced in the previous step provides the information
  that a final parallel map needs to produce the packed output
  array.  In pseudocode, calling the result of step (1) {\tt
  bitvector} and the result of step (2) {\tt bitsum}:
\begin{verbatim}
  int output_length = bitsum[bitsum.Length - 1];
  int[] output = new int[output_length];
  FORALL(int i = 0; i < input.Length; i++)
  {
     if (bitvector[i] == 1)
       output[bitsum[i] - 1] = input[i];
  }
\end{verbatim}
Note it is also possible to do step (3) using only {\tt bitsum} and
not {\tt bitvector}, which would allow step (2) to do the prefix sum
\emph{in place}, updating the {\tt bitvector} array.  In either case,
each iteration of the FORALL loop either does not write anything or
writes to a different element of {\tt output} than every other
iteration.
\end{enumerate}

Just as prefix sum was surprisingly useful for pack, pack turns out to
be surprisingly useful for more important algorithms, including a
parallel variant of quicksort...

\subsection{Parallel Quicksort}

Recall that sequential quicksort is an in-place sorting algorithm
with $O(n\log n)$ best- and expected-case running time.  It works as
follows:
\begin{enumerate}
\item Pick a pivot element ($O(1)$)
\item Partition the data into: ($O(n)$)
  \begin{enumerate}
  \item Elements less than the pivot
  \item The pivot
  \item Elements greater than the pivot
  \end{enumerate}
\item Recursively sort the elements less than the pivot
\item Recursively sort the elements greater than the pivot
\end{enumerate}
Let's assume for simplicity the partition is roughly balanced, so the
two recursive calls solve problems of approximately half the size.  If
we write $R(n)$ for the running time of a problem of size $n$, then,
except for the base cases of the recursion which finish in $O(1)$
time, we have $R(n) = O(n) + 2R(n/2)$ due to the $O(n)$ partition and
two problems of half the size.\footnote{The more common notation is
  $T(n)$ instead of $R(n)$, but we will use $R(n)$ to avoid confusion
  with work $T_1$ and span $T_\infty$.}  It turns out that when the running
time is $R(n) = O(n) + 2R(n/2)$, this works out to be $O(n\log n)$, but
we do not show the derivation of this fact here.  Moreover, if pivots
are chosen randomly, the expected running time remains
$O(n\log n)$. For simplicity, the analysis below will continue to
assume the chosen pivot is magically exactly the median.  As with
sequential quicksort, the expected-time asymptotic bounds are the same
if the pivot is chosen uniformly at random, but the proper analysis
under this assumption is more mathematically intricate.

How should we parallelize this algorithm?  The first obvious step is
to perform steps (3) and (4) in parallel.  This has no effect on the
work, but changes the recurrence for the span to $R(n)=O(n)+1R(n/2)$,
(because we can solve the two problems of half the size simultaneously),
which works out to be $O(n)$.  Therefore, the parallelism, $T_1/T_\infty$, is
$O(n\log n)/O(n)$, i.e., $O(\log n)$.  While this is an improvement,
it is a far cry from the exponential parallelism we have seen for
algorithms up to this point.  Concretely, it suggests an infinite
number of processors would sort billions of elements dozens of times
faster than one processor.  This is okay but underwhelming.

To do better, we need to parallelize the step that produces the
partition.  The sequential partitioning algorithm uses clever swapping
of data elements to perform he in-place sort.  To parallelize it, we
will sacrifice the in-place property.  Given Amdahl's Law, this is
likely a good trade-off: use extra space to achieve additional
parallelism.  All we will need is one more array of the same length as
the input array.

To partition into our new extra array, all we need are two pack
operations: one into the left side of the array and one into the right
side.  In particular:
\begin{itemize}
\item Pack all elements less than the pivot into the left side of the
  array: If $x$ elements are less than the pivot, put this data at
  positions $0$ to $x-1$.
\item Pack all elements greater than the pivot into the right side of the
  array: If $x$ elements are greater than the pivot, put this data at
  positions $n-x$ to $n-1$.
\end{itemize}
The first step is exactly like the pack operation we saw earlier.  The
second step just works down from the end of the array instead of up
from the beginning of the array.  After performing both steps, there
is one spot left for the pivot between the two partitions.  
Figure~\ref{fig:parallel_quicksort_pack} shows an example of using two
pack operations to partition in parallel.

\begin{figure}
\begin{center}
\includegraphics[scale=.6]{images/parallel_quicksort_pack.png}
\end{center}
\caption{Example using two pack operations to partition in parallel}
\label{fig:parallel_quicksort_pack}
\end{figure}

Each of the two pack operations is $O(n)$ work and $O(\log n)$ span.
The fact that we look at each element twice, once to decide if it is
less than the pivot and once to decide if it is greater than the pivot
is only a constant factor more work.  Also note that the two
pack operations can be performed in parallel, though this is
unnecessary for the asymptotic bound.

After completing the partition, we continue with the usual recursive
sorting (in parallel) of the two sides.  Rather than create another
auxiliary array, the next step of the recursion can reuse the
original array.  Sequential mergesort often uses this same space-reuse
trick.

Let us now re-analyze the asymptotic complexity of parallel quicksort
using our parallel (but not in-place) partition and, for simplicity,
assuming pivots always divide problems exactly in half.  The work
is still $R(n)=O(n)+2R(n/2)=O(n\log n)$ where the $O(n)$ now includes
the two pack operations.  The span is now $R(n)=O(\log
n)+1R(n/2)$ because the span for the pack operations is $O(\log n)$.
This turns out to be (again, not showing the derivation) $O(\log^2 n)$, not as good as
$O(\log n)$, but much better than the $O(n)$ (in fact, $\Theta(n)$) we
had with a sequential partition.  Hence the available parallelism is
proportional to $n\log n/\log^2 n=n/\log n$, an exponential speed-up.

\subsection{Parallel Mergesort}

As a final parallel algorithm, we develop a parallel version of
mergesort.  As with quicksort, achieving a little parallelism is
trivial, but achieving a lot of parallelism requires much more
cleverness. We first recall the sequential mergesort algorithm, which
always has running time $O(n\log n)$ and is not in-place:
\begin{enumerate}
\item Recursively sort the left half and right half of the input.
\item Merge the sorted results into a new sorted array by repeatedly
  moving the smallest not-yet-moved element into the new array.
\end{enumerate}
The running time for this algorithm is $R(n)=2R(n/2)+O(n)$ because there
are two subproblems of half the size and the merging is $O(n)$ using a
single loop that progresses through the two sorted recursive
results.\footnote{As with the analysis for parallel quicksort, we are
  writing $R(n)$ instead of the more common $T(n)$ just to avoid
  confusion with our notation for work and span.}  This is the same
running time as sequential quicksort, and it works out to $O(n\log
n)$.

The trivial first parallelization step is to do the two recursive
sorts in parallel.  Exactly like with parallel quicksort, this has no
effect on the work and reduces the span to $R(n)=1R(n/2)+O(n)$, which
is $O(n)$.  Hence the parallelism is $O(\log n)$.

To do better, we need to parallelize the merge operation.  Our
algorithm will take two sorted subarrays of length $x$ and $y$ and
merge them.  In sequential mergesort, the two lengths are equal
(or almost equal when merging an odd number of elements), but as we will see, our recursive parallel merging
will create subproblems that may need to merge arrays of different
lengths.  The algorithm is:\footnote{The base case, ignoring a
  sequential cut-off, is when $x$ and $y$ are both $\leq 1$, in which
  case merging is trivially $O(1)$.}
\begin{itemize}
\item Determine the median element of the larger array.  This is
  just the element in the middle of its range, so this operation is
  $O(1)$.  
\item Use binary search to find the position $j$ in the smaller array
  such that all elements to the left of $j$ are less than the larger
  array's median.  Binary search is $O(\log m)$ where $m$ is the
  length of the smaller array.
\item Recursively merge the left half of the larger array with
  positions $0$ to $j$ of the smaller array.
\item Recursively merge the right half of the larger array with
  the rest of the smaller array.  
\end{itemize}
The total number of elements this algorithm merges is $x+y$, which we
will call $n$.  The first two steps are $O(\log n)$ since $n$ is
greater than the length of the array on which we do the binary
search.  That leaves the two subproblems, which are not necessarily of
size $n/2$.  That best-case scenario occurs when the binary search
ends up in the middle of the smaller array.  The worst-case scenario
is when it ends up at one extreme, i.e., all elements of the smaller
array are less than the median of the larger array or all elements of
the smaller array are greater than the median of the larger array.

But we now argue that the worst-case scenario is not that bad.  The
larger array has at least $n/2$ elements --- otherwise it would not be
the larger array.  And we always split the larger array's elements in
half for the recursive subproblems.  So each subproblem has at least
$n/4$ (half of $n/2$) elements.  So the worst-case split is $n/4$ and
$3n/4$.  That turns out to ``good enough'' for a large amount of
parallelism for the merge.  The full analysis of the recursive
algorithm is a bit intricate, so we just mention a few salient facts here.

Because the worst-case split is $n/4$ and $3n/4$, the worst-case span
is $R(n)=R(3n/4)+O(\log n)$: the $R(n/4)$ does not appear because it
can be done in parallel with the $R(3n/4)$ and is expected to finish
first (and the $O(\log n)$ is for the binary search).
$R(n)=R(3n/4)+O(\log n)$ works out to be $O(\log^2 n)$ (which is not
obvious and we do not show the derivation here).  The work is
$R(n)=R(3n/4)+R(n/4) + O(\log n)$, which works out to be $O(n)$ (again
omitting the derivation).

Recall this analysis was just for the merging step.  Adding $O(\log^2
n)$ span and $O(n)$ work for merging back into the overall mergesort
algorithm, we get a span of $R(n)=1R(n/2)+O(\log^2 n)$, which is
$O(\log^3 N)$, and a work of $R(n)=2R(n/2)+O(n)$, which is $O(n\log
n)$.  While the span and resulting parallelism is $O(\log n)$ worse
than for parallel quicksort, it is a worst-case bound compared to
quicksort's expected case.

\section{Basic Shared-Memory Concurrency}
\label{sec:basic-concurrency}

We now leave fork-join parallelism behind to focus on concurrent
programming, which is about correctly and efficiently controlling
access by multiple threads to shared resources.  As described in
Section~\ref{sec:intro}, the line between parallel and concurrent
programming is not clear, and many programs have aspects of both, but
it is best at first to study each by itself.

We will still have threads and shared memory.  We will use C\#
built-in threads ({\tt System.Threading.Thread}), not any classes in the
Task Parallel Library.  The ``shared resources'' will be memory locations
(fields of objects) used by more than one thread.  We will learn how
to write code that provides properly synchronized access to shared
resources even though it may not be known what order the threads may
access the data.  In fact, multiple threads may \emph{try} to access
and/or modify the data at the same time. We will see why this cannot 
be allowed and how programmers must use programming-language features to 
avoid it. The features we will focus on involve \emph{mutual-exclusion locks},
introduced in Section~\ref{sec:locks}.

Here are some simple high-level examples of shared resources where we
need to control concurrent access:
\begin{enumerate}
\item We are writing banking software where we have an object for each
  bank account.  Different threads (e.g., one per bank teller or ATM)
  deposit or withdraw funds from various accounts.  What if two
  threads try to manipulate the same account at the same time?
\item We have a hashtable storing a cache of previously-retrieved
  results.  For example, maybe the table maps patients to medications
  they need to be given.  What if two threads try to insert patients
  --- maybe even the same patient --- at the same time?  Could we end
  up with the same patient in the table twice?  Could that cause other
  parts of the program to indicate double the appropriate amount of
  medication for the patient?
\item Suppose we have a \emph{pipeline}, which is like an assembly
  line, where each \emph{stage} is a separate thread that does some
  processing and then gives the result to the subsequent stage.  To
  pass these results, we could use a queue where the result-provider
  enqueues and the result-receiver dequeues.  So an $n$-stage pipeline
  would have $n-1$ queues; one queue between each pair of adjacent
  stages.  What if an enqueue and a dequeue happen at the same time?
  What if a dequeue is attempted when there are no elements in the
  queue (yet)?  In sequential programming, dequeueing from an empty
  queue is typically an error.  In concurrent programing, we may
  prefer instead to \emph{wait} until another thread enqueues something.
\end{enumerate}

\subsection{The Programming Model}

As the examples above hint at, in concurrent programming we have
multiple threads that are ``largely doing their own thing'' but
occasionally need to coordinate since they are accessing shared
resources.  It is like different cooks working in the same kitchen ---
easy if they are using different utensils and stove burners, but more
difficult if they need to share things.  The cooks may be working
toward a shared goal like producing a meal, but while they are each
working on a different recipe, the shared goal is not the focus.

In terms of programming, the basic model comprises multiple
threads that are running in a mostly uncoordinated way.  We might
create a new thread when we have something new to do.  The operations
of each thread are \emph{interleaved} (running alongside, before,
after, or at the same time) with operations by other threads.  This is
very different from fork-join parallelism.  In fork-join algorithms,
one thread creates a helper thread to solve a specific subproblem and
the creating thread waits (by calling join) for the other thread to
finish.  Here, we may have 4 threads processing bank-account changes
as they arrive.  While it is unlikely that two threads would access
the same account at the same time, it is possible and we must be
correct in this case.

One thing that makes concurrent programs very difficult to debug is
that the bugs can be very unlikely to occur.  If a program exhibits a
bug and you re-run the program another million times with the same
inputs, the bug may not appear again.  This is because what happens
can depend on the order that threads access shared resources, which is
not entirely under programmer control.  It can depend on how the
threads are \emph{scheduled} onto the processors, i.e., when each
thread is chosen to run and for how long, something that is decided
\emph{automatically} (when debugging, it often feels
\emph{capriciously}) by the implementation of the programming
language, with help from the operating system.  Therefore, concurrent
programming is \emph{nondeterministic}, the output does not depend
only on the input.  Because testing concurrent programs is so
difficult, it is exceedingly important to design them well using
well-understood design principles (see Section~\ref{sec:guidelines}) from the
beginning.

As an example, suppose a bank-account class has methods {\tt Deposit}
and {\tt Withdraw}.  Suppose the latter throws an exception if the
amount to be withdrawn is larger than the current balance.  If one
thread deposits into the account and another thread withdraws from the
account, then whether the withdrawing thread throws an exception
could depend on the order the operations occur.  And that is still
assuming all of one operation completes before the other starts.  In upcoming
sections, we will learn how to use \emph{locks} to ensure the
operations themselves are not interleaved, but even after ensuring
this, the program can still be nondeterministic.

As a contrast, we can see how fork-join parallelism made it relatively
easy to avoid having two threads access the same memory at the same
time.  Consider a simple parallel reduction like summing an array.
Each thread accesses a disjoint portion of the array, so there is no
sharing like there potentially is with bank accounts.  The
sharing is with fields of the thread objects: One thread initializes
fields (like the array range) before creating the helper thread.  Then
the helper thread may set some result fields that the other thread
reads after the helper thread terminates.  The \emph{synchronization}
here is accomplished entirely via (1) thread creation (not calling
{\tt start} or {\tt fork} until the correct fields are written) and
(2) {\tt join} (not reading results until the other thread has
terminated).  But in our concurrent programming model, this form of
synchronization will not work because we will not wait for another
thread to finish running before accessing a shared resource like a
bank account.

It is worth asking why anyone would use this difficult programming
model.  It would certainly be simpler to have one thread that does
everything we need to do.  There are several reasons:
\begin{itemize}
\item \emph{Parallelism:} Despite conceptual differences between
  parallelism and concurrency, it may well be that a parallel
  algorithm needs to have different threads accessing some of the same
  data structures in an unpredictable way.  For example, we could have
  multiple threads search through a graph, only occasionally crossing
  paths.
\item \emph{Responsiveness:} Many programs, including operating systems and
  programs with user interfaces, want/need to respond to external
  events quickly.  One way to do this is to have some threads doing
  the program's expensive computations while other threads are
  responsible for ``listening for'' events like buttons being clicked
  or typing occurring.  The listening threads can then (quickly) write
  to some fields that the computation threads later read.
\item \emph{Processor utilization:} If one thread needs to read data from
  disk (e.g., a file), this will take a very long time relatively
  speaking.  In a conventional single-threaded program, the program
  will not do anything for the milliseconds it takes to get the
  information.  But this is enough time for another thread to perform
  millions of instructions.  So by having other threads, the
  program can do useful work while waiting for I/O\@.  This use of
  multithreading is called \emph{masking (or hiding) I/O latency}.
\item \emph{Failure/performance isolation:} Sometimes having multiple threads
  is simply a more convenient way to structure a program.  In
  particular, when one thread throws an exception or takes too long to
  compute a result, it affects only what code is executed by that
  thread.  If we have multiple independent pieces of work, some of
  which might (due to a bug, a problem with the data, or some other
  reason) cause an exception or run for too long, the other threads
  can still continue executing.  There are other approaches to these
  problems, but threads often work well.
\end{itemize}
It is common to hear that threads are useful \emph{only} for performance.
This is true only if ``performance'' includes responsiveness and
isolation, which stretches the definition of performance.

\subsection{The Need for Synchronization}

This section presents in detail an initial example to demonstrate why
we should prevent multiple threads from simultaneously performing
operations on the same memory.  We will focus on showing
\emph{possible interleavings} that produce the wrong answer.  However,
the example also has \emph{data races}, which Section~\ref{sec:races}
explains must be prevented.  We will show that using ``ordinary'' C\#
features to try to prevent bad interleavings simply does not work.
Instead, we will learn to use \emph{locks}, which are primitives
provided by C\# and other programming languages that provide what
we need.  We will not learn how to \emph{implement} locks, since the
techniques use low-level features more central to courses in operating
systems and computer architecture.

Consider this code for a {\tt BankAccount} class:
\begin{verbatim}
public class BankAccount
{
    // No auto property for consistency with
    // further variations of the BankAccoutn class
    private int _balance = 0;
    public int Balance
    {
        get { return _balance; }
        set { _balance = value; }
    }

    public void Withdraw(int amount)
    {
        if (amount > Balance)
        {
            throw new WithdrawTooLargeException();
        }

        Balance -= amount;
    }

    // ... other operations like deposit, etc.
}
\end{verbatim}

This code is correct in a single-threaded program.  But suppose we
have two threads, one calls {\tt x.Withdraw(100)}, and the other calls
{\tt y.Withdraw(100)}.  The two threads could truly run at the same
time on different processors.  Or they may run one at a time, but the
\emph{thread scheduler} might stop one thread and start the other at
any point, switching between them any number of times.  Still, in many
scenarios it is not a problem for these two method calls to execute concurrently:
\begin{itemize}
\item If {\tt x} and {\tt y} are not aliases, meaning they refer to
  distinct bank accounts, then there is no problem because the calls
  are using different memory.  This is like two cooks using different
  pots at the same time.
\item If one call happens to finish before the other starts, then the behavior
  is like in a sequential program.  This is like one cook using a pot
  and then another cook using the same pot.  When this is not the
  case, we say the calls \emph{interleave}.  Note that interleaving
  can happen even with one processor because a thread can be
  \emph{pre-empted} at any point, meaning the thread scheduler stops
  the thread and runs another one.
\end{itemize}

So let us consider two interleaved calls to {\tt Withdraw} on the same
bank account.  We will ``draw'' interleavings using a vertical
timeline (earlier operations closer to the top) with each thread in a
separate column.  There are many possible interleavings since even the
operations in a helper method like {\tt Balance} getter can be interleaved
with operations in other threads.  But here is one incorrect
interleaving.  Assume initially the {\tt \_balance} field holds 150 and
both withdrawals are passed 100.  Remember that each thread executes a
different method call with its own ``local'' variables {\tt b} and
{\tt amount} but the calls to {\tt Balance} getter and setter 
are, we assume, reading/writing the one {\tt \_balance} field of the
\emph{same} object.

\begin{verbatim}
Thread 1                    Thread 2
--------                    --------
int b = Balance;
                            int b = Balance;
                            if (amount > b)
                              throw new ...;
                            Balance = b - amount; // sets balance to 50
if (amount > b) // no exception: b holds 150
  throw new ...;
Balance = b - amount;
\end{verbatim}

If this interleaving occurs, the resulting {\tt \_balance} will be 50
and no exception will be thrown.  But two {\tt Withdraw} operations
were supposed to occur --- there \emph{should} be an exception.
Somehow we ``lost'' a withdraw, which would not make the bank happy.
The problem is that {\tt \_balance} changed after Thread 1 retrieved it
and stored it in {\tt b}.

When first learning concurrent programming there is a natural but
almost-always WRONG attempted fix to this sort of problem.  It is tempting
to rearrange or repeat operations to try to avoid using ``stale''
information like the value in {\tt b}.  Here is a WRONG idea:
\begin{verbatim}
public void Withdraw(int amount)
{
    if (amount > Balance)
    {
        throw new WithdrawTooLargeException();
    }
    // maybe balance changed, so get the new balance
    Balance = Balance - amount;
}
\end{verbatim}
The idea above is to call {\tt Balance} a second time to get any
updated values.  But there is \emph{still} the potential for an
incorrect interleaving.  Just because {\tt Balance = Balance -
  amount} is on one line in our source code does not mean it happens
all at once.  This code still (a) calls {\tt Balance} getter, then (b)
subtracts {\tt amount} from the result, then (c) calls {\tt
  Balance} setter.  Moreover (a) and (c) may consist of multiple steps.
In any case, the balance can change between (a) and (c), so we have
not really accomplished anything except we might now produce a
negative balance without raising an exception.

The sane way to fix this sort of problem, which arises whenever we
have concurrent access to shared memory that might change (i.e., the
contents are \emph{mutable}), is to enforce \emph{mutual exclusion}:
allow only one thread to access any particular account at a time.  The
idea is that a thread will ``hang a do-not-disturb sign'' on the
account before it starts an operation such as {\tt Withdraw} and not
remove the sign until it is finished.  Moreover, all threads will
check for a ``do-not-disturb sign'' before trying to hang a sign and
do an operation.  If such a sign is there, a thread will \emph{wait}
until there is no sign so that it can be sure it is the only one
performing an operation on the bank account.  Crucially, the act of
``checking there is no sign hanging and then hanging a sign'' has to
be done ``all-at-once'' or, to use the common terminology,
\emph{atomically}.  We call the work done
``while the sign is hanging'' a \emph{critical section}; it is
critical that such operations not be interleaved with other
conflicting ones.  (Critical sections often have other technical
requirements, but this simple informal definition will suffice for our
purposes.)

Here is one WRONG way to try to implement this idea.  You should never
write code that tries to do this manually, but it is worth
understanding why it is WRONG:
\begin{verbatim}
public class BankAccount
{
    private int _balance = 0;
    public int Balance
    {
        get { return _balance; }
        set { _balance = value; }
    }

    private bool _busy = false;

    public void Withdraw(int amount)
    {
        while (_busy)
        { /* spin-wait */ }
        _busy = true;

        if (amount > Balance)
        {
            throw new WithdrawTooLargeException();
        }

        Balance -= amount;

        _busy = false;
    }
}
\end{verbatim}
The idea is to use the {\tt \_busy} variable as our do-not-disturb
sign.  All the other account operations would use the {\tt \_busy}
variable in the same way so that only one operation occurs at a time.
The while-loop is perhaps a little inefficient since it would be
better not to do useless work and instead be notified when the account
is ``available'' for another operation, but that is not the real
problem.  The {\tt \_busy} variable does not work, as this interleaving
shows:
\begin{verbatim}
Thread 1                    Thread 2
--------                    --------
while(_busy) { }
                            while(_busy) { }
_busy = true;
                            _busy = true;
int b = Balance;
                            int b = Balance;
                            if(amount > b)
                              throw new ...;
                            Balance = b - amount;
if(amount > b)
  throw new ...;
Balance = b - amount;
\end{verbatim}

Essentially we have the same problem with {\tt \_busy} that we had with
{\tt \_balance}: we can check that {\tt \_busy} is false, but then it
might get set to true before we have a chance to set it to true
ourselves.  We need to check there is no do-not-disturb sign
\emph{and} put our own sign up while \emph{we still know there is no
  sign}.  Hopefully you can see that using a variable to see if {\tt
  \_busy} is busy is only going to push the problem somewhere else
again.

To get out of this conundrum, we rely on \emph{synchronization
  primitives} provided by the programming language.  These typically
rely on special hardware instructions that can do a little bit more
``all at once'' than just read or write a variable.  That is, they
give us the \emph{atomic} check-for-no-sign-and-hang-a-sign that we
want. This turns out to be enough to implement mutual exclusion
properly.  The particular kind of synchronization we will use is
\emph{locking}.

\subsection{Locks}
\label{sec:locks}

We can define a \emph{mutual-exclusion lock}, also known as just a
\emph{lock} (or sometimes called a \emph{mutex}), as an abstract
datatype designed for concurrent programming and supporting three
operations:
\begin{itemize}
\item {\tt new} creates a new lock that is initially ``not held''
\item {\tt acquire} takes a lock and blocks until it is currently
  ``not held'' (this could be right away, i.e., not blocking at all,
  since the lock might already be not held).  It sets the lock to
  ``held'' and returns.
\item {\tt release} takes a lock and sets it to ``not held.''
\end{itemize}
This is exactly the ``do-not-disturb'' sign we were looking for, where
the {\tt acquire} operation blocks (does not return) until the caller
is the thread that most recently hung the sign.  It is up to the lock
implementation to ensure that it always does the right thing no matter
how many acquires and/or releases different threads perform
simultaneously.  For example, if there are three acquire operations at
the same time for a ``not held'' lock, one will ``win'' and return
immediately while the other two will block.  When the ``winner'' calls
{\tt release}, one other thread will get to hold the lock next, and so on.

The description above is a general notion of what locks are.
Section~\ref{sec:csharp-locks} will describe how to use the locks that
are part of the C\# language.  But using the general idea, we can
write this PSEUDOCODE (C\# does not actually have a {\tt Lock} class),
which is almost correct (i.e., still WRONG but close):
\begin{verbatim}
public class BankAccountLockPseudocode
{
    private int _balance = 0;
    public int Balance
    {
        get { return _balance; }
        set { _balance = value; }
    }

    private Lock lk = new Lock();

    public void Withdraw(int amount)
    {
        lk.Acquire(); /* may block execution */
        int b = Balance;
        if (amount > b)
        {
            throw new WithdrawTooLargeException();
        }
        Balance = b;
        lk.Release();
    }

    // deposit would also acquire/release lk
}
\end{verbatim}
Though we show only the {\tt Withdraw} operation, remember that all
operations accessing {\tt \_balance} and whatever other fields the
account has should acquire and release the lock held in {\tt lk}.  That way,
only one thread will perform any operation on a particular account at
a time.  Interleaving a {\tt Withdraw} and a {\tt Deposit}, for
example, is incorrect.  If any operation does not use the lock
correctly, it is like threads calling that operation are ignoring the
do-not-disturb sign.  Because each account uses a different lock,
different threads can still operate on different accounts at the same
time.  But because locks work correctly to enforce that only one
thread holds any given lock at a time, we have the mutual exclusion we
need for each account.  If instead we used one lock for all the
accounts, then we would still have mutual exclusion, but we could have
worse performance since threads would be waiting unnecessarily.

What, then, is wrong with the pseudocode?  The biggest problem occurs if
{\tt amount > b}.  In this case, an exception is thrown and therefore
the lock is not released.  So no thread will ever be able to perform
another operation on this bank account and any thread that attempts to
do so will be blocked \emph{forever} waiting for the lock to be
released.  This situation is like someone leaving the room
through a window without removing the do-not-disturb sign from the door, so nobody
else ever enters the room.

A fix in this case would be to add {\tt lk.Release} in the branch of
the if-statement before the throw-statement.  But more generally we
have to worry about any exception that might be thrown, even while
some other method called from within the critical section is executing
or due to a null-pointer or array-bounds violation.  So more generally
it would be safer to use a catch-statement or finally-statement to make
sure the lock is always released.  Fortunately, as
Section~\ref{sec:csharp-locks} describes, when we use C\# locks
instead of our pseudocode locks, this will not be necessary.

A second problem, also not an issue in C\#, is more subtle.  Notice
that {\tt Withdraw} calls {\tt Balance} while
holding the lock for the account.  This property might also be called
directly from outside the bank-account implementation, assuming it is 
public.  Therefore, it should also acquire and release the lock
before accessing the account's field {\tt \_balance}.  But if {\tt
  Withdraw} calls this property \emph{while holding the lock},
then as we have described it, the thread would block forever ---
waiting for ``some thread'' to release the lock when in fact the
thread itself already holds the lock.

There are two solutions to this problem.  The first solution is more
difficult for programmers: we can define two versions of {\tt
  Balance}.  One would acquire the lock and
one would assume the caller already holds it.  The latter could be
private, perhaps, though there are any number of ways to manually keep
track in the programmer's head (and hopefully documentation!) what
locks the program holds where.  

The second solution is to change our definition of the {\tt Acquire}
and {\tt Release} operations so that it is okay for a thread to
(re)acquire a lock it already holds.  Locks that support this are
called \emph{reentrant locks}.  (The name fits well with our
do-not-disturb analogy.) To define reentrant locks, we need to adjust
the definitions for the three operations by adding a \emph{count} to
track how many times the current holder has reacquired the lock (as
well as the identity of the current holder):
\begin{itemize}
\item {\tt new} creates a new lock with no current holder and a count
  of 0
\item {\tt acquire} blocks if there is a current holder
  \emph{different from the thread calling it}.  Else if the current holder
  is the thread calling it, do not block and increment the counter.
  Else there is no current holder, so set the current holder to the
  calling thread.
\item {\tt release} only releases the lock (sets the current holder to
  ``none'') if the count is 0.  Otherwise, it decrements the count.
\end{itemize}
In other words, a lock is released when the number of {\tt release}
operations by the holding thread equals the number of {\tt acquire}
operations.

\subsection{Locks in C\#}
\label{sec:csharp-locks}

The C\# language has built-in support for locks that is different
than the pseudocode in the previous section, but easy to understand in
terms of what we have already discussed.  The main addition to the
language is a {\tt lock} statement, which looks like this:
\begin{verbatim}
  lock (expression)
  {
     statements
  }
\end{verbatim}
Here, {\tt lock} is a keyword.  Basically, the \emph{syntax}
is just like a while-loop using a different keyword, but this is
\emph{not} a loop.  Instead it works as follows:
\begin{enumerate}
\item The {\tt expression} is evaluated.  It must produce (a reference
  to) an object --- not {\tt null} or a number.  This object is
  treated as a lock.  In C\#, \emph{every object is a lock} that any
  thread can acquire or release.  This decision is a bit strange, but
  is convenient in an object-oriented language as we will see.
\item The {\tt lock} statement acquires the lock, i.e., the object
  that is the result of step (1).  Naturally, this may block until the
  lock is available.  Locks are reentrant in C\#, so the statement
  will not block if the executing thread already holds it.
\item After the lock is successfully acquired, the {\tt statements}
  are executed.
\item When control leaves the {\tt statements}, the lock is released.
  This happens either when the final {\tt \}} is reached \emph{or}
  when the program ``jumps out of the statement'' via an exception, a
  {\tt return}, a {\tt break}, or a {\tt continue}.
\end{enumerate}
Step (4) is the other unusual thing about C\# locks, but it is quite
convenient.  Instead of having an explicit release statement, it
happens implicitly at the ending {\tt \}}.  More importantly, the
lock is still released if an exception causes the thread to not 
finish the {\tt statements}.  Naturally, {\tt statements} can contain
arbitrary C\# code: method calls, loops, other synchronized
statements, etc.  The only shortcoming is that there is no way to
release the lock \emph{before} reaching the ending {\tt \}}, but it
is rare that you want to do so.

Here is an implementation of our bank-account class using synchronized
statements as you might expect. The key thing to notice is that any
object can serve as a lock, so for the lock we simply create an
instance of the built-in {\tt Object} class.
\begin{verbatim}
public class BankAccountLockObject
{
    private readonly object lk = new object();

    private int _balance = 0;
    public int Balance
    {
        get
        {
            lock (lk)
            {
                return _balance;
            }
        }
        set
        {
            lock (lk)
            {
                _balance = value;
            }
        }
    }

    public void Withdraw(int amount)
    {
        lock (lk)
        {
            int b = Balance;
            if (amount > b)
            {
                throw new WithdrawTooLargeException();
            }
            Balance = b - amount;
        }
    }

    // deposit and other operations
    // would also use lock (lk)
}
\end{verbatim}
Because locks are reentrant, it is no problem for {\tt Withdraw} to
call {\tt Balance}.  Because locks are automatically released, 
the exception in {\tt Withdraw} is not a problem.

While it may seem naturally good style to make {\tt lk} a {\tt
  private} field since it is an implementation detail (clients need
not care \emph{how} instances of {\tt BankAccount} provide mutual
exclusion), this choice prevents clients from writing their own
critical sections involving bank accounts. For example, suppose a
client wants to double an account's balance.  In a sequential program,
another class could include this correct method:
\begin{verbatim}
void DoubleBalance(BankAccount acct)
{
    acct.Balance = acct.Balance * 2;
}
\end{verbatim}
But this code is subject to a bad interleaving: the balance might be
changed by another thread after the call to {\tt Balance}.  If {\tt
  lk} were instead {\tt public}, then clients could use it --- and the
convenience of reentrant locks --- to write a proper critical
section:
\begin{verbatim}
void DoubleBalance(BankAccount acct)
{
    lock (acct.lk)
    {
        acct.Balance = acct.Balance * 2;
    }
}
\end{verbatim}

There is a simpler and more idiomatic way, however: Why create a new
{\tt Object} and store it in a {\tt public} field?  Remember any
object can serve as a lock, so it is more convenient 
to use the instance of {\tt BankAccount} itself (or in
general, the object that has the fields relevant to the
synchronization).  So we would remove the {\tt lk} field entirely like
this (there is one more slightly shorter version coming):
\begin{verbatim}
public class BankAccountLockThis
{
    private int _balance = 0;
    public int Balance
    {
        get
        {
            lock (this)
            {
                return _balance;
            }
        }
        set
        {
            lock (this)
            {
                _balance = value;
            }
        }
    }

    public void Withdraw(int amount)
    {
        lock (this)
        {
            int b = Balance;
            if (amount > b)
            {
                throw new WithdrawTooLargeException();
            }
            Balance = b - amount;
        }
    }

    // deposit and other operations
    // would also use lock (this)
}
\end{verbatim}
All we are doing differently is using the object itself (referred to
by the keyword {\tt this}) rather than a different object.  Of course,
all methods need to agree on what lock they are using.  Then a client
could write the {\tt DoubleBalance} method like this:
\begin{verbatim}
void DoubleBalance(BankAccount acct)
{
    lock (acct)
    {
        acct.Balance = acct.Balance * 2;
    }
}
\end{verbatim}
In this way, {\tt DoubleBalance} acquires the same lock that the
methods of {\tt acct} do, ensuring mutual exclusion.  That is, {\tt
  DoubleBalance} will not be interleaved with other operations on the
same account.

Because this idiom is so common, C\# has one more shortcut to keep 
the code a bit cleaner.  When an entire method body should be
synchronized on {\tt this}, we can omit the {\tt lock} statement and
instead decorate a method or a property with the attribute 
{\tt MethodImpl(MethodImplOptions.Synchronized)}.  This is just another 
way of saying the same thing.  So our FINAL VERSION looks like this:
\begin{verbatim}
public class BankAccountMethodImplSync
{
    private int _balance = 0;
    public int Balance
    {
        [MethodImpl(MethodImplOptions.Synchronized)]
        get { return _balance; }
        [MethodImpl(MethodImplOptions.Synchronized)]
        set { _balance = value; }
    }

    [MethodImpl(MethodImplOptions.Synchronized)]
    public void Withdraw(int amount)
    {
        int b = Balance;
        if (amount > b)
        {
            throw new WithdrawTooLargeException();
        }
        Balance = b - amount;
    }
    
    // deposit and other operations
    // would also use [MethodImpl(MethodImplOptions.Synchronized)]
}
\end{verbatim}
There is no change to the {\tt DoubleBalance} method.  It must still
use a {\tt lock} statement to acquire {\tt acct}.

While we have built up to our final version in small steps, in
practice it is common to have classes defining shared resources
consist of all synchronized methods.  This approach tends to work well
provided that critical sections only access the state of a single
object.  For some other cases, see the guidelines in
Section~\ref{sec:guidelines}.

Remember that each C\# object represents a \emph{different} lock.  If
two threads use synchronized statements to acquire different locks,
the bodies of the synchronized statements can still execute
simultaneously.  Here is a subtle BUG emphasizing this point, assuming
{\tt arr} is an array of objects and {\tt i} is an {\tt int}:
\begin{verbatim}
void SomeMethod()
{
    lock(arr[i])
    {
        if (SomeCondition()) {
            arr[i] = new Foo();
        }
        // some more statements using arr[i]
    }
}
\end{verbatim}
If one thread executes this method and {\tt SomeCondition()} evaluates
to {\tt true}, then it updates {\tt arr[i]} to hold a different
object, so another thread executing {\tt SomeMethod()} could acquire
this different lock and also execute the {\tt lock} statement at the same
time as the first thread.

Finally, note that the {\tt lock} statement is far from the only
support for concurrent programming in C\#.  In
Section~\ref{sec:condvars} we will show C\# support for condition
variables.  The namespace {\tt System.Collections.Concurrent} also has many
library classes useful for different tasks.  For example, you almost
surely should not implement your own concurrent dictionary or queue
since carefully optimized correct implementations are already
provided.  In addition to concurrent data structures, the standard
library also has many features for controlling threads.

\section{Race Conditions: Bad Interleavings and Data Races}
\label{sec:races}

A \emph{race condition} is a mistake in your program (i.e., a bug)
such that whether the program behaves correctly or not depends on the
order that the threads execute.  The name is a bit difficult to
motivate: the idea is that the program will ``happen to work'' if
certain threads during program execution ``win the race'' to do their
next operation before some other threads do their next operations.
Race conditions are very common bugs in concurrent programming that,
by definition, do not exist in sequential programming.  This section
defines two kinds of race conditions and explains why you must avoid
them.  Section~\ref{sec:guidelines} describes programming guidelines
that help you do so.

One kind of race condition is a \emph{bad interleaving}, sometimes
called a \emph{higher-level race} (to distinguish it from the second
kind).  Section~\ref{sec:basic-concurrency} showed some bad interleavings
and this section will show several more.  The key point is that ``what
is a bad interleaving'' \emph{depends entirely on what you are trying
  to do}.  Whether or not it is okay to interleave two bank-account
withdraw operations depends on some specification of how a bank
account is supposed to behave.

A \emph{data race} is a specific kind of race condition that is better
described as a ``simultaneous access error'' although nobody uses that
term.  There are two kinds of data races:
\begin{itemize}
\item When one thread might \emph{read} an object field at the same moment
  that another thread \emph{writes} the same field.
\item When one thread might \emph{write} an object field at the same moment
  that another thread also \emph{writes} the same field.
\end{itemize}
Notice it is \emph{not an error} for two threads to both \emph{read}
the same object field at the same time.  

As Section~\ref{sec:data-races} explains, \emph{your programs must
  never have data races even if it looks like a data race would not
  cause an error --- if your program has data races, the execution of
  your program is allowed to do very strange things.}

To better understand the difference between bad interleavings and data races,
consider the final version of the {\tt BankAccount} class from
Section~\ref{sec:basic-concurrency}.  If we accidentally omit the {\tt
  MethodImpl(MethodImplOptions.Synchronized)} attribute from the {\tt Withdraw} definition, the
program will have many bad interleavings, such as the first one we
considered in Section~\ref{sec:basic-concurrency}.  But since {\tt
  Balance} is still decorated with {\tt MethodImpl(MethodImplOptions.Synchronized)}, there
are no data races: Every piece of code that reads or writes {\tt
  balance} does so while holding the appropriate lock.  So there can
never be a simultaneous read/write or write/write of this field.

Now suppose {\tt Withdraw} is synchronized but we accidentally
omitted {\tt MethodImpl(MethodImplOptions.Synchronized)} from {\tt Balance} getter. 
Data races now exist and the program is wrong regardless of what a bank account is
supposed to do.  Figure~\ref{fig:data_race} depicts an example that
meets our definition: two threads reading/writing the same field at
potentially the same time.  (If both threads were writing, it would
also be a data race.)  Interestingly, simultaneous calls to {\tt
  Balance} getter do \emph{not} cause a data race because simultaneous
reads are not an error.  But if one thread calls {\tt Balance} at
the same time another thread sets {\tt Balance}, then a data race
occurs.  Even though {\tt Balance} setter is synchronized, {\tt
  Balance} getter is not, so they might read and write {\tt \_balance} at
the same time.  If your program has data races, it is infeasible to
reason about what might happen.

\begin{figure}
\begin{center}
\includegraphics[scale=.6]{images/data_race.png}
\end{center}
\caption{Visual depiction of a data race: Two threads accessing the
  same field of the same object, at least one of them writing to the
  field, without synchronization to ensure the accesses cannot happen,
  ``at the same time.''}
\label{fig:data_race}
\end{figure}

\subsection{Bad Interleavings: An Example with Stacks}
\label{sec:interleavings}

A useful way to reason about bad interleavings is to think about
\emph{intermediate states} that other threads must not ``see.''  Data
structures and libraries typically have \emph{invariants} necessary
for their correct operation.  For example, a {\tt size} field may need
to store the correct number of elements in a data structure.
Operations typically violate invariants temporarily, restoring the
invariant after making an appropriate change.  For example, an
operation might add an element to the structure \emph{and} increment
the size field.  No matter what order is used for these two steps,
there is an intermediate state where the invariant does not hold.  In
concurrent programming, our synchronization strategy often amounts to
using mutual exclusion to ensure that no thread can observe an
invariant-violating intermediate state produced by another thread.
What invariants matter for program correctness depends on the program.

Let's consider an extended example using a basic implementation of a
bounded-size stack.  (This is not an interesting data structure; the
point is to pick a small example so we can focus on the interleavings.
Section~\ref{sec:condvars} discusses an alternative to throwing
exceptions for this kind of structure.)
\begin{verbatim}
class Stack<E>
{
    private E[] array;
    private int index = 0;

    public Stack(int size)
    {
        array = new E[size];
    }

    public bool IsEmpty()
    {
        lock (this)
        {
            return index == 0;
        }
    }

    public void Push(E val)
    {
        lock (this)
        {
            if (index == array.Length)
            {
                throw new StackFullException();
            }
            array[index++] = val;
        }
    }

    public E Pop()
    {
        lock (this)
        {
            if (index == 0)
            {
                throw new StackEmptyException();
            }
            return array[--index];
        }
    }
}
\end{verbatim}
The key invariant is that if {\tt index$>$0} then {\tt index-1} is the
position of the most recently pushed item that has not yet been
popped.  Both {\tt Push} and {\tt Pop} temporarily violate this
invariant because they need to modify {\tt index} and the contents of
the array.  Even though this is done in one C\# statement, it does
not happen all at once.  By making each method synchronized, no thread
using these operations can see an incorrect intermediate state.  It
effectively ensures there is some global order of calls to {\tt
  IsEmpty}, {\tt Push}, and {\tt Pop}.  That order might differ across
different executions because scheduling is nondeterministic, but the
calls will not be interleaved.

Also notice there are no data races because {\tt array} and {\tt size}
are accessed only while holding the {\tt this} lock.  Not holding the
lock in the constructor is fine because a new object is not yet
reachable from any other thread.  The new object that the constructor
produces (i.e., the new instance of {\tt Stack}) will
have to be assigned to some thread-shared location before a second
thread could use it, and such an assignment will not happen until
after the constructor finishes executing.\footnote{This would not
  necessarily be the case if the constructor did something like 
  {\tt someObject.F = this;}, but doing such things in constructors is
  usually a bad idea.}

Now suppose we want to implement a new operation for our stack called
{\tt Peek} that returns the newest
not-yet-popped element in the stack without popping it.  (This
operation is also sometimes called {\tt Top}.) A correct
implementation would be:
\begin{verbatim}
public E Peek()
{
    lock (this)
    {
        if (index == 0)
        {
            throw new StackEmptyException();
        }
        return array[index - 1];
    }
}
\end{verbatim}
If we omit the lock, then this operation would
cause data races with simultaneous {\tt Push} or {\tt Pop} operations.

Consider instead this alternate also-correct implementation:
\begin{verbatim}
public E Peek()
{
    lock (this)
    {
        E ans = Pop();
        Push(ans);
        return ans;
    }
}
\end{verbatim}
This version is perhaps worse style, but it is certainly correct.  It
also has the advantage that this approach could be taken by a helper
method outside the class where the first approach could not since {\tt
  array} and {\tt index} are private fields:
\begin{verbatim}
class CorrectStackHelper
{
    static E PeekHelper<E>(Stack<E> s)
    {
        lock (s)
        {
            E ans = s.Pop();
            s.Push(ans);
            return ans;
        }
    }
}
\end{verbatim}
Notice this version could not be written if stacks used some private
inaccessible lock.  Also notice that it relies on reentrant locks.
However, we will consider instead this WRONG version where the helper
method omits its own synchronized statement:
\begin{verbatim}
class WrongStackHelper
{
    static E PeekHelper<E>(Stack<E> s)
    {
        E ans = s.Pop();
        s.Push(ans);
        return ans;
    }
}
\end{verbatim}
Notice this version has no data races because the {\tt Pop} and {\tt
  Push} calls still acquire and release the appropriate lock.  Also
  notice that {\tt WrongStackHelper.PeekHelper<>} uses the stack operators exactly as
  it is supposed to.  Nonetheless, it is incorrect because
  a peek operation is not supposed to modify the stack.  While
  the \emph{overall result} is the same stack if the code runs without
  interleaving from other threads, the code produces an
  \emph{intermediate state} that other threads should not see or modify.  This
  intermediate state would not be observable in a single-threaded
  program or in our correct version that made the entire method body a
  critical section.

To show why the wrong version can lead to incorrect behavior, we can
demonstrate interleavings with other operations such that the stack
does the wrong thing.  Writing out such interleavings is good practice
for reasoning about concurrent code and helps determine what needs to
be in a critical section.  As it turns out in our small example, 
{\tt WrongStackHelper} causes race conditions with all other stack
operations, including itself.

\medskip
\noindent\textbf{{\tt WrongStackHelper} and {\tt IsEmpty}:}
\medskip

If a stack is not empty, then {\tt IsEmpty} should return {\tt false}.
Suppose two threads share a stack {\tt stk} that has one element in
it.  If one thread calls {\tt IsEmpty} and the other calls {\tt
  WrongStackHelper.PeekHelper<>}, the first thread can get the wrong result:
\begin{verbatim}
Thread 1                    Thread 2 (calls WrongStackHelper.PeekHelper<>)
--------                    ----------------------------------
                            E ans = stk.Pop();
bool b = IsEmpty();
                            stk.Push(ans);
                            return ans;
\end{verbatim}
Figure~\ref{fig:stack_states} shows how this interleaving produces the
wrong result when the stack has one element.  Notice there is nothing wrong with what ``C\# is doing'' here.  It is
the programmer who wrote {\tt WrongStackHelper.PeekHeper<>} such that it
does not behave like a peek operation should.  Yet the code that gets the
wrong answer is {\tt IsEmpty}, which is another reason debugging
concurrent programs is difficult.

\begin{figure}
\begin{center}
\includegraphics[scale=0.6]{images/stack_states.png}
\end{center}
\caption{A bad interleaving for a stack with a peek operation that is
  incorrect in a concurrent program.}
\label{fig:stack_states}
\end{figure}


\medskip
\noindent\textbf{{\tt WrongStackHelper} and {\tt Push}:}
\medskip

Another key property of stacks is that they return elements in
last-in-first-out order.  But consider this interleaving, where one of
Thread 1's {\tt Push} operations happens in the middle of a concurrent
call to {\tt WrongStackHelper.PeekHelper<>}.
\begin{verbatim}
Thread 1                    Thread 2 (calls WrongStackHelper.PeekHelper<>)
--------                    ----------------------------------
stk.Push(x);
                            E ans = stk.Pop();
stk.Push(y);
                            stk.Push(ans);
                            return ans;
E z = stk.Pop();
\end{verbatim}
This interleaving has the effect of reordering the top two elements of
the stack, so {\tt z} ends up holding the wrong value.

\medskip
\noindent\textbf{{\tt WrongStackHelper} and {\tt Pop}:}
\medskip

Similar to the previous example, a {\tt Pop} operation (instead of a
{\tt Push}) that views the intermediate state of\\ {\tt
  WrongStackHelper.PeekHelper<>} can get the wrong (not last-in-first-out) answer:
\begin{verbatim}
Thread 1                    Thread 2 (calls WrongStackHelper.PeekHelper<>)
--------                    ----------------------------------
stk.Push(x);
stk.Push(y);
                            E ans = stk.Pop();
E z = stk.Pop();
                            stk.Push(ans);
                            return ans;
\end{verbatim}

\medskip
\noindent\textbf{{\tt WrongStackHelper} and {\tt WrongStackHelper}:}
\medskip

Finally, two threads both calling {\tt WrongStackHelper.PeekHelper<>} causes race
conditions.  First, if {\tt stk} initially has one element, then
a bad interleaving of the two calls could raise a {\tt
  StackEmptyException}, which should happen only when a stack has zero
elements:
\begin{verbatim}
Thread 1                    Thread 2 (calls WrongStackHelper.PeekHelper<>)
--------                    ----------------------------------
E ans = stk.Pop();
                            E ans = stk.Pop(); // exception!
stk.Push(ans);
return ans;
\end{verbatim}
Second, even if the stack has more than 1 element, we can extend the
interleaving above to swap the order of the top two elements, which
will cause later {\tt Pop} operations to get the wrong answer.
\begin{verbatim}
Thread 1                    Thread 2 (calls WrongStackHelper.PeekHelper<>)
--------                    ----------------------------------
E ans = stk.Pop();
                            E ans = stk.Pop();
stk.Push(ans);
return ans;
                            stk.Push(ans);
                            return ans;
\end{verbatim}

\bigskip

In general, bad interleavings can involve any number of threads using
any operations.  The defense is to define large enough critical
sections such that every possible thread schedule is correct.  Because
{\tt Push}, {\tt Pop}, and {\tt IsEmpty} are synchronized, we do not
have to worry about calls to these methods being interleaved with each
other.  To implement {\tt Peek} correctly, the key insight is to
realize that its intermediate state must not be visible and therefore
we need its calls to {\tt Pop} and {\tt Push} to be part of \emph{one}
critical section instead of two separate critical sections.  As
Section~\ref{sec:guidelines} discusses in more detail, making your
critical sections large enough --- but not too large --- is an
essential task in concurrent programming.

\subsection{Data Races: Wrong Even When They Look Right}
\label{sec:data-races}

This section provides more detail on the rule that data races ---
simultaneous read/write or write/write of the same field --- must be
avoided even if it ``seems like'' the program would be correct
anyway.  

For example, consider the {\tt Stack} implementation from Section~\ref{sec:interleavings}
and suppose we omit synchronization from the {\tt IsEmpty} method.
Given everything we have discussed so far, doing so seems okay, if
perhaps a bit risky.  After all, {\tt IsEmpty} only reads the {\tt
  index} field and since all the other critical sections write
to {\tt index} at most once, it cannot be that {\tt IsEmpty} observes an
intermediate state of another critical section: it would see {\tt
  index} either before or after it was updated --- and both
possibilities remain when {\tt IsEmpty} is properly synchronized.

Nonetheless, omitting synchronization introduces data races with
concurrent {\tt Push} and {\tt Pop} operations.  As soon as a C\#
program has a data race, it is extremely difficult to reason about
what might happen.  (The author of these notes is genuinely unsure if
the stack implementation is correct with {\tt IsEmpty}
unsynchronized.)  In the C++11 standard, it is
\emph{impossible} to do such reasoning: any program with a data race
is as wrong as a program with an array-bounds violation.  So data
races must be avoided.  The rest of this section gives some indication
as to \emph{why} and a little more detail on \emph{how} to avoid data
races.

\subsubsection{Inability to Reason In the Presence of Data Races}
\label{sec:data-race-example}

Let's consider an example that is simpler than the stack
implementation but is so simple that we won't motivate why anyone
would write code like this:
\begin{verbatim}
class ClassWithDataRaces
{
    private int x = 0;
    private int y = 0;

    void F()
    {
        x = 1; // line A
        y = 1; // line B
    }

    void G()
    {
        int a = x; // line C
        int b = y; // line D
        Debug.Assert(b >= a);
    }
}
\end{verbatim}
Notice {\tt F} and {\tt G} are not synchronized, leading to potential
data races on fields {\tt x} and {\tt y}.  Therefore, it turns out the
assertion in {\tt G} can fail.  But there is no interleaving of
operations that justifies the assertion failure!  Here we prove that
all interleavings are correct.  Then Section~\ref{sec:data-race-why}
explains why the assertion can fail anyway.

\begin{itemize}
\item If a call to {\tt G} begins after at least one call to {\tt F}
  has ended, then {\tt x} and {\tt y} are both 1, so the assertion
  holds.
\item Similarly, if a call to {\tt G} completes before any call to
  {\tt F} begins, then {\tt x} and {\tt y} are both 0, so the
  assertion holds.
\item So we need only concern ourselves with a call to {\tt G} that is
  interleaved with the first call to {\tt F}.  One way to proceed is
  to consider all possible interleavings of the lines marked A, B, C,
  and D and show the assertion holds under all of them.  Because A
  must precede B and C must precede D, there are only 6 such
  interleavings: ABCD, ACBD, ACDB, CABD, CADB, CDAB.  Manual
  inspection of all six possibilities completes the proof.  
  
  A more illuminating approach is a proof by contradiction: Assume the
  assertion fails, meaning {\tt !(b>=a)}.  Then {\tt a==1} and {\tt
    b==0}.  Since {\tt a==1}, line B happened before line C\@.  Since
  (1) A must happen before B, (2) C must happen before D, and (3) ``happens
  before'' is a transitive relation, A must happen before D\@.  But
  then {\tt b==1} and the assertion holds.
\end{itemize}

There is nothing wrong with the proof except its assumption that we can
reason in terms of ``all possible interleavings'' or that everything
happens in certain orders.  We can reason this way \emph{only} if the
program has no data races.

\subsubsection{Partial Explanation of Why Data Races Are Disallowed}
\label{sec:data-race-why}

To understand why one cannot always reason in terms of interleavings
requires taking courses in compilers and/or computer architecture.

Compilers typically perform \emph{optimizations} to execute code
faster without changing its meaning.  If we required compilers to
never change the possible interleavings, then it would be too
difficult for compilers to be effective.  Therefore, language
definitions allow compilers to do things like execute line B above
before line A.  Now, in this simple example, there is no reason why
the compiler \emph{would} do this reordering.  The point is that it is
\emph{allowed to}, and in more complicated examples with code
containing loops and other features there are reasons to do so.  Once
such a reordering is allowed, the assertion is allowed to fail.
Whether it will ever do so depends on exactly how a particular C\#
implementation works.

Similarly, in hardware, there isn't \emph{really} one single shared
memory containing a single copy of all data in the program.  Instead
there are various caches and buffers that let the processor access
some memory faster than other memory.  As a result, the hardware has
to keep track of different copies of things and move things around.
As it does so, memory operations might not become ``visible'' to other
threads in the order they happened in the program.  As with compilers,
requiring the hardware to expose all reads and writes in the exact
order they happen is considered too onerous from a performance
perspective.

To be clear, compilers and hardware cannot just ``do whatever they
want.''  All this reordering is \emph{completely hidden from you} and
you never need to worry about it \emph{if} you avoid data races.  This
issue is irrelevant in single-threaded programming because with one
thread you can never have a data race.

\subsubsection{The Grand Compromise}

One way to summarize the previous section is in terms of a ``grand
compromise'' between programmers who want to be able to reason easily
about what a program might do and compiler/hardware implementers who
want to implement things efficiently and easily:
\begin{itemize}
\item The programmer promises not to write data races.
\item The implementers promise that if the programmer does his/her
  job, then the implementation will be indistinguishable from one that
  does no reordering of memory operations.  That is, it will preserve
  the illusion that all memory operations are interleaved in a global
  order.
\end{itemize}

Why is this compromise the state of the art rather than a more extreme
position of requiring the implementation to preserve the
``interleaving illusion'' even in the presence of data races?  In
short, it would make it much more difficult to develop compilers and
processors.  And for what?  Just to support programs with data races,
which are almost always bad style and difficult to reason about
anyway!  Why go to so much trouble just for ill-advised programming
style?  On the other hand, if your program accidentally has data
races, it might be more difficult to debug.

\subsubsection{Avoiding Data Races}

To avoid data races, we need synchronization.  Given C\# locks,
which we already know about, we could rewrite our example from
Section~\ref{sec:data-race-example} as follows:
\begin{verbatim}
class LockNoDataRaces
{
    private int x = 0;
    private int y = 0;

    void F()
    {
        lock (this) { x = 1; } // line A
        lock (this) { y = 1; } // line B
    }

    void G()
    {
        int a, b;
        lock (this) { a = x; } // line C
        lock (this) { b = y; } // line D
        Debug.Assert(b >= a);
    }
}
\end{verbatim}
In practice you might choose to implement each method with one
critical section instead of two, but using two is still sufficient to
eliminate data races, which allows us to reason in terms of the six
possible interleavings, which ensures the assertion cannot fail.  It
would even be correct to use two different locks, one to protect {\tt
  x} (lines A and D) and another to protect {\tt y} (lines B and C).
The bottom line is that by avoiding data races we can reason in terms
of interleavings (ignoring reorderings) because we have fulfilled our
obligations under The Grand Compromise.

There is a second way to avoid data races when writing tricky code
that depends on the exact ordering of reads and writes to fields.
Instead of using locks, we can declare fields to be \emph{volatile}.
By \emph{definition}, accesses to volatile fields \emph{do not count
  as data races}, so programmers using volatiles are still upholding
their end of the grand compromise.  In fact, this definition is the
reason that {\tt volatile} is a keyword in C\# --- and the reason
that until you study concurrency you will probably not run across it.
Reading/writing a volatile field is less efficient than
reading/writing a regular field, but more efficient than acquiring and
releasing a lock.  For our example, the code looks like this:
\begin{verbatim}
class VolatileNoDataRaces
{
    private volatile int x = 0;
    private volatile int y = 0;

    void F()
    {
        x = 1; // line A
        y = 1; // line B
    }

    void G()
    {
        int a = x; // line C
        int b = y; // line D
        Debug.Assert(b >= a);
    }
}
\end{verbatim}

Volatile fields are typically used only by concurrency experts.  They
can be convenient when concurrent code is sharing only a single
field.  Usually with more than one field you need critical sections
longer than a single memory access, in which case the right tool is a
lock, not a volatile field.

\subsubsection{A More Likely Example}

Since the previous discussion focused on a strange and unmotivated
example (who would write code like {\tt F} and {\tt G}?), this section
concludes with a data-race example that would arise naturally.

Suppose we want to perform some expensive iterative computation, for
example, rendering a really ugly monster for a video game, until we
have achieved a perfect solution or another thread informs us that the
result is ``good enough'' (it's time to draw the monster already) or
no longer needed (the video-game player has left the room with the
monster).  We could have a boolean {\tt stop} field that the expensive
computation checks periodically:
\begin{verbatim}
class MonsterDraw
{
    bool stop = false;
    void Draw(...)
    {
        while(!stop)
        {
          ... keep making uglier ...
        }
        ...
    }
}
\end{verbatim}
Then one thread would execute {\tt Draw} and other threads could set
{\tt stop} to true as necessary.  Notice that doing so introduces data
races on the {\tt stop} field.  To be honest, even though this
programming approach is wrong, with most C\# implementations it will
probably work.  But if it does not, it is the programmer's fault.
Worse, the code might work for a long time, but when the C\#
implementation changes or the compiler is run with different pieces of
code, the idiom might stop working.  Most likely what would happen is
the {\tt Draw} method would never ``see'' that {\tt stop} was changed
and it would not terminate.

The simplest ``fix'' to the code above is to declare {\tt stop}
volatile.  However, many concurrency experts would consider this idiom
poor style and would recommend learning more about C\# thread library, 
which has built-in support for having one thread
``interrupt'' another one.  C\# concurrency libraries are large
with built-in support for many coding patterns.  This is just one
example of a pattern where using the existing libraries is ``the right
thing to do,'' but we do not go into the details of the libraries in
these notes.

\section{Concurrency Programming Guidelines}
\label{sec:guidelines}

This section does not introduce any new primitives for concurrent
programming.  Instead, we acknowledge that writing correct concurrent
programs is difficult --- just saying, ``here is how locks work, try
to avoid race conditions but still write efficient programs'' would
leave you ill-equipped to write non-trivial multithreaded programs.
Over the decades that programmers have struggled with shared memory
and locks, certain guidelines and methodologies have proven useful.
Here we sketch some of the most widely agreed upon approaches to
getting your synchronization correct.  Following them does not make
concurrency easy, but it does make it much easier.  We focus on how to
design/organize your code, but related topics like how to use tools to
test and debug concurrent programs are also important.

\subsection{Conceptually Splitting Memory in Three Parts}

Every memory location in your program (for example an object field),
should meet \emph{at least one} of the following three criteria:

\begin{enumerate}
\item It is \emph{thread-local}: Only one thread ever accesses it.
\item It is \emph{immutable}: (After being initialized,) it is only
  read, never written.
\item It is \emph{synchronized}: Locks are used to ensure there are no
  race conditions.
\end{enumerate}

Put another way, memory that is thread-local or immutable is
irrelevant to synchronization.  You can ignore it when considering how
to use locks and where to put critical sections.  So the more memory
you have that is thread-local or immutable, the easier concurrent
programming will be.  It is often worth changing the data structures
and algorithms you are using to choose ones that have less sharing of
mutable memory, leaving synchronization only for the unavoidable
communication among threads.  We cannot emphasize enough that each
memory location needs to meet at least one of the three conditions
above to avoid race conditions, and if it meets either of the first
two (or both), then you do not need locks.

So a powerful guideline is to make as many objects as you can
immutable or thread-local or both.  Only when doing so is ineffective
for your task do you need to figure out how to use synchronization
correctly.  Therefore, we next consider ways to make more memory
thread-local and then consider ways to make more memory immutable.

%% In terms of a Venn diagram, we can view memory as containing these
%% sets:

%% \begin{center}
%% \includegraphics[scale=.5]{images/memoryVenn.jpg}
%% \end{center}

%% In these terms, the suggestion is to make the thread-local memory and
%% immutable memory sets as large as possible.  We consider approaches
%% for the two sets separately:

\medskip
\noindent\textbf{More Thread-Local Memory}
\medskip

Whenever possible, do not share resources.  Instead of having one
resource, have a separate (copy of the) resource for each thread.  For
example, there is no need to have one shared {\tt Random} object for
generating pseudorandom numbers, since it will be just as random for
each thread to use its own object.  Dictionaries storing
already-computed results can also be thread-local: This has the
disadvantage that one thread will not insert a value that another
thread can then use, but the decrease in synchronization is often
worth this trade-off.

Note that because each thread's call-stack is thread-local, there is
never a need to synchronize on local variables.  So using local
variables instead of object fields can help where possible, but this
is good style anyway even in single-threaded programming.

Overall, the vast majority of objects in a typical concurrent program
should and will be thread-local.  It may not seem that way in these
notes, but only because we are focusing on the shared objects, which
are the difficult ones.  In conclusion, do not share objects unless
those objects really have the \emph{explicit purpose} of enabling
shared-memory communication among threads.

\medskip
\noindent\textbf{More Immutable Memory}
\medskip

Whenever possible, do not update fields of objects: Make new objects
instead.  This guideline is a key tenet of \emph{functional
  programming}, which unfortunately is typically not taught in the
same course as data structures or concurrency.  Even in
single-threaded programs, it is often helpful to avoid mutating fields
of objects because other too-often-forgotten-about \emph{aliases} to
that object will ``see'' any mutations performed.

Let's consider a small example that demonstrates some of the issues.
Suppose we have a dictionary such as a hashtable that maps student ids
to names where the name class is represented like this:
\begin{verbatim}
class Name
{
    public string First;
    public string Middle;
    public string Last;

    public Name(string f, string m, string l)
    {
        First = f;
        Middle = m;
        Last = l;
    }

    public override string ToString()
    {
        return First + " " + Middle + " " + Last;
    }
}
\end{verbatim}
Suppose we want to write a method that looks up a student by id in 
some table and returns the name, but using a middle initial instead of
the full middle name.  The following direct solution is probably the
BEST STYLE:
\begin{verbatim}
    Name n = table.Lookup(id);
    return n.First + " " + n.Middle[0] + " " + n.Last;
\end{verbatim}
Notice that the computation is almost the same as that already done by
{\tt ToString}.  So if the computation were more complicated than just
a few string concatenations, it would be tempting and arguably
reasonable to try to reuse the {\tt ToString} method.  This approach
is a BAD IDEA:
\begin{verbatim}
    Name n = table.Lookup(id);
    n.Middle = n.Middle.Substring(0, 1);
    return n.ToString();
\end{verbatim}
While this code produces the right answer, it has the
\emph{side-effect} of changing the name's {\tt Middle} field.  Worse,
the Name object is \emph{still in the table}.  Actually, this depends on how the
{\tt Lookup} method is implemented.  We assume here it returns a
reference to the object in the table, rather than creating a new 
  Name object.  If Name objects are not mutated, then returning an
  alias is simpler and more efficient.  Under this assumption, the
  full middle name for some student has been \emph{replaced} in the
  table by the middle initial, which is presumably a bug.

In a SINGLE-THREADED PROGRAM, the following workaround is correct but
POOR STYLE:
\begin{verbatim}
    Name n = table.Lookup(id);
    String m = n.Middle;
    n.Middle = m.Substring(0, 1);
    String ans = n.ToString();
    n.Middle = m;
    return ans;
\end{verbatim}
Again, this is total overkill for our example, but the idea makes
sense: undo the side effect before returning.

But if the table is shared among threads, the APPROACH ABOVE IS WRONG.
Simultaneous calls using the same ID would have data races.  Even if
there were no data races, there would be interleavings where other
operations might see the intermediate state where the {\tt Middle}
field held only the initial.  The point is that for shared memory, you
cannot perform side effects and undo them later: later is too late in the
presence of concurrent operations.  This is a major reason that the
functional-programming paradigm is even more useful with concurrency.

The solution is to make a copy of the object and, if necessary, mutate the
(thread-local) copy.  So this WORKS:
\begin{verbatim}
    Name n1 = table.Lookup(id);
    Name n2 = new Name(n1.First, n1.Middle.Substring(0, 1), n1.Last);
    return n2.ToString();
\end{verbatim}

Note that it is no problem for multiple threads to lookup up the same
Name object at the same time.  They will all \emph{read} the same
fields of the same object (they all have aliases to it), but
simultaneous reads never lead to race conditions.  It is like
multiple pedestrians reading the same street sign at the same time:
They do not bother each other.

\subsection{Approaches to Synchronization}

Even after maximizing the thread-local and immutable objects, there
will remain some shared resources among the threads (else threads have
no way to communicate).  The remaining guidelines suggest ways to
think about correctly synchronizing access to these resources.

\medskip
\noindent\textbf{Guideline \#0: Avoid data races}  
\medskip

\emph{Use locks to ensure that two threads never simultaneously
  read/write or write/write the same field.}  While guideline \#0 is
\emph{necessary} (see Section~\ref{sec:data-races}), it is not
\emph{sufficient} (see Section~\ref{sec:interleavings} and other
examples of wrong programs that have no data races).

\medskip
\noindent\textbf{Guideline \#1: Use consistent locking}  
\medskip

\emph{For each location that needs synchronization, identify a lock
  that is always held when accessing that location.}  If some lock $l$
  is always held when memory location $m$ is read or written, then we
  say that $l$ \emph{guards} the location $m$.  It is extremely good
  practice to document (e.g., via comments) for each field needing
  synchronization what lock guards that location.  For example, one
  possible situation would be, ``{\tt this} guards all
  fields of instances of the class.''

  Note that each location would have exactly one guard, but one lock can
  guard multiple locations.  A simple example would be multiple fields
  of the same object, but any mapping from locks to locations is
  okay.  For example, perhaps all nodes of a linked list are protected
  by one lock.  Another way to think about consistent locking is
  to imagine that all the objects needing synchronization are
  \emph{partitioned}.  Each piece of the partition has a lock that
  guards all the locations in the piece.
  
  Consistent locking is \emph{sufficient} to prevent data races, but
  is \emph{not sufficient} to prevent bad interleavings.  For example,
  the extended example in Section~\ref{sec:interleavings} used
  consistent locking.
  
  Consistent locking is \emph{not necessary} to prevent data races nor
  bad interleavings.  It is a guideline that is typically the default
  assumption for concurrent code, violated only when carefully
  documented and for good reason.  One good reason is when the program
  has multiple conceptual ``phases'' and all threads globally
  coordinate when the program moves from one phase to another.  In
  this case, different phases can use different synchronization
  strategies.

  For example, suppose in an early phase multiple threads are
  inserting different key-value pairs into a dictionary.  Perhaps
  there is one lock for the dictionary and all threads acquire this
  lock before performing any dictionary operations.  Suppose that at
  some point in the program, the dictionary becomes fixed, meaning no
  more insertions or deletions will be performed on it.  Once all threads
  \emph{know} this point has been reached (probably by reading some
  other synchronized shared-memory object to make sure that all
  threads are done adding to the dictionary), it would be correct to
  perform subsequent lookup operations \emph{without synchronization}
  since the dictionary has \emph{become immutable}.

\medskip
\noindent\textbf{Guideline \#2: Start with coarse-grained locking and
  move to finer-grained locking only if contention is hurting performance}
\medskip

This guideline introduces some new terms.  \emph{Coarse-grained
  locking} means using fewer locks to guard more objects.  For
example, one lock for an entire dictionary or for an entire array of
bank accounts would be coarse-grained.  Conversely, \emph{fine-grained
  locking} means using more locks, each of which guards fewer memory
locations.  For example, using a separate lock for each bucket in a
chaining hashtable or a separate lock for each bank account would be
fine-grained.  Honestly, the terms ``coarse-grained'' and
``fine-grained'' do not have a strict dividing line: we can really
only say that one locking strategy is ``more coarse-grained'' or
``more fine-grained'' than another one.  That is, \emph{locking
  granularity} is really a continuum, where one direction is coarser
and the other direction is finer.

Coarse-grained locking is typically easier.  After grabbing just one
lock, we can access many different locations in a critical section.
With fine-grained locking, operations may end up needing to acquire
multiple locks (using nested synchronized statements).  It is easy to
forget to acquire a lock or to acquire the wrong lock.  It also
introduces the possibility of deadlock (Section~\ref{sec:deadlock}).

But coarse-grained locking leads to threads waiting for other threads
to release locks unnecessarily, i.e., in situations when no errors
would result if the threads proceeded concurrently.  In the extreme,
the coarsest strategy would have just one lock for the entire program!
Under this approach, it is obvious what lock to acquire, but no two
operations on shared memory can proceed in parallel.  Having one lock
for many, many bank accounts is probably a bad idea because with
enough threads, there will be multiple threads attempting to access
bank accounts at the same time and they will be unable to.
\emph{Contention} describes the situation where threads are blocked
waiting for each other: They are \emph{contending} (in the sense of
competing) for the same resource, and this is hurting performance.  If
there is little contention (it is rare that a thread is blocked), then
coarse-grained locking is sufficient.  When contention becomes
problematic, it is worth considering finer-grained locking, realizing
that changing the code without introducing race conditions is
difficult.  A good execution profiler\footnote{A profiler is a tool
  that reports where a run of a program is spending time or other resources.} should provide information on
where threads are most often blocked waiting for a lock to be
released.

\medskip
\noindent\textbf{Guideline \#3: Make critical sections large enough
  for correctness but no larger.  Do not perform I/O or expensive
  computations within critical sections.}
\medskip

The granularity of critical sections is completely orthogonal to the
granularity of locks (guideline \#2).  Coarse-grained critical
sections are ``large'' pieces of code whereas fine-grained critical
sections are ``small'' pieces of code.  However, what matters is
\emph{not} how many lines of code a critical section occupies.  What
matters is how long a critical section takes to execute (longer means
larger) and how many shared resources such as memory locations it
accesses (more means larger).

If you make your critical sections too short, you are introducing more
possible interleavings and potentially incorrect race conditions.  For
example, compare:
\begin{verbatim}
lock (lk) { /* do first thing */ }
/* do second thing */
lock (lk) { /* do third thing */ }
\end{verbatim}
with:
\begin{verbatim}
lock (lk)
{
    /* do first thing */
    /* do second thing */
    /* do third thing */
}
\end{verbatim}
The first version has smaller critical sections.  Smaller critical
sections lead to less contention.  For example, other threads can use
data guarded by {\tt lk} while the code above is doing the ``second
thing.''  But smaller critical sections expose more states to other
threads.  Perhaps only the second version with one larger critical
section is correct.  This guideline is therefore a tough one to follow
because it requires moderation: make critical sections no longer or
shorter than necessary.

Basic operations like assignment statements and method calls are so
fast that it is almost never worth the trouble of splitting them into
multiple critical sections.  However, expensive computations inside
critical sections should be avoided wherever possible for the obvious
reason that other threads may end up blocked waiting for the lock.
Remember in particular that reading data from disk or the network is
typically orders of magnitude slower than reading memory.  Long
running loops and other computations should also be avoided.

\emph{Sometimes} it is possible to reorganize code to avoid long
critical sections by repeating an operation in the (hopefully) rare
case that it is necessary.  For example, consider this large critical
section that replaces a table entry by performing some computation on
its old value:
\begin{verbatim}
lock (lk)
{
    v1 = table.Lookup(k);
    v2 = Expensive(v1);
    table.Remove(k);
    table.Insert(k, v2);
}
\end{verbatim}
We assume the program is incorrect if this critical section is any
smaller: We need other threads to see either the old value in the
table or the new value.  And we need to compute {\tt v2} using the
current {\tt v1}.  So this variation would be WRONG:
\begin{verbatim}
lock (lk)
{
    v1 = table.Lookup(k);
}
v2 = Expensive(v1);
lock (lk)
{
    table.Remove(k);
    table.Insert(k, v2);
}
\end{verbatim}
If another thread updated the table to map {\tt k} to some {\tt v3}
while this thread was executing {\tt Expensive(v1)}, then we would not
be performing the correct computation.  In this case, either the final
value in the table should be {\tt v3} (if that thread ran second) or
the result of {\tt Expensive(v3)} (if that thread ran first), but
never {\tt v2}.

However, this more complicated version would WORK in this situation,
under the CAVEAT described below:
\begin{verbatim}
bool loop_done = false;
while (!loop_done)
{
    lock (lk)
    {
        v1 = table.Lookup(k);
    }
    v2 = Expensive(v1);
    lock (lk)
    {
        if (table.Lookup(k) == v1)
        {
            loop_done = true;
            table.Remove(k);
            table.Insert(k, v2);
        }
    }
}
\end{verbatim}
The essence of the trick is the second critical section.  If {\tt
  table.Lookup(k) == v1}, then we know what {\tt Expensive(v1)} would
  compute, having already computed it!  So the critical section is
  small, but does the same thing as our original large critical
  section.  But if {\tt table.Lookup(k) != v1}, then our precomputation
  did useless work and the while-loop starts over.  If we expect
  concurrent updates to be rare, then this approach is a ``good
  performance bet'' while still correct in all cases.
  
  As promised, there is an important caveat to mention.  This approach
  does \emph{not} ensure the table entry for key {\tt k} was
  unmodified during the call to {\tt Expensive}.  There could have
  been any number of concurrent updates, removals, insertions, etc.
  with this key.  All we know is that when the second critical section
  executes, the key {\tt k} has value {\tt v1}.  Maybe it was
  unchanged or maybe it was changed and then changed back.

In our example, we assumed it did not matter.  There are many
situations where it \emph{does} matter and in such cases, checking the
value is wrong because it does not ensure the entry has not been
modified.  This is known as an ``A-B-A'' problem, meaning the value
started as A, changed to B, and changed back to A, causing some
incorrect code to conclude, wrongly, that it was never changed.

\medskip
\noindent\textbf{Guideline \#4: Think in terms of what operations need
to be \emph{atomic}.  Determine a locking strategy after you know what
the critical sections are.}
\medskip

An operation is \emph{atomic}, as in indivisible, if it executes
either entirely or not at all, with no other thread able to observe
that it has partly executed.  (In the databases literature, this would
be called atomic and \emph{isolated}, but it is common in concurrent
programming to conflate these two ideas under the term \emph{atomic}.)
Ensuring necessary operations appear to be atomic is exactly why we
have critical sections.  Since this is the essential point, this
guideline recommends thinking first in terms of what the critical
sections are \emph{without thinking about locking granularity}.  Then,
once the critical sections are clear, how to use locks to implement
the critical sections can be the next step.

In this way, we ensure we actually develop the software we want ---
the correct critical sections --- without being distracted prematurely
by the implementation details of how to do the locking.  In other
words, think about atomicity (using guideline \#3) first and the
locking protocol (using guidelines \#0, \#1, and \#2) second.

Unfortunately, one of the most difficult aspects of lock-based
programming is when software needs change.  When new critical sections
are needed in ``version 2.0'' of a program, it may require changing
the locking protocol, which in turn will require carefully modifying
the code for many other already-working critical sections.
Section~\ref{sec:deadlock} even includes an example where there is
really no good solution.  Nonetheless, following guideline \#4 makes
it easier to think about why and how a locking protocol may need
changing.

\goodbreak
\medskip
\noindent\textbf{Guideline \#5: Do not implement your own concurrent
  data structures.  Use carefully tuned ones written by experts and
  provided in standard libraries.}
\medskip

Widely used libraries for popular programming languages already
contain many reusable data structures that you should use whenever
they meet your needs.  For example, there is little reason in C\# to
implement your own dictionary since the {\tt Dictionary} class in the
standard library has been carefully tuned and tested.  For concurrent
programming, the advice to ``reuse existing libraries'' is even more
important, precisely because writing, debugging, and testing
concurrent code is so difficult.  Experts who devote their lives to
concurrent programming can write tricky fine-grained locking code once
and the rest of us can benefit.  For example, the {\tt
  ConcurrentDictionary} class implements a dictionary that can be safely
used by multiple threads with very little contention.  It
uses some techniques more advanced than discussed in these notes, but
clients to the library have such trickiness hidden from them.  For
basic things like queues and dictionaries, do not implement your own.

If standard libraries are so good, then why learn about concurrent
programming at all?  For the same reason that educated computer
scientists need a basic understanding of sequential data structures in
order to pick the right ones and use them correctly (e.g., designing a
good hash function), concurrent programmers need to understand how to
debug and performance tune their use of libraries (e.g., to avoid
contention).

Moreover, standard libraries do not typically handle all the necessary
synchronization for an application.  For example, {\tt
  ConcurrentDictionary} does not support atomically removing one element
and inserting two others.  If your application needs to do that, then
you will need to implement your own locking protocol to synchronize
access to a shared data structure.  Understanding race conditions is
crucial.  While the examples in these notes consider race conditions
on simple data structures like stacks, larger applications using
standard libraries for concurrent data structures will still often
have bad interleavings at higher levels of abstraction.

\section{Deadlock}
\label{sec:deadlock}

There is another common concurrency bug that you must avoid when
writing concurrent programs: If a collection of threads are blocked
forever, all waiting for another thread in the collection to do
something (which it won't because it's blocked), we say the threads
are \emph{deadlocked}.  Deadlock is a different kind of bug from a
race condition, but unfortunately preventing a potential race
condition can cause a potential deadlock and vice-versa.  Deadlocks
typically result only with some, possibly-rare, thread schedules
(i.e., like race conditions they are nondeterministic).  One
advantage compared to race conditions is that it is easier to tell a
deadlock occurred: Some threads never terminate because they are
blocked.

As a canonical example of code that could cause a deadlock, consider a
bank-account class that includes a method to transfer money to another
bank-account.  With what we have learned so far, the following WRONG
code skeleton looks reasonable:
\begin{verbatim}
class BankAccount
{
    [MethodImpl(MethodImplOptions.Synchronized)]
    void Withdraw(int amt) { }

    [MethodImpl(MethodImplOptions.Synchronized)]
    void Deposit(int amt) { }

    [MethodImpl(MethodImplOptions.Synchronized)]
    void TransferToWrong(int amt, BankAccount a)
    {
        this.Withdraw(amt);
        a.Deposit(amt);
    }
}
\end{verbatim}
This class uses a fine-grained locking strategy where each account has
its own lock.  There are no data races because the only methods that
directly access any fields are (we assume) synchronized like {\tt
  Withdraw} and {\tt Deposit}, which acquire the correct lock.  More
subtly, the {\tt TransferTo} method appears atomic.  For another
thread to see the intermediate state where the withdraw has occurred
but not the deposit would require operating on the withdrawn-from
account.  Since {\tt TransferTo} is synchronized, this cannot occur.

Nonetheless, there are interleavings of two {\tt TransferTo}
operations where neither one ever finishes, which is clearly not
desired.  For simplicity, suppose {\tt x} and {\tt y} are static
fields each holding a {\tt BankAccount}.  Consider this interleaving
in pseudocode:

\begin{verbatim}
Thread 1: x.TransferTo(1,y)      Thread 2: y.TransferTo(1,x)
--------------------------       ---------------------------
acquire lock for x     
withdraw 1 from x
                                 acquire lock for y
                                 withdraw 1 from y
                                 block on lock for x
block on lock for y
\end{verbatim}

In this example, one thread transfers from one account (call it ``A'')
to another (call it ``B'') while the other thread transfers from B to
A.  With just the wrong interleaving, the first thread holds the lock
for A and is blocked on the lock for B while the second thread holds
the lock for B and is blocked on the lock for A.  So both are waiting
for a lock to be released by a thread that is blocked.  They will wait
forever.

More generally, a deadlock occurs when there are threads $T_1, T_2,
..., T_n$ such that:
\begin{itemize}
\item For $1\leq i\leq (n-1)$, $T_i$ is waiting for a resource held by
  $T_{i+1}$ 
\item $T_n$ is waiting for a resource held by $T_1$
\end{itemize}
In other words, there is a cycle of waiting.  

Returning to our example, there is, unfortunately, no obvious way to
write an atomic {\tt TransferTo} method that will not deadlock.
Depending on our needs we could write:
\begin{verbatim}
void TransferTo(int amt, BankAccount a)
{
    this.Withdraw(amt);
    a.Deposit(amt);
}
\end{verbatim}
All we have done is make {\tt TransferTo} not be synchronized.
Because {\tt Withdraw} and {\tt Deposit} are still synchronized
methods, the only negative effect is that an intermediate state is
potentially exposed:  another thread that retrieved the balances of
the two accounts in the transfer could now ``see'' the state where the
withdraw had occurred but not the deposit.  If that is okay in our
application, then this is a sufficient solution.  If not, another
approach would be to resort to coarse-grained locking where all
accounts use the same lock.  

Either of these approaches avoids deadlock because they ensure that no
thread ever holds more than one lock at a time.  This is a sufficient
but not necessary condition for avoiding deadlock because it cannot
lead to a cycle of waiting.  A much more flexible
sufficient-but-not-necessary strategy that subsumes the
often-unworkable ``only one lock at a time strategy'' works as
follows: for all pairs of locks $x$ and $y$, either do not have a
thread ever (try to) hold both $x$ and $y$ simultaneously \emph{or}
have a globally agreed upon order, meaning either $x$ is always
acquired before $y$ or $y$ is always acquired before $x$.

This strategy effectively defines a conceptual partial order on locks
and requires that a thread tries to acquire a lock only if all locks
currently held by the thread (if any) come earlier in the partial
order.  It is merely a programming convention that the entire program
must get right.

To understand how this ordering idea can be useful, we return to our
{\tt TransferTo} example.  Suppose we wanted {\tt TransferTo} to be
atomic.  A simpler way to write the code that still has the deadlock
problem we started with is:
\begin{verbatim}
void TransferTo(int amt, BankAccount a)
{
    lock (this)
    {
        lock (a)
        {
            this.Withdraw(amt);
            a.Deposit(amt);
        }
    }
}
\end{verbatim}
Here we make explicit that we need the locks guarding both accounts
and we hold those locks throughout the critical section.  This version
is easier to understand and the potential deadlock --- still caused by
another thread performing a transfer to the accounts in reverse order
--- is also more apparent.  (Note in passing that a deadlock involving
more threads is also possible.  For example, thread 1 could transfer
from A to B, thread 2 from B to C, and thread 3 from C to A.)

The critical section has two locks to acquire.  If all threads
acquired these locks in the same order, then no deadlock could occur.  In
our deadlock example where one thread calls {\tt x.TransferTo(1,y)}
and the other calls {\tt y.TransferTo(1,x)}, this is exactly what goes
wrong: one acquires {\tt x} before {\tt y} and the other {\tt y}
before {\tt x}.  We want one of the threads to acquire the locks in
the other order.

In general, there is no clear way to do this, but sometimes our data
structures have some unique identifier that lets us pick an arbitrary
order to prevent deadlock.  Suppose, as is the case in actual banks,
that every {\tt BankAccount} already has an {\tt acctNumber} field of type
{\tt int} that is distinct from every other {\tt acctNumber}.  Then we
can use these numbers to require that threads always acquire locks for
bank accounts in increasing order.  (We could also require decreasing
order, the key is that all code agrees on one way or the other.)  We
can then write {\tt TransferTo} like this:
\begin{verbatim}
class BankAccount {
  ...
  private int acctNumber;
  void transfertToAtomic(int amt, BankAccount a)
  {
      if (this.acctNumber < a.acctNumber)
      {
          lock (this)
          {
              lock (a)
              {
                  this.Withdraw(amt);
                  a.Deposit(amt);
              }
          }
      }
      else
      {
          lock (a)
          {
              lock (this)
              {
                  this.Withdraw(amt);
                  a.Deposit(amt);
              }
          }
      }
  }
}
\end{verbatim}
While it may not be obvious that this code prevents deadlock, it
should be obvious that it follows the strategy described above.  And
\emph{any} code following this strategy will not deadlock because we cannot
create a cycle if we acquire locks according to the agreed-upon
partial order.

In practice, methods like {\tt TransferTo} where we have two instances
of the same class are a difficult case to handle. Fortunately, 
there are other situations where preventing deadlock
amounts to fairly simple rules about the order that locks are
acquired.  Here are two examples:
\begin{itemize}
\item If you have two different types of objects, you can order the
  locks by the types of data they protect.  For example, the
  documentation could state, ``When moving an item from the dictionary
  to the work queue, never try to acquire the queue's lock while
  holding the dictionary's lock (the other order is acceptable).''
\item If you have an acyclic data structure like a tree, you can use
  the references in the data structure to define the locking order.
  For example, the documentation could state, ``If holding a lock for
  a node in the tree, do not acquire other nodes' locks unless they
  are descendants in the tree.''  
\end{itemize}

\section{Additional Synchronization Primitives}
\label{sec:other-synch}

So far, the synchronization primitives we have seen are (reentrant)
locks, {\tt join}, and volatile fields.  This section describes two
more useful primitives --- reader/writer locks and condition variables
--- in detail, emphasizing how they provide facilities that basic
locks do not.  Then Section~\ref{sec:other-other} briefly mentions some other
primitives.

\subsection{Reader/Writer Locks}

These notes have emphasized multiple times that it is \emph{not} an
error for multiple threads to read the same information
at the same time.  Simultaneous reads of the same field by any number
of threads is not a data race.  Immutable data does not need to be
accessed from within critical sections.  But as soon as there might be
concurrent writes, we have used locks that allow only one thread at a
time.  \emph{This is unnecessarily conservative.}  Considering all the
data guarded by a lock, it would be fine to allow multiple
simultaneous \emph{readers} of the data provided that any
\emph{writer} of the data does so exclusively, i.e., while there are
neither concurrent readers nor writers.  In this way, we still prevent
any data race or bad interleaving resulting from read/write or
write/write errors.  A real-world (or at least on-line) analogy could
be a web-page: It is fine for many people to read the same page at the
same time, but there should be at most one person editing the page,
during which nobody else should be reading or writing the page.

As an example where this would be useful, consider a dictionary such
as a hashtable protected by a single lock (i.e., simple,
coarse-grained locking).  Suppose that lookup operations are common
but insert or delete operations are very rare.  As long as lookup
operations do not actually mutate any elements of the dictionary, it
is fine to allow them to proceed in parallel and doing so avoids
unnecessary contention.\footnote{A good question is why lookups
  \emph{would} mutate anything.  To \emph{clients} of the data
  structure, no mutation should be apparent, but there are
  data-structure techniques that \emph{internally} modify memory
  locations to improve asymptotic guarantees.  Two examples are
  move-to-front lists and splay trees.  These techniques conflict with
  using reader/writer locks as described in this section, which is
  another example of the disadvantages of mutating shared memory
  unnecessarily.}  When an insert or delete operation does arrive, it
would need to block until there were no other operations active
(lookups or other operations that mutate the dictionary), but (a) that
is rare and (b) that is what we expect from coarse-grained locking.
In short, we would like to support simultaneous concurrent reads.
Fine-grained locking might help with this, but only indirectly and
incompletely (it still would not help with reads of the same
dictionary key), and it is much more difficult to implement.

Instead a \emph{reader/writer lock} provides exactly what we are
looking for as a primitive.  It has a different interface than a
regular lock: A reader/writer lock is acquired either ``to be a
reader'' or ``to be a writer.''  The lock allows multiple threads to
hold the lock ``as a reader'' at the same time, but only one to hold
it as a writer and only if there are no holders as readers.  In
particular, here are the operations:
\begin{itemize}
\item {\tt new} creates a new lock that initially has ``0 readers and
  0 writers''
\item {\tt acquire\_write} blocks if there are currently any readers
  or writers, else it makes the lock ``held for writing''
\item {\tt release\_write} returns the lock to its initial state
\item {\tt acquire\_read} blocks if the lock is currently ``held for
  writing'' else it increments the number of readers
\item {\tt release\_read} decrements the number of readers, which may or
  may not return the lock to its initial state
\end{itemize}
A reader/writer lock will block an {\tt acquire\_write} or {\tt acquire\_read} operation as necessary to
maintain the following invariants, where we write $|r|$ and $|w|$ for
the number of threads holding the lock for reading and writing, respectively.
\begin{itemize}
\item $0\leq|w|\leq 1$, i.e., $|w|$ is $0$ or $1$
\item $|w|*|r|=0$, i.e., $|w|$ is $0$ or $|r|$ is $0$
\end{itemize}

Note that the name \emph{reader/writer lock} is really a misnomer.
Nothing about the definition of the lock's operations has anything to
do with reading or writing.  A better name might be a
\emph{shared/exclusive lock}: Multiple threads can hold the lock in
``shared mode'' but only one can hold it in ``exclusive mode'' and
only when no threads hold it in ``shared mode.''  It is up to the
programmer to use the lock correctly, i.e., to ensure that it is okay
for multiple threads to proceed simultaneously when they hold the lock for
reading.  And the most sensible way to ensure this is to read memory
but not write it.

Returning to our dictionary example, using a reader/writer lock is
entirely straightforward: The lookup operation would acquire the
coarse-grained lock for reading, and other operations (insert, delete,
resize, etc.) would acquire it for writing.  If these other operations
are rare, there will be little contention.

There are a few details related to the semantics of reader/writer
locks worth understanding.  To learn how a particular library resolves these
questions, you need to read the documentation.  First, some
reader/writer locks give \emph{priority} to writers.  This means that
an ``acquire for writing'' operation will succeed before any ``acquire
for reading'' operations that \emph{arrive later}.  The ``acquire for
writing'' still has to wait for any threads that \emph{already} hold
the lock for reading to release the lock.  This priority can prevent
writers from \emph{starving} (blocking forever) if readers are so
common or have long enough critical sections that the number of
threads wishing to hold the lock for reading is never 0.

Second, some libraries let a thread \emph{upgrade} from being a reader
to being a writer without releasing the lock.  Upgrading might lead to
deadlock though if multiple threads try to upgrade.

Third, just like regular locks, a reader/writer lock may or may not
be reentrant.  Aside from the case of upgrading, this is an orthogonal
issue.

C\# {\tt lock} statement supports only regular locks, not
reader/writer locks.  But C\# standard library has reader/writer
locks, such as the classes {\tt ReaderWriterLockSlim} 
and {\tt ReaderWriterLock} in the {\tt System.Threading} namespace. 
The interface is slightly different than with synchronized statements 
(you have to be careful to release the lock even in the case of exceptions). 
It also has slightly different operations than what we described above.  Methods {\tt
  AcquireReaderLock} and {\tt AcquireWriterLock} allow to specify timeouts 
  after which lock operation expires, {\tt UpgradeToWriterLock} allows to 
  switch from reader to writer lock, and property {\tt WriterSeqNum} provides information 
  on how many threads acquired the writer lock before. However general idea is the same --- 
  an instance of one of the classes above serves as a lock object, 
  and methods allow clients to acquire reader or writer locks.

\subsection{Condition Variables}
\label{sec:condvars}

Condition variables are a mechanism that lets threads \emph{wait}
(block) until another thread \emph{notifies} them that it might be
useful to ``wake up'' and try again to do whatever could not be done
before waiting.  (We assume the thread that waited chose to do so
because some \emph{condition} prevented it from continuing in a useful
manner.)  It can be awkward to use condition variables
correctly, but most uses of them are within standard libraries, so
arguably the most useful thing is to understand when and why this
wait/notification approach is desirable.

To understand condition variables, consider the canonical example of a
\emph{bounded buffer}.  A bounded buffer is just a queue with a
maximum size.  Bounded buffers that are shared among threads are
useful in many concurrent programs.  For example, if a program is
conceptually a pipeline (an assembly line), we could assign some
number of threads to each stage and have a bounded buffer between each
pair of adjacent stages.  When a thread in one stage produces an
object for the next stage to process, it can enqueue the object in the
appropriate buffer.  When a thread in the next stage is ready to
process more data, it can dequeue from the same buffer.  From the
perspective of this buffer, we call the enqueuers \emph{producer
  threads} (or just \emph{producers}) and the dequeuers \emph{consumer
threads} (or just \emph{consumers}).  

Naturally, we need to synchronize access to the queue to make sure
each object is enqueued/dequeued exactly once, there are no data
races, etc.  One lock per bounded buffer will work fine for this
purpose.  More interesting is what to do if a producer encounters a
full buffer or a consumer encounters an empty buffer.  In
single-threaded programming, it makes sense to throw an exception when
encountering a full or empty buffer since this is an unexpected
condition that cannot change.  But here:
\begin{itemize}
\item If the thread simply waits and tries again later, a producer may
  find the queue no longer full (if a consumer has dequeued) and a 
  consumer may find the queue no longer empty (if a producer has
  enqueued).
\item It is not unexpected to find the queue in a ``temporarily bad''
  condition for continuing.  The buffer is for managing the handoff of
  data and it is entirely natural that at some point the producers
  might get slightly ahead (filling the buffer and wanting to enqueue
  more) or the consumers might get slightly ahead (emptying the buffer
  and wanting to dequeue more).
\end{itemize}
The fact that the buffer is bounded is useful.  Not only does it save
space, but it ensures the producers will not get too far ahead of the
consumers.  Once the buffer fills, we want to stop running the
producer threads since we would rather use our processors to run the
consumer threads, which are clearly not running enough or are taking
longer.  So we really do want waiting threads to stop using
computational resources so the threads that are not stuck can get
useful work done.

With just locks, we can write a version of a bounded buffer that
never results in a wrong answer, but it will be INEXCUSABLY
INEFFICIENT because threads will not stop using resources when they
cannot proceed.  Instead they keep checking to see if they can
proceed:
\begin{verbatim}
class BufferLocksOnly<E>
{
    // not shown: an array of fixed size for the queue with two indices
    // for the front and back, along with methods IsEmpty() and IsFull()

    void Enqueue(E elt)
    {
        while (true)
        {
            lock (this)
            {
                if (!IsFull())
                {
                    // do enqueue
                    return;
                }
            }
        }
    }

    E Dequeue()
    {
        while (true)
        {
            lock (this)
            {
                if (!IsEmpty())
                {
                    E result = default(E);
                    // do dequeue
                    return result;
                }
            }
        }
    }
}
\end{verbatim}
The purpose of condition variables is to avoid this \emph{spinning},
meaning threads checking the same thing over and over again until they
can proceed.  This spinning is particularly bad because it causes
contention for exactly the lock that other threads need to acquire in
order to do the work before the spinners can do something useful.
Instead of constantly checking, the threads could ``sleep for a
while'' before trying again.  But how long should they wait?  Too
little waiting slows the program down due to wasted spinning.  Too
much waiting is slowing the program down because threads that could
proceed are still sleeping.

It would be best to have threads ``wake up'' (unblock) exactly when
they might usefully continue.  In other words, if producers are
blocked on a full queue, wake them up when a dequeue occurs and if
consumers are blocked on an empty queue, wake them up when an enqueue
occurs.  This cannot be done correctly with just locks, which is
exactly why condition variables exist.  

In C\#, just like every object can be used as a lock, every object 
can \emph{also} be used as a condition variable. Necessary operations are 
provided as static methods of a class {\tt Monitor} in namespace {\tt System.Threading}. 
So one can turn any single object into conditional variable just by 
calling methods from {\tt Monitor} on it. We will see later that while this
set-up is often convenient, associating (only) one condition variable
with each lock (the same object) is not always what you want.  There
are three methods:
\begin{itemize}
\item {\tt Wait} should be called only while holding the lock for the
  object.  (Say while using {\tt this} as a lock, the call is to 
  {\tt Monitor.Wait(this)} so the lock {\tt this} should be held.) 
  The call will atomically (a) block the thread,
  (b) release the lock, and (c) register the thread as
  ``waiting'' to be notified.  (This is the step you cannot reasonably
  implement on your own; there has to be no ``gap'' between the
  lock-release and registering for notification else the thread might ``miss''
  being notified and never wake up.)  When the thread is later woken
  up it will re-acquire the same lock before {\tt Wait} returns.  (If
  other threads also seek the lock, then this may lead to more
  blocking.  There is no guarantee that a notified thread gets the
  lock next.)  Notice the thread then continues as part of a \emph{new
    critical section}; the entire point is that the state of shared
  memory has changed in the meantime while it did not hold the lock.
\item {\tt Pulse} wakes up \emph{one} thread that is blocked on the
  condition variable (i.e., has called {\tt Wait} on the same object).
  If there are no such threads, then {\tt Pulse} has no effect, but
  this is fine.  (And this is why there has to be no ``gap'' in the
  implementation of {\tt Wait}.)  If there are multiple threads
  blocked, there is no guarantee which one is notified.
\item {\tt PulseAll} is like {\tt Pulse} except it wakes up all the
  threads blocked on the condition variable.
\end{itemize}
The term \emph{Wait} is standard, but the others vary in different
languages.  Sometimes \emph{pulse} is called \emph{signal} or
\emph{notify}.  Sometimes \emph{pulse all} is called \emph{broadcast}
or \emph{notify all}.

Here is a WRONG use of these primitives for our bounded buffer.  It is
close enough to get the basic idea. Identifying the bugs is
essential for understanding how to use condition variables correctly.
\begin{verbatim}
class BufferWrongNotifyWait<E>
{
    // not shown: an array of fixed size for the queue with two indices
    // for the front and back, along with methods IsEmpty() and IsFull()
    void Enqueue(E elt)
    {
        while (true)
        {
            lock (this)
            {
                if (IsFull())
                {
                    Monitor.Wait(this);
                }
                // do enqueue as normal
                if (...buffer was empty (i.e., now has 1 element) ...)
                {
                    Monitor.Pulse(this);
                }
            }
        }
    }

    E Dequeue()
    {
        while (true)
        {
            lock (this)
            {
                if (IsEmpty())
                {
                    Monitor.Wait(this);
                }
                E ans = default(E);
                // E ans = do dequeue as normal
                if(... buffer was full (i.e., now has room for 1 element) ...)
                {
                    Monitor.Pulse(this);
                }
                return ans;
            }
        }
    }
}
\end{verbatim}
Enqueue waits if the buffer is full.  Rather than spinning, the thread
will consume no resources until awakened.  While waiting, it does not
hold the lock (per the definition of {\tt Wait}), which is crucial so
other operations can complete.  On the other hand, if the enqueue puts
one item into an empty queue, it needs to call {\tt Pulse} in case
there are dequeuers waiting for data.  Dequeue is symmetric.  Again,
this is the \emph{idea} --- wait if the buffer cannot handle what you
need to do and notify others who might be blocked after you change the
state of the buffer --- but this code has two serious bugs.  It may
not be easy to see them because thinking about condition variables
takes some getting used to.

The first bug is that we have to account for the fact that after a
thread is notified that it should try again, but before it actually
tries again, the buffer can undergo other operations by other
threads.  That is, there is a ``gap'' between the notification and
when the notified thread actually re-acquires the lock to begin its
new critical section.  Here is an example bad interleaving with the
{\tt Enqueue} in Thread 1 doing the wrong thing; a symmetric example
exists for {\tt Dequeue}.
\goodbreak
\begin{verbatim}
Initially the buffer is full (so Thread 1 blocks)

Thread 1 (enqueue)            Thread 2 (dequeue)        Thread 3 (enqueue)
------------------            ------------------        -------------------
if(IsFull())
   Monitor.Wait(this);
                              ... do dequeue ...
                              if(... was full ...)
                                  Monitor.Pulse(this);
                                                        if(IsFull()) (not full: don't wait)
 
                                                        ... do enqueue ...           
... do enqueue ... (wrong!)
\end{verbatim}
Thread 3 ``snuck in'' and re-filled the buffer before Thread 1 ran again.
Hence Thread 1 adds an object to a full buffer, when what it should do
is wait again.  The fix is that Thread 1, after (implicitly)
re-acquiring the lock, must again check whether the buffer is full.  A
second if-statement does not suffice because if the buffer is full
again, it will wait and need to check a third time after being awakened
and so on.  Fortunately, we know how to check something repeatedly
until we get the answer we want: a while-loop.  So this version fixes
this bug, but is still WRONG:
\begin{verbatim}
class BufferWrongPartiallyFixed<E>
{
    // not shown: an array of fixed size for the queue with two indices
    // for the front and back, along with methods IsEmpty() and IsFull()
    void Enqueue(E elt)
    {
        while (true)
        {
            lock (this)
            {
                while (IsFull())
                {
                    Monitor.Wait(this);
                }
                // do enqueue as normal
                if (...buffer was empty (i.e., now has 1 element) ...)
                {
                    Monitor.Pulse(this);
                }
            }
        }
    }

    E Dequeue()
    {
        while (true)
        {
            lock (this)
            {
                while (IsEmpty())
                {
                    Monitor.Wait(this);
                }
                E ans = default(E);
                // E ans = do dequeue as normal
                if(... buffer was full (i.e., now has room for 1 element) ...)
                {
                    Monitor.Pulse(this);
                }
                return ans;
            }
        }
    }
}
\end{verbatim}
The only change is using a while-loop instead of an if-statement for
deciding whether to {\tt Wait}.  \emph{Calls to {\tt Wait} should
  always be inside while-loops that re-check the condition.}  Not only
is this in almost every situation the right thing to do for your
program, but technically for obscure reasons .Net is allowed to pulse
(i.e., wake up) a thread even if your program does not!  So it is
mandatory to re-check the condition since it is possible that nothing
changed.  This is still much better than the initial spinning version
because each iteration of the while-loop corresponds to being
notified, so we expect very few (probably 1) iterations.

Unfortunately, the version of the code above is still wrong.  It does
not account for the possibility of multiple threads being blocked due
to a full (or empty) buffer.  Consider this interleaving:
\begin{verbatim}
Initially the buffer is full (so Threads 1 and 2 block)

Thread 1 (enqueue)         Thread 2 (enqueue)      Thread 3 (two dequeues)
------------------         ------------------      ----------------------
while(IsFull())
   Monitor.Wait(this);
                           while(IsFull())
                             Monitor.Wait(thos);
                                                   ... do dequeue ...
                                                   Monitor.Pulse(this);

                                                   ... do dequeue ...
                                                   // no notification!
\end{verbatim}
Sending one notification on the first dequeue was fine at that point,
but that still left a thread blocked.  If the second dequeue happens
before any enqueue, no notification will be sent, which leaves the
second blocked thread waiting when it should not wait.  

The simplest solution in this case is to change the code to use {\tt
  PulseAll} instead of {\tt Pulse}.  That way, whenever the buffer
  becomes non-empty, \emph{all} blocked producers know about it and
  similarly whenever the buffer becomes non-full, \emph{all} blocked
  consumers wake up.  If there are $n$ blocked threads and there are
  no subsequent operations like the second dequeue above, then $n-1$
  threads will simply block again, but at least this is correct.  So
  this is RIGHT:
\begin{verbatim}
class BufferRight<E>
{
    // not shown: an array of fixed size for the queue with two indices
    // for the front and back, along with methods IsEmpty() and IsFull()
    void Enqueue(E elt)
    {
        while (true)
        {
            lock (this)
            {
                while (IsFull())
                {
                    Monitor.Wait(this);
                }
                // do enqueue as normal
                if (...buffer was empty (i.e., now has 1 element) ...)
                {
                    Monitor.PulseAll(this);
                }
            }
        }
    }

    E Dequeue()
    {
        while (true)
        {
            lock (this)
            {
                while (IsEmpty())
                {
                    Monitor.Wait(this);
                }
                E ans = default(E);
                // E ans = do dequeue as normal
                if(... buffer was full (i.e., now has room for 1 element) ...)
                {
                    Monitor.PulseAll(this);
                }
                return ans;
            }
        }
    }
}
\end{verbatim}
If we do not expect very many blocked threads, this {\tt PulseAll}
solution is reasonable.  Otherwise, it causes a lot of
probably-wasteful waking up (just to have $n-1$ threads probably block
again).  Another tempting solution ALMOST works, but of course
``almost'' means ``wrong.''  We could have every enqueue and dequeue
call {\tt Monitor.Pulse(this)}, not just if the buffer had been empty or
full.  (In terms of the code, just replace the if-statements with
calls {\tt Monitor.Pulse(this)}.)  In terms of the interleaving above, the
first dequeue would wake up one blocked enqueuer and the second
dequeue would wake up another one.  If no thread is blocked, then the
{\tt Pulse} call is unnecessary but does no harm.

The reason this is incorrect is subtle, but here is one scenario where
the wrong thing happens.  For simplicity, assume the size of the
buffer is 1 (any size will do but larger sizes require more threads
before there is a problem).
\begin{enumerate}
\item Assume the buffer starts empty.
\item Then two threads $T_1$ and $T_2$ block trying to dequeue.
\item Then one thread $T_3$ enqueues (filling the buffer) and calls
  {\tt notify}.  Suppose this wakes up $T_1$.
\item But before $T_1$ re-acquires the lock, another thread $T_4$
  tries to enqueue and blocks (because the buffer is still full).
\item Now $T_1$ runs, emptying the buffer and calling {\tt notify},
  \emph{but in a stroke of miserable luck this wakes up $T_2$ instead
  of $T_4$}.
\item $T_2$ will see an empty buffer and wait again.  So now $T_2$ and
  $T_4$ are blocked even though $T_4$ should enqueue and then $T_2$
  could dequeue.
\end{enumerate}

The way to fix this problem is to use \emph{two condition variables}
--- one for producers to notify consumers and one for consumers to
notify producers.  But we need both condition variables to be
``associated with'' the same lock, since we still need all producers
and consumers to use one lock to enforce mutual exclusion.  This is
also supported by the {\tt Monitor} class, which allows one to create 
as many conditional variables as needed. You can call {\tt Wait} and 
{\tt Pulse} on any of those individual variables as needed.

In any case, condition variables are:
\begin{itemize}
\item Important: Concurrent programs need ways to have threads wait
  without spinning until they are notified to try again.  Locks are
  not the right tool for this.
\item Subtle: Bugs are more difficult to reason about and the
  correct programming idioms are less intuitive than with locks.
\end{itemize}
In practice, condition variables tend to be used in very stylized
ways.  Moreover, it is rare that you need to use condition variables
explicitly rather than using a library that internally uses condition
variables.  For example, since .Net 4 standard library includes a class
{\tt System.Collections.Concurrent.BlockingCollection<T>} that is exactly what
we need for a bounded buffer.  (It uses the names {\tt Add} and {\tt
  Take} rather than {\tt enqueue} and {\tt dequeue}.)  It is good to
know that any blocking caused by calls to the library will be
efficient thanks to condition variables, but we do not need to write
or understand the condition-variable code.

\subsection{Other}
\label{sec:other-other}

Locks and condition variables are not the only synchronization
primitives.  For one, do not forget that {\tt join} synchronizes two
threads in a particular way and is very convenient when it is the
appropriate tool.  You may encounter other primitives as you gain
experience with concurrent programming.  Your experience
learning about locks should make it much easier to learn other
constructs.  Key things to understand when learning about
synchronization are:
\begin{itemize}
\item What interleavings does a primitive prevent?
\item What are the rules for using a primitive correctly?
\item What are the standard idioms where a primitive proves most
  useful?
\end{itemize}

Another common exercise is to implement one primitive in terms of
another, perhaps inefficiently.  For example, it is possible to
implement {\tt join} in terms of locks: The helper thread can hold a lock
until just before it terminates.  The {\tt join} primitive can then
(try to) acquire and immediately release this lock.  Such an
\emph{encoding} is poor style if {\tt join} is provided directly, but
it can help understand how {\tt join} works and can be useful if you
are in a setting where only locks are provided as primitives.

Here is an incomplete list of other primitives you may encounter:
\begin{itemize}
\item \emph{Semaphores} are rather low-level mechanisms where two
  operations, called $P$ and $V$ for historical reasons, increment or
  decrement a counter.  There are rules about how these operations are
  synchronized and when they block.  Semaphores can be easily used to
  implement locks as well as barriers. .Net standard library provides
  {\tt System.Threading.Semaphore} class that implements this primitive.
\item \emph{Barriers} are a bit more like {\tt join} and are often
  more useful in parallel programming than concurrent programming.
  When threads get to a barrier, they block until $n$ of them (where
  $n$ is a property of the barrier) reach the barrier.  Unlike {\tt
  join}, the synchronization is not waiting for \emph{one} other
  thread to \emph{terminate}, but rather $n$ other threads to
  \emph{get to the barrier}. Again, {\tt System.Threading.Barrier} 
  is the implementation of this primitive in .Net.
\item \emph{Monitors} provide synchronization that corresponds more
  closely to the structure of your code.  Like an object, they can
  encapsulate some private state and provide some entry points
  (methods).  As synchronization, the basic idea is that only one
  thread can call any of the methods at a time, with the others
  blocking.  This is \emph{very} much like an objects we've seen before 
  where all the methods are synchronized on some lock, say {\tt this}.
  So it should come at no surprise that C\# {\tt lock} keyword is a just 
  a shortcut for this code (C\# 4 or later):
\begin{verbatim}
bool lockWasTaken = false;
var temp = obj;
try
{
    Monitor.Enter(temp, ref lockWasTaken);
    ...code inside "lock" goes here...
}
finally
{
    if (lockWasTaken)
    {
        Monitor.Exit(temp);
    }
}
\end{verbatim}
  Thus {\tt Monitor} class in C\# actually provides means to implement
  \emph{Lock}, \emph{Re-entrant Lock} and \emph{Monitor} primitives. 
\end{itemize}

\end{document}